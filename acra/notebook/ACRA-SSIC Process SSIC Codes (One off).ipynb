{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# One off operations during a change of SSIC Code Excel file\n",
        "\n",
        "## README\n",
        "\n",
        "Run this code only if there is a change in SSIC Code Excel file, or configurations\n",
        "\n",
        "### Data Preperation\n",
        "\n",
        "All data should be stored in the Google Cloud Storage\n",
        "\n",
        "- data/ssic2020-detailed-definitions.xlsx\n",
        "  - Please modify config files for the following:\n",
        "      - Which row the headers are located\n",
        "      - The SSIC Code column name\n",
        "      - The SSIC Description column name\n",
        "      - The SSIC Definitions column name\n",
        "  - The data logic must be the same, ie. all SSIC codes in the one column, regardless if they belong to 3 digits or 4 digits, and both must be present\n",
        "\n",
        "- Check config variable below and save it by running all the cells\n",
        "  - project_id: GCP project ID\n",
        "  - bucket name\n",
        "  - model names\n",
        "  \n",
        "- Config is saved as config.yaml in the root folder of the bucket in GCS"
      ],
      "metadata": {
        "id": "lXFcb-dco7LB"
      },
      "id": "lXFcb-dco7LB"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5nftkcVyR_s",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1704938386663,
          "user_tz": -480,
          "elapsed": 19642,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1d2c3ef8-784e-483a-bbb6-2ca3b6b3349d"
      },
      "id": "d5nftkcVyR_s",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.0-py3-none-any.whl (797 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.9 (from langchain)\n",
            "  Downloading langchain_community-0.0.11-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
            "  Downloading langchain_core-0.1.9-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
            "  Downloading langsmith-0.0.79-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.0 langchain-community-0.0.11 langchain-core-0.1.9 langsmith-0.0.79 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Function"
      ],
      "metadata": {
        "id": "RxJICBrJXAbX"
      },
      "id": "RxJICBrJXAbX"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "4XVrmDDBqv5-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1704938386664,
          "user_tz": -480,
          "elapsed": 7,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "4XVrmDDBqv5-",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()  # Authenticate with Google Cloud\n",
        "\n",
        "from google.cloud import storage\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tempfile\n",
        "import math\n",
        "\n",
        "class TextEmbedder:\n",
        "    def __init__(self, model_name, project_id, bucket_name, from_local = False, model_location = None, tokenizer_location = None):\n",
        "        self.project_id = project_id\n",
        "        self.model_location = model_location\n",
        "        self.tokenizer_location = tokenizer_location\n",
        "        self.bucket_name = bucket_name\n",
        "        self.model_name = model_name\n",
        "\n",
        "        if model_name == \"textembedding-gecko\":\n",
        "            from vertexai.preview.language_models import TextEmbeddingModel\n",
        "            self.model = TextEmbeddingModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            if from_local:\n",
        "                self.download_directory_from_gcs(model_location, './model')\n",
        "                self.download_directory_from_gcs(tokenizer_location, './tokenizer')\n",
        "                self.model = AutoModel.from_pretrained('./model')\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained('./tokenizer')\n",
        "            else:\n",
        "                # Load model\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                self.model = AutoModel.from_pretrained(self.model_name)\n",
        "\n",
        "                self.client = storage.Client(project=self.project_id)\n",
        "\n",
        "                # Save the model and tokenizer locally\n",
        "                self.model.save_pretrained('./model')\n",
        "                self.tokenizer.save_pretrained('./tokenizer')\n",
        "                self.upload_directory_to_gcs('./model', model_location)\n",
        "                self.upload_directory_to_gcs('./tokenizer', tokenizer_location)\n",
        "\n",
        "    def download_directory_from_gcs(self, source_directory_name, destination_directory_name):\n",
        "        \"\"\"Downloads a directory from the bucket.\"\"\"\n",
        "        bucket = self.client.get_bucket(self.bucket_name)\n",
        "        blobs = bucket.list_blobs(prefix=source_directory_name)\n",
        "\n",
        "        for blob in blobs:\n",
        "            filename = blob.name.replace('/', '_')\n",
        "            blob.download_to_filename(os.path.join(destination_directory_name, filename))\n",
        "\n",
        "    def upload_directory_to_gcs(self, source_directory_name, destination_blob_name):\n",
        "        \"\"\"Uploads a directory to the bucket.\"\"\"\n",
        "        bucket = self.client.get_bucket(self.bucket_name)\n",
        "        for root, dirs, files in os.walk(source_directory_name):\n",
        "            for filename in files:\n",
        "                blob = bucket.blob(os.path.join(destination_blob_name, filename))\n",
        "                blob.upload_from_filename(os.path.join(root, filename))\n",
        "\n",
        "    def save_model(self):\n",
        "\n",
        "        # Create a temporary directory to save the model and tokenizer\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            local_model_path = f\"{temp_dir}/model\"\n",
        "            local_tokenizer_path = f\"{temp_dir}/tokenizer\"\n",
        "\n",
        "            # Save model and tokenizer locally\n",
        "            self.model.save_pretrained(local_model_path)\n",
        "            self.tokenizer.save_pretrained(local_tokenizer_path)\n",
        "\n",
        "            # Authenticate with Google Cloud Storage\n",
        "            gcs_client = storage.Client()\n",
        "\n",
        "            # Upload model and tokenizer to GCS\n",
        "            bucket = gcs_client.bucket(bucket_name)\n",
        "            blob_model = bucket.blob(f\"{self.model_location}/model\")\n",
        "            blob_tokenizer = bucket.blob(f\"{self.model_location}/tokenizer\")\n",
        "\n",
        "            blob_model.upload_from_filename(local_model_path, content_type=\"application/octet-stream\")\n",
        "            blob_tokenizer.upload_from_filename(local_tokenizer_path, content_type=\"application/octet-stream\")\n",
        "\n",
        "    def _average_pool(self, last_hidden_states, attention_mask):\n",
        "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "    def encode_text(self, input_texts, batch_size):\n",
        "        if self.model_name == \"textembedding-gecko\":\n",
        "            # Note only 250 items, and limited tokens\n",
        "            batch_size = 3\n",
        "            self.embeddings = torch.Tensor([])\n",
        "            num_batches = math.ceil(len(input_texts) / batch_size)\n",
        "            for i in range(num_batches):\n",
        "                batch_texts = input_texts[i*batch_size:min((i+1)*batch_size, len(input_texts))]\n",
        "                embeddings = self.model.get_embeddings(batch_texts)\n",
        "                self.embeddings = torch.cat((self.embeddings, torch.stack([torch.tensor(inner_list.values) for inner_list in embeddings])), dim=0)\n",
        "\n",
        "        else:\n",
        "            # Tokenize the input texts\n",
        "            embeddings = []\n",
        "\n",
        "            # Get the total number of input texts\n",
        "            total_texts = len(input_texts)\n",
        "\n",
        "            with tqdm(total=total_texts, desc=\"Embedding Progress\") as pbar:\n",
        "                with torch.no_grad():\n",
        "                    for i in range(0, len(input_texts), batch_size):\n",
        "                        batch_dict = self.tokenizer(input_texts[i:i+batch_size], max_length=1024, padding=True, truncation=True, return_tensors='pt')\n",
        "                        input_chunk = batch_dict['input_ids']\n",
        "                        mask_chunk = batch_dict['attention_mask']\n",
        "                        outputs = self.model(input_ids=input_chunk, attention_mask=mask_chunk)\n",
        "                        chunk_embeddings = self._average_pool(outputs.last_hidden_state, mask_chunk)\n",
        "                        embeddings.append(chunk_embeddings)\n",
        "                        pbar.update(batch_size)\n",
        "\n",
        "            # Concatenate embeddings from all batches\n",
        "            embeddings = torch.cat(embeddings, dim=0)\n",
        "            self.embeddings = embeddings\n",
        "\n",
        "    def save_embeddings(self, bucket_name, file_name):\n",
        "        \"\"\"Save the embeddings to a Google Cloud Storage bucket.\"\"\"\n",
        "\n",
        "        # Convert the embeddings to a NumPy array\n",
        "        embeddings_np = self.embeddings.detach().numpy()\n",
        "\n",
        "        # Convert the NumPy array to bytes\n",
        "        embeddings_bytes = embeddings_np.tobytes()\n",
        "\n",
        "        # Initialize the Google Cloud Storage client\n",
        "        storage_client = storage.Client(project=self.project_id)\n",
        "\n",
        "        # Get the bucket\n",
        "        bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "        # Create a new blob and upload the embeddings\n",
        "        blob = storage.Blob(file_name, bucket)\n",
        "        blob.upload_from_string(embeddings_bytes)\n",
        "\n",
        "        print(f'Embeddings ({embeddings_np.shape}) uploaded to gs://{bucket_name}/{file_name}')"
      ],
      "metadata": {
        "id": "bIldDDqV_eNn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1704938404579,
          "user_tz": -480,
          "elapsed": 17921,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "bIldDDqV_eNn",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fileloading Function\n",
        "\n",
        "Only for connecting to google cloud storage!"
      ],
      "metadata": {
        "id": "5uzjwSQqXCwy"
      },
      "id": "5uzjwSQqXCwy"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()  # Authenticate with Google Cloud\n",
        "\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "import io\n",
        "import json\n",
        "import yaml\n",
        "\n",
        "## File loading\n",
        "class FileLoader:\n",
        "    def __init__(self, bucket_name):\n",
        "         # Authenticate with Google Cloud\n",
        "        auth.authenticate_user()\n",
        "\n",
        "        self.bucket_name = bucket_name\n",
        "\n",
        "        # Initialize a client to access Google Cloud Storage\n",
        "        storage_client = storage.Client()\n",
        "\n",
        "        # Get the bucket and blob (object)\n",
        "        self.bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    def save_json_in_gcs(self, data, file_path):\n",
        "\n",
        "        blob = self.bucket.blob(file_path)\n",
        "\n",
        "        # Convert the list to a JSON string\n",
        "        json_data = json.dumps(data)\n",
        "\n",
        "        # Upload the JSON data to the blob\n",
        "        blob.upload_from_string(json_data)\n",
        "\n",
        "\n",
        "    def load_json_from_gcs(self, file_path):\n",
        "\n",
        "        blob = self.bucket.blob(file_path)\n",
        "\n",
        "        # Download the JSON data from the blob\n",
        "        json_data = blob.download_as_text()\n",
        "\n",
        "        # Parse the JSON data into a Python list\n",
        "        data_list = json.loads(json_data)\n",
        "        return data_list\n",
        "\n",
        "    def load_df_from_gcs(self, file_path, header = 0):\n",
        "\n",
        "        blob = self.bucket.blob(file_path)\n",
        "        content = blob.download_as_text(encoding='latin1')\n",
        "\n",
        "        # Convert the string content to bytes\n",
        "        content_bytes = content.encode('latin1')\n",
        "\n",
        "        # Create a DataFrame from the Excel content\n",
        "        df = pd.read_excel(io.BytesIO(content_bytes), header = header)\n",
        "        return df\n",
        "\n",
        "    def save_df_from_gcs(self, df, file_path):\n",
        "\n",
        "        # Convert DataFrame to CSV string\n",
        "        csv_string = df.to_csv(index=False)\n",
        "\n",
        "        # Upload the CSV string to the blob\n",
        "        blob = self.bucket.blob(file_path)\n",
        "        blob.upload_from_filename(csv_string)\n",
        "\n",
        "    def save_dict_to_yaml_gcs(self, file_name, data):\n",
        "        # Convert dictionary to YAML format\n",
        "        yaml_data = yaml.dump(data, default_flow_style=False)\n",
        "\n",
        "        # Create blob (file) in the bucket\n",
        "        blob = self.bucket.blob(file_name)\n",
        "\n",
        "        # Upload YAML data to the blob\n",
        "        blob.upload_from_string(yaml_data)\n",
        "\n",
        "    def read_yaml_from_gcs(self, file_name):\n",
        "        blob = self.bucket.blob(file_name)\n",
        "\n",
        "        # Download the YAML content as bytes\n",
        "        yaml_bytes = blob.download_as_bytes()\n",
        "\n",
        "        # Load the YAML content as a dictionary\n",
        "        yaml_dict = yaml.safe_load(yaml_bytes)\n",
        "\n",
        "        return yaml_dict"
      ],
      "metadata": {
        "id": "O3UECiTZWJ-V",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1704938404580,
          "user_tz": -480,
          "elapsed": 13,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "O3UECiTZWJ-V",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Pipeline"
      ],
      "metadata": {
        "id": "pV9_HoHmo3MD"
      },
      "id": "pV9_HoHmo3MD"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class SSICProcessor:\n",
        "    def __init__(self, text_embedder, fileloader, config):\n",
        "        \"\"\"\n",
        "        Class to create all artifacts for SSIC\n",
        "        args:\n",
        "            text_embedder (object): TextEmbedder class\n",
        "            fileloader (object): FileLoader class\n",
        "            config (dict): Configuration dictionary\n",
        "        \"\"\"\n",
        "        self.text_embedder = text_embedder\n",
        "        self.fileloader = fileloader\n",
        "        self.config = config\n",
        "\n",
        "        self.ssic_df = self.fileloader.load_df_from_gcs(config['dir']['ssic_df_fp'], config['columns']['header_row'])\n",
        "\n",
        "        self.l2_l3_mapper()\n",
        "\n",
        "        self.activities = [\n",
        "        \"\"\"Primary activities, segments, revenue stream\n",
        "        Main operations, subsidiaries\n",
        "        Principal Sectors\n",
        "        \"\"\"\n",
        "        ]\n",
        "\n",
        "    def remove_digits_and_bullet(self, text):\n",
        "        \"\"\"\n",
        "        Remove digits and bullet from text\n",
        "        args:\n",
        "            text (str): text to process\n",
        "        \"\"\"\n",
        "        # Use a regular expression to match digits and the '•' character\n",
        "        pattern = r'[0-9•]'\n",
        "        # Use the re.sub function to replace all matches with an empty string\n",
        "        cleaned_text = re.sub(pattern, '', text)\n",
        "        return cleaned_text\n",
        "\n",
        "    def l2_l3_mapper(self):\n",
        "        \"\"\"Creates mapping from L2 to L3\"\"\"\n",
        "        # Initialize an empty dictionary\n",
        "        l2_l3_mapping = {}\n",
        "        ssic_code_col = self.config['columns']['ssic_code']\n",
        "        ssic_title_col = self.config['columns']['ssic_desc']\n",
        "\n",
        "        # Iterate through the DataFrame rows\n",
        "        for _, row in self.ssic_df.iterrows():\n",
        "            key = row[ssic_code_col]\n",
        "            value = row[ssic_title_col]\n",
        "\n",
        "            # Check if the key is a 4-digit value -- L2\n",
        "            if len(key) == 4:\n",
        "                l2_l3_mapping[value] = []\n",
        "                current_key = value\n",
        "            elif len(key) == 5: # Check if the key is a 5-digit value -- L3\n",
        "                l2_l3_mapping[current_key].append(value)\n",
        "        self.l2_l3_mapping = l2_l3_mapping\n",
        "\n",
        "\n",
        "    def map_l2_l3(self, rel_l2):\n",
        "      \"\"\"Get l2 from l3\"\"\"\n",
        "      if rel_l2:\n",
        "          rel_l3 = []\n",
        "          for l2 in rel_l2:\n",
        "              l3_list = self.l2_l3_mapping[l2]\n",
        "              for l3 in l3_list:\n",
        "                 rel_l3.append(l3)\n",
        "          return rel_l3\n",
        "\n",
        "    def construct_ssic_lists(self):\n",
        "        \"\"\"Construct all json artifacts\"\"\"\n",
        "        # Get columns\n",
        "        ssic_code_col = self.config['columns']['ssic_code']\n",
        "        ssic_title_col = self.config['columns']['ssic_desc']\n",
        "\n",
        "        self.ssic_df[ssic_code_col] = self.ssic_df[ssic_code_col].str.replace(' ', '')\n",
        "        df = self.ssic_df.copy()\n",
        "\n",
        "        # Others filter condition\n",
        "        filter_cond = (df[ssic_title_col].str.lower().str.contains('n.e.c')|df[ssic_title_col].str.lower().str.contains('other'))\n",
        "\n",
        "        # Get sections for L2, L3 and others\n",
        "        ssic_l2_other = df[df[ssic_code_col].str.len()==4][filter_cond].reset_index(drop=True)\n",
        "        ssic_l3 = df[df[ssic_code_col].str.len()==5][~filter_cond].reset_index(drop=True)\n",
        "        ssic_l3_other = df[df[ssic_code_col].str.len()==5][filter_cond].reset_index(drop=True)\n",
        "\n",
        "        # For L3 others, it includes both L2 others and L3 others\n",
        "        l3_others = list(set(self.map_l2_l3(ssic_l2_other[ssic_title_col].tolist()) + ssic_l3_other[ssic_title_col].tolist()))\n",
        "        #ssic_l3_other = df[df[ssic_title_col].isin(l3_others)]\n",
        "        ssic_l3 = ssic_l3[~ssic_l3[ssic_title_col].isin(l3_others)]\n",
        "\n",
        "        # Form lists\n",
        "        self.l3_list = ssic_l3[ssic_title_col].tolist()\n",
        "        self.l3_list_others = l3_others\n",
        "\n",
        "\n",
        "        print(\"Total number of 5-digit SSIC Codes (non-others): \", len(self.l3_list))\n",
        "        print(\"Total number of 5-digit SSIC Codes (others): \",len(self.l3_list_others))\n",
        "\n",
        "        # Save lists\n",
        "        self.fileloader.save_json_in_gcs(self.l3_list, config[\"dir\"][\"l3_list_fp\"])\n",
        "        self.fileloader.save_json_in_gcs(self.l3_list_others, config[\"dir\"][\"l3_list_others_fp\"])\n",
        "\n",
        "        print(\"Constructed SSIC Artifacts.\")\n",
        "\n",
        "    def generate_df_from_list(self, df, filter_list, column):\n",
        "\n",
        "        # Filter DataFrame based on the list\n",
        "        filtered_df = df[df[column].isin(filter_list)]\n",
        "\n",
        "        # Create a custom sorting based on the order of the list\n",
        "        filtered_df['Order'] = filtered_df[column].apply(lambda x: filter_list.index(x))\n",
        "\n",
        "        # Sort the DataFrame based on the custom order\n",
        "        filtered_df = filtered_df.sort_values(by='Order')\n",
        "\n",
        "        # Drop the 'Order' column if it's not needed anymore\n",
        "        filtered_df = filtered_df.drop('Order', axis=1)\n",
        "\n",
        "        return filtered_df\n",
        "\n",
        "    def construct_ssic_embeddings(self):\n",
        "        \"\"\"Construct all embedding artifacts\"\"\"\n",
        "        # Generate activity embeddings\n",
        "        text_embedder.encode_text(self.activities,1)\n",
        "        self.text_embedder.save_embeddings(self.config['bucket_name'], self.config[\"dir\"][\"activity_embedding_fp\"])\n",
        "\n",
        "        # Generate L3 others by combining description, definitions and level 2\n",
        "        df = self.ssic_df.copy()\n",
        "        ssic_code_col = self.config['columns']['ssic_code']\n",
        "        ssic_definitions_col = self.config['columns']['ssic_definitions']\n",
        "        ssic_title_col = self.config['columns']['ssic_desc']\n",
        "\n",
        "        # Clean df\n",
        "        df[ssic_definitions_col] = df[ssic_definitions_col].apply(self.remove_digits_and_bullet)\n",
        "\n",
        "        ## Construct l2_l3_df\n",
        "        l2_l3_df = pd.DataFrame(list(self.l2_l3_mapping.items()), columns=['l2', 'l3']).explode('l3').reset_index(drop=True)\n",
        "        l2_l3_df['l2_l3'] = l2_l3_df['l3'] + ', ' + l2_l3_df['l2']\n",
        "        l2_l3_df = l2_l3_df.drop_duplicates('l3').reset_index(drop=True)\n",
        "\n",
        "        # Merge dataframes to get L2, L3, Descriptions per entry\n",
        "        ssic_df_l3 = df.merge(l2_l3_df, left_on = ssic_title_col, right_on='l3', how='right').drop_duplicates('l3').reset_index(drop=True)\n",
        "        ssic_df_l3['l3_desc'] = ssic_df_l3[[ssic_title_col, ssic_definitions_col, 'l2_l3']].apply(lambda x: str(x[2]) + ', ' + str(x[1]) + \", \" + str(x[0]), axis=1)\n",
        "        ssic_df_l3_others = self.generate_df_from_list(ssic_df_l3, self.l3_list_others, ssic_title_col)\n",
        "        ssic_df_l3_nonothers = self.generate_df_from_list(ssic_df_l3, self.l3_list, ssic_title_col)\n",
        "\n",
        "        assert len(ssic_df_l3_nonothers) == len(self.l3_list), print(f\"Generate dataframe {len(ssic_df_l3_nonothers)} not of correct length {len(self.l3_list)}.\")\n",
        "        assert len(ssic_df_l3_others) == len(self.l3_list_others), print(f\"Generate dataframe (others) {len(ssic_df_l3_others)} not of correct length {len(self.l3_list_others)}\")\n",
        "\n",
        "        # Construct Emebddings\n",
        "        print(\"Generating Embeddings for non-others descriptors...\")\n",
        "        text_embedder.encode_text(ssic_df_l3_nonothers['l3_desc'].tolist(),self.config[\"batch_size\"])\n",
        "        self.text_embedder.save_embeddings(self.config['bucket_name'], self.config[\"dir\"][\"l2_l3_embeddings_fp\"])\n",
        "\n",
        "        print(\"Generating Embeddings for Others descriptors...\")\n",
        "        text_embedder.encode_text(ssic_df_l3_others['l3_desc'].tolist(),self.config[\"batch_size\"])\n",
        "        self.text_embedder.save_embeddings(self.config['bucket_name'], self.config[\"dir\"][\"l2_l3_embeddings_others_fp\"])\n",
        "\n",
        "        print(\"Constructed Embeddings.\")\n",
        "\n",
        "        #self.l2_embedding_others = text_embedder.encode_text(self.l2_list_others,self.config[\"batch_size\"])\n",
        "        #self.text_embedder.save_embeddings(self.bucket_name, self.config[\"dir\"][\"l2_embedding_others_fp\"])"
      ],
      "metadata": {
        "id": "VpLrfR0N8ghN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1704938404580,
          "user_tz": -480,
          "elapsed": 12,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "VpLrfR0N8ghN",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"project_id\": \"double-insight-395609\",\n",
        "    \"bucket_name\": \"acra-ssic-classification\",\n",
        "    \"google_model_name\": \"text-bison\",\n",
        "     #\"classifier_model_name\": \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\",\n",
        "   \"classifier_model_name\": \"text-bison\",\n",
        "    \"summarizer_model_name\": \"text-bison\",\n",
        "    \"text_embedding_model_name\": \"textembedding-gecko\",\n",
        "    \"evaluation_mode\": True,\n",
        "    #\"text_embedding_model_name\": \"thenlper/gte-large\",\n",
        "    \"batch_size\": 3,\n",
        "    \"columns\":{\n",
        "        \"ssic_code\": \"SSIC 2020\",\n",
        "        \"ssic_desc\": \"SSIC 2020 Title\",\n",
        "        \"ssic_definitions\": \"Detailed Definitions\",\n",
        "        \"header_row\": 4\n",
        "    },\n",
        "    \"dir\":{\n",
        "        \"model_location\": \"model\",\n",
        "        \"tokenizer_location\": \"tokenizer\",\n",
        "        \"ssic_df_fp\": \"data/ssic2020-detailed-definitions.xlsx\",\n",
        "        \"l3_list_fp\": \"data/l3_list.json\",\n",
        "        \"l3_list_others_fp\": \"data/l3_list_others.json\",\n",
        "        \"l2_l3_embeddings_fp\": \"embeddings/l2_l3_embeddings.npy\",\n",
        "        \"l2_l3_embeddings_others_fp\": \"embeddings/l2_l3_embeddings_others.npy\",\n",
        "        \"activity_embedding_fp\": \"embeddings/activity_embedding.npy\",\n",
        "        \"fs_dir\": \"data/fs\",\n",
        "        \"output_fp\": \"data/output/output.csv\",\n",
        "        \"label_fp\": \"data/Company SSIC Code.csv\",\n",
        "        \"evaluator_fp\": \"data/output/evaluator.csv\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "eZvOUieVh5lL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1704938404580,
          "user_tz": -480,
          "elapsed": 11,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "eZvOUieVh5lL",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'acra-ssic-classification'\n",
        "fileloader = FileLoader(bucket_name)\n",
        "fileloader.save_dict_to_yaml_gcs(\"config.yaml\",config)\n",
        "config = fileloader.read_yaml_from_gcs('config.yaml')\n",
        "config"
      ],
      "metadata": {
        "id": "m4cIe932h_6A",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1704938405505,
          "user_tz": -480,
          "elapsed": 936,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9307969-f212-4e4d-d8e1-fae005b9f913"
      },
      "id": "m4cIe932h_6A",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 3,\n",
              " 'bucket_name': 'acra-ssic-classification',\n",
              " 'classifier_model_name': 'text-bison',\n",
              " 'columns': {'header_row': 4,\n",
              "  'ssic_code': 'SSIC 2020',\n",
              "  'ssic_definitions': 'Detailed Definitions',\n",
              "  'ssic_desc': 'SSIC 2020 Title'},\n",
              " 'dir': {'activity_embedding_fp': 'embeddings/activity_embedding.npy',\n",
              "  'evaluator_fp': 'data/output/evaluator.csv',\n",
              "  'fs_dir': 'data/fs',\n",
              "  'l2_l3_embeddings_fp': 'embeddings/l2_l3_embeddings.npy',\n",
              "  'l2_l3_embeddings_others_fp': 'embeddings/l2_l3_embeddings_others.npy',\n",
              "  'l3_list_fp': 'data/l3_list.json',\n",
              "  'l3_list_others_fp': 'data/l3_list_others.json',\n",
              "  'label_fp': 'data/Company SSIC Code.csv',\n",
              "  'model_location': 'model',\n",
              "  'output_fp': 'data/output/output.csv',\n",
              "  'ssic_df_fp': 'data/ssic2020-detailed-definitions.xlsx',\n",
              "  'tokenizer_location': 'tokenizer'},\n",
              " 'evaluation_mode': True,\n",
              " 'google_model_name': 'text-bison',\n",
              " 'project_id': 'double-insight-395609',\n",
              " 'summarizer_model_name': 'text-bison',\n",
              " 'text_embedding_model_name': 'textembedding-gecko'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedder = TextEmbedder(config[\"text_embedding_model_name\"],\n",
        "                             config[\"project_id\"],\n",
        "                             config[\"bucket_name\"],\n",
        "                             from_local = False,\n",
        "                             model_location = config[\"dir\"][\"model_location\"],\n",
        "                             tokenizer_location = config[\"dir\"][\"tokenizer_location\"]\n",
        "                             )"
      ],
      "metadata": {
        "id": "IfcyzNn7iD_7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1704938431705,
          "user_tz": -480,
          "elapsed": 26203,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "IfcyzNn7iD_7",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ssic = SSICProcessor(text_embedder, fileloader, config)\n",
        "ssic.construct_ssic_lists()\n",
        "ssic.construct_ssic_embeddings()"
      ],
      "metadata": {
        "id": "D7BZCWyYiKcr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1704938552519,
          "user_tz": -480,
          "elapsed": 120830,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fc3487-41ef-4a1b-b667-11fea00e52fc"
      },
      "id": "D7BZCWyYiKcr",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of 5-digit SSIC Codes (non-others):  624\n",
            "Total number of 5-digit SSIC Codes (others):  399\n",
            "Constructed SSIC Artifacts.\n",
            "Embeddings ((1, 768)) uploaded to gs://acra-ssic-classification/embeddings/activity_embedding.npy\n",
            "Generating Embeddings for non-others descriptors...\n",
            "Embeddings ((624, 768)) uploaded to gs://acra-ssic-classification/embeddings/l2_l3_embeddings.npy\n",
            "Generating Embeddings for Others descriptors...\n",
            "Embeddings ((399, 768)) uploaded to gs://acra-ssic-classification/embeddings/l2_l3_embeddings_others.npy\n",
            "Constructed Embeddings.\n"
          ]
        }
      ]
    }
  ]
}