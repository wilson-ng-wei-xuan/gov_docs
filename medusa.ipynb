{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/ctlllll/axolotl.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR-9FMF2iLpJ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1728541657251,
          "user_tz": -480,
          "elapsed": 1487,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "dffcb018-0a83-42e5-81e1-af6a9a9c2a2c"
      },
      "id": "dR-9FMF2iLpJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'axolotl'...\n",
            "remote: Enumerating objects: 6023, done.\u001b[K\n",
            "remote: Total 6023 (delta 0), reused 0 (delta 0), pack-reused 6023 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6023/6023), 2.45 MiB | 26.92 MiB/s, done.\n",
            "Resolving deltas: 100% (3878/3878), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cXFvtt8HR5Z",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730952253765,
          "user_tz": -480,
          "elapsed": 3426,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "8ba29ec1-c273-474b-bfd3-2f851afea9a7"
      },
      "id": "-cXFvtt8HR5Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch \\\n",
        "auto-gptq \\\n",
        "packaging \\\n",
        "peft==0.13.0 \\\n",
        "bitsandbytes>=0.41.1 \\\n",
        "accelerate==0.34.2 \\\n",
        "deepspeed \\\n",
        "addict \\\n",
        "fire \\\n",
        "PyYAML \\\n",
        "datasets \\\n",
        "sentencepiece \\\n",
        "wandb \\\n",
        "einops \\\n",
        "xformers>=0.0.22 \\\n",
        "optimum \\\n",
        "hf_transfer \\\n",
        "colorama \\\n",
        "numba \\\n",
        "numpy \\\n",
        "bert-score \\\n",
        "evaluate \\\n",
        "rouge-score \\\n",
        "scipy \\\n",
        "scikit-learn \\\n",
        "pynvml \\\n",
        "art \\\n",
        "fschat"
      ],
      "metadata": {
        "id": "W03iA7u4RL67"
      },
      "id": "W03iA7u4RL67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers>=0.0.22"
      ],
      "metadata": {
        "id": "I7N6upUHZxeE"
      },
      "id": "I7N6upUHZxeE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_1hT7KzFNOk",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730958789916,
          "user_tz": -480,
          "elapsed": 6512558,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "cbd55efb-2c22-4900-aba0-85d6b396fd08"
      },
      "id": "H_1hT7KzFNOk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.6.3.tar.gz (2.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=186087206 sha256=473ffc1f7f5f552b64b2f85fbde7b78953100f51839959dae808b726e0e29886\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/axolotl.zip -d /content/axolotl\n",
        "\n",
        "#!zip -r axolotl.zip /content/axolotl/\n",
        "#from google.colab import files\n",
        "#files.download('axolotl.zip')"
      ],
      "metadata": {
        "id": "BKdY0XrU8G5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730898076801,
          "user_tz": -480,
          "elapsed": 244,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "0b14b096-a32c-4452-c99d-32a46c9dffa3"
      },
      "id": "BKdY0XrU8G5_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/axolotl.zip\n",
            "   creating: /content/axolotl/common/\n",
            "  inflating: /content/axolotl/common/cli.py  \n",
            "  inflating: /content/axolotl/common/const.py  \n",
            "  inflating: /content/axolotl/common/__init__.py  \n",
            "   creating: /content/axolotl/common/__pycache__/\n",
            "  inflating: /content/axolotl/common/__pycache__/cli.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/common/__pycache__/const.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/common/__pycache__/__init__.cpython-310.pyc  \n",
            "   creating: /content/axolotl/core/\n",
            "  inflating: /content/axolotl/core/trainer_builder.py  \n",
            "  inflating: /content/axolotl/core/__init__.py  \n",
            "   creating: /content/axolotl/core/__pycache__/\n",
            "  inflating: /content/axolotl/core/__pycache__/trainer_builder.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/core/__pycache__/__init__.cpython-310.pyc  \n",
            "   creating: /content/axolotl/models/\n",
            "   creating: /content/axolotl/models/phi/\n",
            "  inflating: /content/axolotl/models/phi/configuration_mixformer_sequential.py  \n",
            "  inflating: /content/axolotl/models/phi/modeling_mixformer_sequential.py  \n",
            "  inflating: /content/axolotl/models/phi/__init__.py  \n",
            "  inflating: /content/axolotl/models/__init__.py  \n",
            "   creating: /content/axolotl/monkeypatch/\n",
            "  inflating: /content/axolotl/monkeypatch/btlm_attn_hijack_flash.py  \n",
            "  inflating: /content/axolotl/monkeypatch/fastchat_conversation_turns.py  \n",
            "  inflating: /content/axolotl/monkeypatch/llama_attn_hijack_flash.py  \n",
            "  inflating: /content/axolotl/monkeypatch/llama_attn_hijack_sdp.py  \n",
            "  inflating: /content/axolotl/monkeypatch/llama_attn_hijack_xformers.py  \n",
            "  inflating: /content/axolotl/monkeypatch/llama_expand_mask.py  \n",
            "  inflating: /content/axolotl/monkeypatch/llama_landmark_attn.py  \n",
            "  inflating: /content/axolotl/monkeypatch/medusa_utils.py  \n",
            "  inflating: /content/axolotl/monkeypatch/mistral_attn_hijack_flash.py  \n",
            "  inflating: /content/axolotl/monkeypatch/neft_embeddings.py  \n",
            "  inflating: /content/axolotl/monkeypatch/relora.py  \n",
            "  inflating: /content/axolotl/monkeypatch/stablelm_attn_hijack_flash.py  \n",
            "  inflating: /content/axolotl/monkeypatch/utils.py  \n",
            "  inflating: /content/axolotl/monkeypatch/xpos_rope_llama_monkey_patch.py  \n",
            "   creating: /content/axolotl/monkeypatch/__pycache__/\n",
            "  inflating: /content/axolotl/monkeypatch/__pycache__/fastchat_conversation_turns.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/monkeypatch/__pycache__/llama_attn_hijack_flash.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/monkeypatch/__pycache__/llama_expand_mask.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/monkeypatch/__pycache__/medusa_utils.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/monkeypatch/__pycache__/neft_embeddings.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/monkeypatch/__pycache__/relora.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/monkeypatch/__pycache__/utils.cpython-310.pyc  \n",
            "   creating: /content/axolotl/prompt_strategies/\n",
            "  inflating: /content/axolotl/prompt_strategies/alpaca_chat.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/alpaca_instruct.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/alpaca_w_system.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/completion.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/context_qa.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/creative_acr.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/llama2_chat.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/metharme.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/orcamini.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/pygmalion.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/sharegpt.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/sharegpt_jokes.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/user_defined.py  \n",
            "  inflating: /content/axolotl/prompt_strategies/__init__.py  \n",
            "   creating: /content/axolotl/prompt_strategies/__pycache__/\n",
            "  inflating: /content/axolotl/prompt_strategies/__pycache__/alpaca_w_system.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/prompt_strategies/__pycache__/sharegpt.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/prompt_strategies/__pycache__/user_defined.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/prompt_strategies/__pycache__/__init__.cpython-310.pyc  \n",
            "   creating: /content/axolotl/utils/\n",
            "  inflating: /content/axolotl/utils/bench.py  \n",
            "  inflating: /content/axolotl/utils/callbacks.py  \n",
            "  inflating: /content/axolotl/utils/collators.py  \n",
            "  inflating: /content/axolotl/utils/config.py  \n",
            "  inflating: /content/axolotl/utils/data.py  \n",
            "  inflating: /content/axolotl/utils/dataloader.py  \n",
            "  inflating: /content/axolotl/utils/dict.py  \n",
            "  inflating: /content/axolotl/utils/distributed.py  \n",
            "  inflating: /content/axolotl/utils/models.py  \n",
            "  inflating: /content/axolotl/utils/schedulers.py  \n",
            "  inflating: /content/axolotl/utils/tokenization.py  \n",
            "  inflating: /content/axolotl/utils/trainer.py  \n",
            "  inflating: /content/axolotl/utils/wandb_.py  \n",
            "  inflating: /content/axolotl/utils/__init__.py  \n",
            "   creating: /content/axolotl/utils/__pycache__/\n",
            "  inflating: /content/axolotl/utils/__pycache__/bench.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/callbacks.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/collators.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/config.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/data.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/dataloader.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/dict.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/distributed.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/models.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/schedulers.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/tokenization.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/trainer.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/wandb_.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/utils/__pycache__/__init__.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/__init__.py  \n",
            "  inflating: /content/axolotl/convert.py  \n",
            "  inflating: /content/axolotl/datasets.py  \n",
            "  inflating: /content/axolotl/logging_config.py  \n",
            "  inflating: /content/axolotl/prompt_tokenizers.py  \n",
            "  inflating: /content/axolotl/prompters.py  \n",
            "  inflating: /content/axolotl/train.py  \n",
            "   creating: /content/axolotl/__pycache__/\n",
            "  inflating: /content/axolotl/__pycache__/datasets.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/__pycache__/logging_config.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/__pycache__/prompters.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/__pycache__/prompt_tokenizers.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/__pycache__/train.cpython-310.pyc  \n",
            "  inflating: /content/axolotl/__pycache__/__init__.cpython-310.pyc  \n",
            "   creating: /content/axolotl/cli/\n",
            "  inflating: /content/axolotl/cli/inference.py  \n",
            "  inflating: /content/axolotl/cli/merge_lora.py  \n",
            "  inflating: /content/axolotl/cli/preprocess.py  \n",
            "  inflating: /content/axolotl/cli/shard.py  \n",
            "  inflating: /content/axolotl/cli/train.py  \n",
            "  inflating: /content/axolotl/cli/__init__.py  \n",
            "   creating: /content/axolotl/cli/__pycache__/\n",
            "  inflating: /content/axolotl/cli/__pycache__/__init__.cpython-310.pyc  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fschat==0.2.24 colorama art addict datasets==2.15.0"
      ],
      "metadata": {
        "id": "BYkKhFclRkkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730952276509,
          "user_tz": -480,
          "elapsed": 4633,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "18f5fb9c-80b2-4eb6-8e5d-f69973358937"
      },
      "id": "BYkKhFclRkkk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fschat==0.2.24\n",
            "  Using cached fschat-0.2.24-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "Requirement already satisfied: art in /usr/local/lib/python3.10/dist-packages (6.3)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: datasets==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (0.115.4)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (0.27.2)\n",
            "Requirement already satisfied: markdown2[all] in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (2.5.1)\n",
            "Requirement already satisfied: nh3 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (0.2.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (1.26.4)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (3.0.48)\n",
            "Collecting pydantic<2,>=1 (from fschat==0.2.24)\n",
            "  Downloading pydantic-1.10.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.6/152.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (2.32.3)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (13.9.3)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (1.0.13)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (0.8.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.24) (0.32.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.0->fschat==0.2.24) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.24) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.24) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.24) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.24) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.24) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.24) (2.18.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->fschat==0.2.24) (0.41.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.24) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.24) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.24) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->fschat==0.2.24) (0.14.0)\n",
            "Requirement already satisfied: wavedrom in /usr/local/lib/python3.10/dist-packages (from markdown2[all]->fschat==0.2.24) (2.0.3.post3)\n",
            "Requirement already satisfied: latex2mathml in /usr/local/lib/python3.10/dist-packages (from markdown2[all]->fschat==0.2.24) (3.77.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0) (2024.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->fschat==0.2.24) (2024.9.11)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->fschat==0.2.24) (8.1.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.24) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->fschat==0.2.24) (1.2.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.10/dist-packages (from wavedrom->markdown2[all]->fschat==0.2.24) (1.4.3)\n",
            "Using cached fschat-0.2.24-py3-none-any.whl (189 kB)\n",
            "Downloading pydantic-1.10.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic, fschat\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.9.2\n",
            "    Uninstalling pydantic-2.9.2:\n",
            "      Successfully uninstalled pydantic-2.9.2\n",
            "  Attempting uninstall: fschat\n",
            "    Found existing installation: fschat 0.2.36\n",
            "    Uninstalling fschat-0.2.36:\n",
            "      Successfully uninstalled fschat-0.2.36\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.15 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\n",
            "deepspeed 0.15.3 requires pydantic>=2.0.0, but you have pydantic 1.10.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fschat-0.2.24 pydantic-1.10.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.35.2"
      ],
      "metadata": {
        "id": "zypFWVx1ORQp"
      },
      "id": "zypFWVx1ORQp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from IPython.display import display\n",
        "display(transformers.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "hvpYwo5zjtKH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729767899400,
          "user_tz": -480,
          "elapsed": 722,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "6e648258-e6f7-48ef-bb73-0eb002059e46"
      },
      "id": "hvpYwo5zjtKH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'4.35.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "kKdeBPY5T-hp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730888446186,
          "user_tz": -480,
          "elapsed": 134,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5b2927d9-c19e-4ad5-cb22-7095b1bb5983"
      },
      "id": "kKdeBPY5T-hp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "id": "jiq6dEErf4tU"
      },
      "id": "jiq6dEErf4tU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.stdout = open(os.devnull, 'w')  # Redirect stdout\n",
        "sys.stdout = sys.__stdout__         # Restore stdout to default\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Save original stderr\n",
        "original_stderr = sys.stderr\n",
        "sys.stderr = open(os.devnull, 'w')  # Redirect stderr\n",
        "\n",
        "# Import your libraries here\n",
        "from transformers.integrations import is_deepspeed_zero3_enabled\n"
      ],
      "metadata": {
        "id": "MJv1nc2Zj6nx"
      },
      "id": "MJv1nc2Zj6nx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from axolotl.cli import check_accelerate_default_config\n",
        "\n",
        "from axolotl.train import train\n",
        "\n",
        "from axolotl.cli import check_user_token\n",
        "from axolotl.cli import load_cfg\n",
        "from axolotl.cli import load_datasets\n",
        "from axolotl.cli import print_axolotl_text_art\n",
        "from axolotl.common.cli import TrainerCliArgs\n"
      ],
      "metadata": {
        "id": "h6QOHkTLYvGj"
      },
      "id": "h6QOHkTLYvGj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import shutil\n",
        "\n",
        "# Specify the folder or file to be zipped\n",
        "shutil.make_archive('content/axolotl', 'zip', 'axolotl')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Download the zipped file\n",
        "files.download('content/axolotl.zip')\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Qfunwj8hEjVT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729245663781,
          "user_tz": -480,
          "elapsed": 93,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "cc221954-176d-46cc-b598-862a87ea40c1"
      },
      "id": "Qfunwj8hEjVT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_518bcce5-e83b-444c-a379-499ac69de5d2\", \"axolotl.zip\", 242175)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from transformers import HfArgumentParser\n",
        "\n",
        "def create_data(config_path=\"examples/\", **kwargs):\n",
        "    # Simulate config input from the notebook\n",
        "    config = Path(config_path)\n",
        "\n",
        "    # Load config as you would in the CLI\n",
        "    parsed_cfg = load_cfg(config, **kwargs)\n",
        "\n",
        "    # Check necessary setups\n",
        "    check_accelerate_default_config()\n",
        "    check_user_token()\n",
        "\n",
        "    # Directly provide CLI arguments instead of parsing from CLI\n",
        "    cli_args = TrainerCliArgs(\n",
        "        # Pass any necessary arguments here\n",
        "    )\n",
        "\n",
        "    # In a CLI environment, HfArgumentParser parses arguments from the command line.\n",
        "    # Here, simulate the parsed CLI args manually if needed\n",
        "    parsed_cli_args = cli_args\n",
        "\n",
        "    # Load datasets with config and manually provided CLI arguments\n",
        "    dataset_meta = load_datasets(cfg=parsed_cfg, cli_args=parsed_cli_args)\n",
        "\n",
        "    return parsed_cfg, parsed_cli_args, dataset_meta\n"
      ],
      "metadata": {
        "id": "qdjtqH2wbceU"
      },
      "id": "qdjtqH2wbceU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage in a notebook cell\n",
        "parsed_cfg, parsed_cli_args, dataset_meta = create_data(config_path=\"vicuna_13b_qlora_stage2.yml\")\n",
        "parsed_cfg.pop('load_in_8bit')\n",
        "parsed_cfg[\"device_map\"] = \"cuda:0\"\n",
        "parsed_cfg"
      ],
      "metadata": {
        "id": "AE8CJqSMcnCE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8a0dc9577a0d469cb5e65c252266acf0",
            "55542feba7c34916b5e340bc3726580f",
            "c38543f18d574142ad67b20e341ab09f",
            "a2c52e45335f44cdac88086bebb1e50e",
            "d5b0b9924cd846999a8e443b5bf4a42d",
            "7f35cb8198a241c08c81975327986016",
            "6dd7e020e2c14322ad95c203d1527e8c",
            "03c5642367524b308f0078e936908d94",
            "4befcf3c3e324b5e9f11cee5a93bdd35",
            "c61da1fb04624f8b97d071e85588c0a4",
            "08550ec1cf2c4be788459a3730791372"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731250987389,
          "user_tz": -480,
          "elapsed": 2187,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "6cce11ff-49a2-4cc3-d614-f817a1444167"
      },
      "id": "AE8CJqSMcnCE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a0dc9577a0d469cb5e65c252266acf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'base_model': 'lmsys/vicuna-7b-v1.5',\n",
              " 'base_model_config': 'lmsys/vicuna-7b-v1.5',\n",
              " 'model_type': 'LlamaForCausalLM',\n",
              " 'tokenizer_type': 'LlamaTokenizer',\n",
              " 'is_llama_derived_model': True,\n",
              " 'load_in_4bit': True,\n",
              " 'strict': False,\n",
              " 'datasets': [{'path': 'hansard_answered_questions_llama3_formatted_train.json',\n",
              "   'type': 'sharegpt'}],\n",
              " 'dataset_prepared_path': None,\n",
              " 'val_set_size': 0.1,\n",
              " 'output_dir': './vicuna_7b_qlora_stage2',\n",
              " 'adapter': 'qlora',\n",
              " 'lora_model_dir': None,\n",
              " 'lora_r': 32,\n",
              " 'lora_alpha': 16,\n",
              " 'lora_dropout': 0.1,\n",
              " 'lora_target_modules': ['gate_proj',\n",
              "  'down_proj',\n",
              "  'up_proj',\n",
              "  'q_proj',\n",
              "  'v_proj',\n",
              "  'k_proj',\n",
              "  'o_proj',\n",
              "  'lm_head'],\n",
              " 'lora_target_linear': None,\n",
              " 'lora_fan_in_fan_out': None,\n",
              " 'lora_modules_to_save': ['embed_tokens', 'lm_head'],\n",
              " 'sequence_len': 4096,\n",
              " 'sample_packing': True,\n",
              " 'pad_to_sequence_len': True,\n",
              " 'wandb_project': None,\n",
              " 'wandb_entity': None,\n",
              " 'wandb_watch': None,\n",
              " 'wandb_run_id': None,\n",
              " 'wandb_log_model': None,\n",
              " 'gradient_accumulation_steps': 4,\n",
              " 'micro_batch_size': 4,\n",
              " 'num_epochs': 5,\n",
              " 'optimizer': 'adamw_bnb_8bit',\n",
              " 'lr_scheduler': 'cosine',\n",
              " 'learning_rate': 0.0001,\n",
              " 'train_on_inputs': False,\n",
              " 'group_by_length': False,\n",
              " 'bf16': True,\n",
              " 'fp16': False,\n",
              " 'tf32': False,\n",
              " 'gradient_checkpointing': True,\n",
              " 'early_stopping_patience': None,\n",
              " 'resume_from_checkpoint': None,\n",
              " 'local_rank': 0,\n",
              " 'logging_steps': 1,\n",
              " 'xformers_attention': None,\n",
              " 'flash_attention': True,\n",
              " 'warmup_steps': 40,\n",
              " 'eval_steps': 40,\n",
              " 'save_steps': None,\n",
              " 'save_total_limit': 1,\n",
              " 'debug': None,\n",
              " 'deepspeed': None,\n",
              " 'weight_decay': 0.0,\n",
              " 'fsdp': None,\n",
              " 'fsdp_config': None,\n",
              " 'special_tokens': {'bos_token': '<s>',\n",
              "  'eos_token': '</s>',\n",
              "  'unk_token': '<unk>'},\n",
              " 'medusa_num_heads': 5,\n",
              " 'medusa_num_layers': 1,\n",
              " 'medusa_heads_coefficient': 0.2,\n",
              " 'medusa_decay_coefficient': 0.8,\n",
              " 'medusa_logging': False,\n",
              " 'medusa_scheduler': 'constant',\n",
              " 'medusa_lr_multiplier': 4.0,\n",
              " 'axolotl_config_path': PosixPath('vicuna_13b_qlora_stage2.yml'),\n",
              " 'batch_size': 16,\n",
              " 'eval_batch_size': 4,\n",
              " 'world_size': 1,\n",
              " 'eval_table_size': 0,\n",
              " 'eval_table_max_new_tokens': 128,\n",
              " 'device': 'cuda:0',\n",
              " 'device_map': 'cuda:0',\n",
              " 'ddp': False,\n",
              " 'torch_dtype': torch.bfloat16,\n",
              " 'dataset_processes': 12,\n",
              " 'model_config_type': 'llama',\n",
              " 'is_falcon_derived_model': False,\n",
              " 'is_mistral_derived_model': False,\n",
              " 'medusa_only_heads': False,\n",
              " 'medusa_num_unfreeze_layers': 0,\n",
              " 'medusa_distillation_regularization': 0.0,\n",
              " 'medusa_self_distillation': False,\n",
              " 'total_num_tokens': 2273934,\n",
              " 'total_supervised_tokens': 191005,\n",
              " 'sample_packing_eff_est': 0.96}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import json\n",
        "with open('ShareGPT_V3_unfiltered_cleaned_split.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "final_d= []\n",
        "for idx, d in enumerate(data):\n",
        "    if (len(d['conversations']) > 1) and idx<50:\n",
        "        final_d.append(data)\n",
        "\n",
        "# Save the dictionary as a JSON file\n",
        "with open('gpt.json', 'w') as json_file:\n",
        "    json.dump(final_d, json_file, indent=4)  # indent for pretty printing\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "import logging\n",
        "\n",
        "# Configure logging and force to override any previous handlers\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    force=True)  # force=True overrides previous logging handlers\n",
        "\n",
        "logging.info('This is an info message')\n",
        "logging.debug('This is a debug message')\n",
        "logging.warning('This is a warning')\n",
        "\n",
        "from IPython.display import display\n",
        "display(\"Hello from script2!\")\n",
        "\n",
        "import sys\n",
        "sys.stdout = sys.__stdout__\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4Goem_SIhezz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1728994285996,
          "user_tz": -480,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1b5b6ff7-6dbe-400a-9cc4-f3cc020ecd79"
      },
      "id": "4Goem_SIhezz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport json\\nwith open('ShareGPT_V3_unfiltered_cleaned_split.json', 'r') as f:\\n    data = json.load(f)\\nfinal_d= []\\nfor idx, d in enumerate(data):\\n    if (len(d['conversations']) > 1) and idx<50:\\n        final_d.append(data)\\n\\n# Save the dictionary as a JSON file\\nwith open('gpt.json', 'w') as json_file:\\n    json.dump(final_d, json_file, indent=4)  # indent for pretty printing\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "bRuz5f_ve-gt"
      },
      "id": "bRuz5f_ve-gt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "accelerator.free_memory()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "g3O4HopNfexc"
      },
      "id": "g3O4HopNfexc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(cfg=parsed_cfg, cli_args=parsed_cli_args, dataset_meta=dataset_meta)"
      ],
      "metadata": {
        "id": "a36yyQZwkHjp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "facd529aad4e40a5980496e1b51b266a",
            "4e79652d2ab941fba60ccf2f107cfb54",
            "e62a773e9e1040e0a514197f6ea48ed5",
            "72e060e0cfbb4d108c2a8a0a53f4836e",
            "cde4d720760e41e9843fcb240151d905",
            "5792ac2b58e0458491588d5cb7d0675d",
            "a568d1ab0eaa454da3fd683fd303a467",
            "bb67298042164657b4cd25302de9bdf2",
            "dd83c13102334183b37b499344d82620",
            "7c73c5954684475eb095751d67a475a3",
            "e17de4d4687e4226851a80a88d894f15"
          ]
        },
        "outputId": "359f3bdd-c11d-4d91-e176-1515326ff7e2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731255442090,
          "user_tz": -480,
          "elapsed": 4446323,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "a36yyQZwkHjp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "facd529aad4e40a5980496e1b51b266a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [175/175 1:12:36, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.742500</td>\n",
              "      <td>3.020306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.707600</td>\n",
              "      <td>2.685841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.073000</td>\n",
              "      <td>2.729794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>2.722976</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PeftModelForCausalLM(\n",
              "   (base_model): LoraModel(\n",
              "     (model): LlamaForCausalLM(\n",
              "       (model): LlamaModel(\n",
              "         (embed_tokens): ModulesToSaveWrapper(\n",
              "           (original_module): Embedding(32000, 4096, padding_idx=0)\n",
              "           (modules_to_save): ModuleDict(\n",
              "             (default): Embedding(32000, 4096, padding_idx=0)\n",
              "           )\n",
              "         )\n",
              "         (layers): ModuleList(\n",
              "           (0-31): 32 x LlamaDecoderLayer(\n",
              "             (self_attn): LlamaAttention(\n",
              "               (q_proj): lora.Linear4bit(\n",
              "                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                 (lora_dropout): ModuleDict(\n",
              "                   (default): Dropout(p=0.1, inplace=False)\n",
              "                 )\n",
              "                 (lora_A): ModuleDict(\n",
              "                   (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                 )\n",
              "                 (lora_B): ModuleDict(\n",
              "                   (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                 )\n",
              "                 (lora_embedding_A): ParameterDict()\n",
              "                 (lora_embedding_B): ParameterDict()\n",
              "                 (lora_magnitude_vector): ModuleDict()\n",
              "               )\n",
              "               (k_proj): lora.Linear4bit(\n",
              "                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                 (lora_dropout): ModuleDict(\n",
              "                   (default): Dropout(p=0.1, inplace=False)\n",
              "                 )\n",
              "                 (lora_A): ModuleDict(\n",
              "                   (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                 )\n",
              "                 (lora_B): ModuleDict(\n",
              "                   (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                 )\n",
              "                 (lora_embedding_A): ParameterDict()\n",
              "                 (lora_embedding_B): ParameterDict()\n",
              "                 (lora_magnitude_vector): ModuleDict()\n",
              "               )\n",
              "               (v_proj): lora.Linear4bit(\n",
              "                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                 (lora_dropout): ModuleDict(\n",
              "                   (default): Dropout(p=0.1, inplace=False)\n",
              "                 )\n",
              "                 (lora_A): ModuleDict(\n",
              "                   (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                 )\n",
              "                 (lora_B): ModuleDict(\n",
              "                   (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                 )\n",
              "                 (lora_embedding_A): ParameterDict()\n",
              "                 (lora_embedding_B): ParameterDict()\n",
              "                 (lora_magnitude_vector): ModuleDict()\n",
              "               )\n",
              "               (o_proj): lora.Linear4bit(\n",
              "                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                 (lora_dropout): ModuleDict(\n",
              "                   (default): Dropout(p=0.1, inplace=False)\n",
              "                 )\n",
              "                 (lora_A): ModuleDict(\n",
              "                   (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                 )\n",
              "                 (lora_B): ModuleDict(\n",
              "                   (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                 )\n",
              "                 (lora_embedding_A): ParameterDict()\n",
              "                 (lora_embedding_B): ParameterDict()\n",
              "                 (lora_magnitude_vector): ModuleDict()\n",
              "               )\n",
              "               (rotary_emb): LlamaRotaryEmbedding()\n",
              "               (v_pro): lora.Linear4bit(\n",
              "                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                 (lora_dropout): ModuleDict(\n",
              "                   (default): Dropout(p=0.1, inplace=False)\n",
              "                 )\n",
              "                 (lora_A): ModuleDict(\n",
              "                   (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                 )\n",
              "                 (lora_B): ModuleDict(\n",
              "                   (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                 )\n",
              "                 (lora_embedding_A): ParameterDict()\n",
              "                 (lora_embedding_B): ParameterDict()\n",
              "                 (lora_magnitude_vector): ModuleDict()\n",
              "               )\n",
              "             )\n",
              "             (mlp): LlamaMLP(\n",
              "               (gate_proj): lora.Linear4bit(\n",
              "                 (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "                 (lora_dropout): ModuleDict(\n",
              "                   (default): Dropout(p=0.1, inplace=False)\n",
              "                 )\n",
              "                 (lora_A): ModuleDict(\n",
              "                   (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                 )\n",
              "                 (lora_B): ModuleDict(\n",
              "                   (default): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                 )\n",
              "                 (lora_embedding_A): ParameterDict()\n",
              "                 (lora_embedding_B): ParameterDict()\n",
              "                 (lora_magnitude_vector): ModuleDict()\n",
              "               )\n",
              "               (up_proj): lora.Linear4bit(\n",
              "                 (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "                 (lora_dropout): ModuleDict(\n",
              "                   (default): Dropout(p=0.1, inplace=False)\n",
              "                 )\n",
              "                 (lora_A): ModuleDict(\n",
              "                   (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                 )\n",
              "                 (lora_B): ModuleDict(\n",
              "                   (default): Linear(in_features=32, out_features=11008, bias=False)\n",
              "                 )\n",
              "                 (lora_embedding_A): ParameterDict()\n",
              "                 (lora_embedding_B): ParameterDict()\n",
              "                 (lora_magnitude_vector): ModuleDict()\n",
              "               )\n",
              "               (down_proj): lora.Linear4bit(\n",
              "                 (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "                 (lora_dropout): ModuleDict(\n",
              "                   (default): Dropout(p=0.1, inplace=False)\n",
              "                 )\n",
              "                 (lora_A): ModuleDict(\n",
              "                   (default): Linear(in_features=11008, out_features=32, bias=False)\n",
              "                 )\n",
              "                 (lora_B): ModuleDict(\n",
              "                   (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                 )\n",
              "                 (lora_embedding_A): ParameterDict()\n",
              "                 (lora_embedding_B): ParameterDict()\n",
              "                 (lora_magnitude_vector): ModuleDict()\n",
              "               )\n",
              "               (act_fn): SiLUActivation()\n",
              "             )\n",
              "             (input_layernorm): LlamaRMSNorm()\n",
              "             (post_attention_layernorm): LlamaRMSNorm()\n",
              "           )\n",
              "         )\n",
              "         (norm): LlamaRMSNorm()\n",
              "       )\n",
              "       (lm_head): ModulesToSaveWrapper(\n",
              "         (original_module): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "         (modules_to_save): ModuleDict(\n",
              "           (default): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "         )\n",
              "       )\n",
              "       (medusa_head): ModuleList(\n",
              "         (0-4): 5 x ModulesToSaveWrapper(\n",
              "           (original_module): Sequential(\n",
              "             (0): ResBlock(\n",
              "               (linear): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "               (act): SiLU()\n",
              "             )\n",
              "             (1): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "           )\n",
              "           (modules_to_save): ModuleDict(\n",
              "             (default): Sequential(\n",
              "               (0): ResBlock(\n",
              "                 (linear): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "                 (act): SiLU()\n",
              "               )\n",
              "               (1): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "             )\n",
              "           )\n",
              "         )\n",
              "       )\n",
              "     )\n",
              "   )\n",
              " ),\n",
              " LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=4096, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
              " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " })"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "# Initialize the client\n",
        "client = storage.Client()\n",
        "bucket_name = 'medusa-decoding'\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "import os\n",
        "\n",
        "def upload_folder_to_gcs(folder_name, destination_blob_name):\n",
        "    \"\"\"\n",
        "    Uploads a folder to Google Cloud Storage.\n",
        "\n",
        "    Args:\n",
        "    folder_name: str, path to the local folder to upload.\n",
        "    destination_blob_name: str, destination folder name in GCS.\n",
        "    \"\"\"\n",
        "    for dirpath, _, filenames in os.walk(folder_name):\n",
        "        for filename in filenames:\n",
        "            local_path = os.path.join(dirpath, filename)\n",
        "            # Create the relative path for GCS\n",
        "            relative_path = os.path.relpath(local_path, folder_name)\n",
        "            blob_path = os.path.join(destination_blob_name, relative_path)\n",
        "\n",
        "            # Upload the file to GCS\n",
        "            blob = bucket.blob(blob_path)\n",
        "            blob.upload_from_filename(local_path)\n",
        "            print(f'Uploaded {local_path} to gs://{bucket_name}/{blob_path}')\n",
        "\n",
        "# Replace with your local folder path and desired destination in GCS\n",
        "local_folder = './vicuna_7b_qlora_stage2/'\n",
        "gcs_destination = 'moodel-7b-2'\n",
        "\n",
        "upload_folder_to_gcs(local_folder, gcs_destination)"
      ],
      "metadata": {
        "id": "2DRsVmpu6cdM"
      },
      "id": "2DRsVmpu6cdM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "x0KpyO2ePPp5"
      },
      "id": "x0KpyO2ePPp5"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/FasterDecoding/Medusa.git"
      ],
      "metadata": {
        "id": "GMhP5Anatn07"
      },
      "id": "GMhP5Anatn07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp fastapi httpx markdown2[all] nh3 numpy prompt_toolkit==3.0.0 pydantic==2.0.0 pydantic-settings psutil requests rich==10.0.0 shortuuid tiktoken uvicorn\n",
        "!pip install gradio==4.10\n",
        "#!pip install accelerate==0.21 peft sentencepiece torch transformers==4.34.1 protobuf\n",
        "#!pip install einops flash-attn>=2.0 wandb\n",
        "!pip install black==23.3.0 pylint==2.8.2\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "tFlaOGAutoaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730996279508,
          "user_tz": -480,
          "elapsed": 17092,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "05064117-3e47-4441-9f2b-a7d1783d31f5"
      },
      "id": "tFlaOGAutoaU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ccelerate (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.10)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.4)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: nh3 in /usr/local/lib/python3.10/dist-packages (0.2.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting prompt_toolkit==3.0.0\n",
            "  Using cached prompt_toolkit-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pydantic==2.0.0\n",
            "  Using cached pydantic-2.0-py3-none-any.whl.metadata (117 kB)\n",
            "Collecting pydantic-settings\n",
            "  Using cached pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting rich==10.0.0\n",
            "  Using cached rich-10.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.10/dist-packages (1.0.13)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.32.0)\n",
            "Requirement already satisfied: markdown2[all] in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit==3.0.0) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.0.0) (0.7.0)\n",
            "Collecting pydantic-core==2.0.1 (from pydantic==2.0.0)\n",
            "  Using cached pydantic_core-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from rich==10.0.0) (0.4.6)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich==10.0.0)\n",
            "  Using cached commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich==10.0.0) (2.18.0)\n",
            "INFO: pip is looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install pydantic==2.0 and rich==10.0.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    pydantic 2.0 depends on typing-extensions>=4.6.1\n",
            "    rich 10.0.0 depends on typing-extensions<4.0.0 and >=3.7.4\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ccelerate (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gradio==4.10 in /usr/local/lib/python3.10/dist-packages (4.10.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (0.115.4)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==0.7.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (0.7.3)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (0.27.2)\n",
            "Collecting huggingface-hub>=0.19.3 (from gradio==4.10)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (0.0.17)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (6.0.2)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.10) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.10) (0.32.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.3->gradio==4.10) (2023.10.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.3->gradio==4.10) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.10) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.10) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.10) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio==4.10) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio==4.10) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio==4.10) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.10) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.10) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.10) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.10) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.10) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.10) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.10) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.10) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.10) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.10) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.10) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.10) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.10) (13.9.3)\n",
            "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==4.10) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==4.10) (0.41.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==4.10) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==4.10) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==4.10) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==4.10) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==4.10) (1.3.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.10) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.10) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.10) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.10) (0.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.10) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.10) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.10) (2.18.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==4.10) (1.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio==4.10) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio==4.10) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.10) (0.1.2)\n",
            "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ccelerate (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.17.3\n",
            "    Uninstalling huggingface-hub-0.17.3:\n",
            "      Successfully uninstalled huggingface-hub-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.26.2\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ccelerate (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting black==23.3.0\n",
            "  Using cached black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting pylint==2.8.2\n",
            "  Using cached pylint-2.8.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black==23.3.0) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black==23.3.0)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black==23.3.0) (24.1)\n",
            "Collecting pathspec>=0.9.0 (from black==23.3.0)\n",
            "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black==23.3.0) (4.3.6)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black==23.3.0) (2.0.2)\n",
            "Collecting astroid<2.7,>=2.5.6 (from pylint==2.8.2)\n",
            "  Using cached astroid-2.6.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting isort<6,>=4.2.5 (from pylint==2.8.2)\n",
            "  Using cached isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mccabe<0.7,>=0.6 (from pylint==2.8.2)\n",
            "  Using cached mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pylint==2.8.2) (0.10.2)\n",
            "Collecting lazy-object-proxy>=1.4.0 (from astroid<2.7,>=2.5.6->pylint==2.8.2)\n",
            "  Using cached lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting wrapt<1.13,>=1.11 (from astroid<2.7,>=2.5.6->pylint==2.8.2)\n",
            "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=20.0 in /usr/local/lib/python3.10/dist-packages (from astroid<2.7,>=2.5.6->pylint==2.8.2) (69.5.1)\n",
            "Using cached black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "Using cached pylint-2.8.2-py3-none-any.whl (357 kB)\n",
            "Using cached astroid-2.6.6-py3-none-any.whl (231 kB)\n",
            "Using cached isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Using cached lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68 kB)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp310-cp310-linux_x86_64.whl size=71448 sha256=4e1f2e73cb392e05ee6afdb6623147883a2471f87bb82e9671a3b2011b4e7cb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/61/d3/d9e7053100177668fa43216a8082868c55015f8706abd974f2\n",
            "Successfully built wrapt\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ccelerate (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: wrapt, mccabe, pathspec, mypy-extensions, lazy-object-proxy, isort, black, astroid, pylint\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "Successfully installed astroid-2.6.6 black-23.3.0 isort-5.13.2 lazy-object-proxy-1.10.0 mccabe-0.6.1 mypy-extensions-1.0.0 pathspec-0.12.1 pylint-2.8.2 wrapt-1.12.1\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ccelerate (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ccelerate (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bitsandbytes accelerate==0.34.2\n",
        "!pip install colorama"
      ],
      "metadata": {
        "id": "NvYQtLn5tq5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730996282556,
          "user_tz": -480,
          "elapsed": 3052,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fd18a062-1d7c-4e3c-adfa-cef810f6894f"
      },
      "id": "NvYQtLn5tq5d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ccelerate (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ccelerate (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import torch\n",
        "from fastchat.serve.cli import SimpleChatIO, RichChatIO, ProgrammaticChatIO\n",
        "from fastchat.model.model_adapter import get_conversation_template\n",
        "from fastchat.conversation import get_conv_template\n",
        "import json\n",
        "from medusa.model.medusa_model import MedusaModel"
      ],
      "metadata": {
        "id": "4-Qi_AWPtwz0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731323041822,
          "user_tz": -480,
          "elapsed": 17501,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20431158-1760-4112-fc04-6a62feb0f5ab"
      },
      "id": "4-Qi_AWPtwz0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-11 11:04:03,964] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MedusaModel.from_pretrained(\n",
        "    \"./vicuna_7b_qlora_stage2/\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"cuda:0\",\n",
        "\n",
        ")\n",
        "tokenizer = model.get_tokenizer()"
      ],
      "metadata": {
        "id": "VHglFtVrtzF8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "ea3d6fb101bb45ba957fe670193034e3",
            "8c27e181c48448bd86314e2b8144f84b",
            "bc09cc6c430144428f35ec08f4c2032e",
            "2e0836949f904ccbbd683de7681b2843",
            "3cf8b2b91cc844039ff2a08b02c23c85",
            "dd9ab5fe3f1346619ad708f4b181969a",
            "4d0238eaec454bfdb0ad258695bcc51d",
            "f4dc2ea640e544a7bcee521f0e51eb89",
            "cd32fd590db0487486fe6240cca34280",
            "9636b0e17f6c450d8dad0b894442ebe1",
            "c1f896cfb22b4b98aa7fdaa762e6656a"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731323150464,
          "user_tz": -480,
          "elapsed": 108644,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "6b935c90-407c-4c3f-f68a-89978b997570"
      },
      "id": "VHglFtVrtzF8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea3d6fb101bb45ba957fe670193034e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=map_location)\n",
            "Some weights of MedusaModelLlama were not initialized from the model checkpoint at lmsys/vicuna-7b-v1.5 and are newly initialized: ['medusa_head.0.0.linear.weight', 'medusa_head.4.0.linear.weight', 'medusa_head.1.0.linear.weight', 'medusa_head.2.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.3.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.1.1.weight', 'medusa_head.0.0.linear.bias', 'medusa_head.2.0.linear.bias', 'medusa_head.4.0.linear.bias', 'medusa_head.4.1.weight', 'medusa_head.3.0.linear.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.2.1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "Loading adapter weights from ./vicuna_7b_qlora_stage2/ led to unexpected keys not found in the model:  ['model.layers.0.self_attn.v_pro.lora_A.default.weight', 'model.layers.0.self_attn.v_pro.lora_B.default.weight', 'model.layers.1.self_attn.v_pro.lora_A.default.weight', 'model.layers.1.self_attn.v_pro.lora_B.default.weight', 'model.layers.2.self_attn.v_pro.lora_A.default.weight', 'model.layers.2.self_attn.v_pro.lora_B.default.weight', 'model.layers.3.self_attn.v_pro.lora_A.default.weight', 'model.layers.3.self_attn.v_pro.lora_B.default.weight', 'model.layers.4.self_attn.v_pro.lora_A.default.weight', 'model.layers.4.self_attn.v_pro.lora_B.default.weight', 'model.layers.5.self_attn.v_pro.lora_A.default.weight', 'model.layers.5.self_attn.v_pro.lora_B.default.weight', 'model.layers.6.self_attn.v_pro.lora_A.default.weight', 'model.layers.6.self_attn.v_pro.lora_B.default.weight', 'model.layers.7.self_attn.v_pro.lora_A.default.weight', 'model.layers.7.self_attn.v_pro.lora_B.default.weight', 'model.layers.8.self_attn.v_pro.lora_A.default.weight', 'model.layers.8.self_attn.v_pro.lora_B.default.weight', 'model.layers.9.self_attn.v_pro.lora_A.default.weight', 'model.layers.9.self_attn.v_pro.lora_B.default.weight', 'model.layers.10.self_attn.v_pro.lora_A.default.weight', 'model.layers.10.self_attn.v_pro.lora_B.default.weight', 'model.layers.11.self_attn.v_pro.lora_A.default.weight', 'model.layers.11.self_attn.v_pro.lora_B.default.weight', 'model.layers.12.self_attn.v_pro.lora_A.default.weight', 'model.layers.12.self_attn.v_pro.lora_B.default.weight', 'model.layers.13.self_attn.v_pro.lora_A.default.weight', 'model.layers.13.self_attn.v_pro.lora_B.default.weight', 'model.layers.14.self_attn.v_pro.lora_A.default.weight', 'model.layers.14.self_attn.v_pro.lora_B.default.weight', 'model.layers.15.self_attn.v_pro.lora_A.default.weight', 'model.layers.15.self_attn.v_pro.lora_B.default.weight', 'model.layers.16.self_attn.v_pro.lora_A.default.weight', 'model.layers.16.self_attn.v_pro.lora_B.default.weight', 'model.layers.17.self_attn.v_pro.lora_A.default.weight', 'model.layers.17.self_attn.v_pro.lora_B.default.weight', 'model.layers.18.self_attn.v_pro.lora_A.default.weight', 'model.layers.18.self_attn.v_pro.lora_B.default.weight', 'model.layers.19.self_attn.v_pro.lora_A.default.weight', 'model.layers.19.self_attn.v_pro.lora_B.default.weight', 'model.layers.20.self_attn.v_pro.lora_A.default.weight', 'model.layers.20.self_attn.v_pro.lora_B.default.weight', 'model.layers.21.self_attn.v_pro.lora_A.default.weight', 'model.layers.21.self_attn.v_pro.lora_B.default.weight', 'model.layers.22.self_attn.v_pro.lora_A.default.weight', 'model.layers.22.self_attn.v_pro.lora_B.default.weight', 'model.layers.23.self_attn.v_pro.lora_A.default.weight', 'model.layers.23.self_attn.v_pro.lora_B.default.weight', 'model.layers.24.self_attn.v_pro.lora_A.default.weight', 'model.layers.24.self_attn.v_pro.lora_B.default.weight', 'model.layers.25.self_attn.v_pro.lora_A.default.weight', 'model.layers.25.self_attn.v_pro.lora_B.default.weight', 'model.layers.26.self_attn.v_pro.lora_A.default.weight', 'model.layers.26.self_attn.v_pro.lora_B.default.weight', 'model.layers.27.self_attn.v_pro.lora_A.default.weight', 'model.layers.27.self_attn.v_pro.lora_B.default.weight', 'model.layers.28.self_attn.v_pro.lora_A.default.weight', 'model.layers.28.self_attn.v_pro.lora_B.default.weight', 'model.layers.29.self_attn.v_pro.lora_A.default.weight', 'model.layers.29.self_attn.v_pro.lora_B.default.weight', 'model.layers.30.self_attn.v_pro.lora_A.default.weight', 'model.layers.30.self_attn.v_pro.lora_B.default.weight', 'model.layers.31.self_attn.v_pro.lora_A.default.weight', 'model.layers.31.self_attn.v_pro.lora_B.default.weight']. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "prompt = '''Question: Question:Mr Leong Mun Wai asked the Prime Minister (a) since the conclusion of the 2023 Presidential Election, how many non-voters have applied to restore their names to the Registers of Electors; and (b) how many of these applications are successful.\n",
        "\n",
        "Supporting points: **Title: Applications to Restore Names to Registers of Electors**\n",
        "\n",
        "**Executive Summary:**\n",
        "\n",
        "As of the end of January 2024, a total of 101,464 non-voters from the 2023 Presidential Election have successfully applied to restore their names to the Registers of Electors. This report details the background, process, and implications of these applications, with a focus on ensuring continued electoral engagement and upholding democratic principles.\n",
        "\n",
        "**Background:**\n",
        "\n",
        "The 2023 Presidential Election saw a significant number of eligible voters abstaining from casting their votes. Consequently, their names were removed from the Registers of Electors in accordance with established electoral regulations. However, recognizing the importance of inclusive participation in democratic processes, the Elections Department provided a mechanism for non-voters to restore their names to the registers.\n",
        "\n",
        "**Application Process:**\n",
        "\n",
        "The process to restore names to the Registers of Electors was designed to be straightforward and accessible. Non-voters could submit their applications through various channels, including:\n",
        "1. **Online Portal:** A dedicated section on the Elections Department's website allowed individuals to complete and submit their applications electronically.\n",
        "2. **In-Person Submissions:** Local electoral offices across the country were equipped to handle in-person applications, ensuring access for individuals without reliable internet access.\n",
        "3. **Mail-In Applications:** For added convenience, applicants could also restore their names via postal mail by sending completed forms to their respective electoral offices.\n",
        "\n",
        "**Volume of Applications:**\n",
        "\n",
        "By the end of January 2024, a total of 101,464 individuals had applied to have their names restored. This significant number reflects a robust response from the public, underscoring the desire among citizens to re-engage with the electoral process. The breakdown of applications received through different channels is as follows:\n",
        "- **Online Portal:** 67,892 applications\n",
        "- **In-Person Submissions:** 22,710 applications\n",
        "- **Mail-In Applications:** 10,862 applications\n",
        "\n",
        "**Approval and Verification:**\n",
        "\n",
        "All 101,464 applications were meticulously reviewed by the Elections Department. The verification process ensured that each applicant met the criteria for restoration, including:\n",
        "- **Verification of Identity:** Confirming the identity of applicants to prevent fraudulent submissions.\n",
        "- **Eligibility Check:** Ensuring that applicants were eligible voters in the 2023 Presidential Election and had not been disqualified for other reasons.\n",
        "- **Data Accuracy:** Cross-referencing application details with existing records to maintain the integrity of the Registers of Electors.\n",
        "\n",
        "Upon successful verification, all applications were approved, and the names of the applicants were restored to the Registers of Electors. This meticulous approach guarantees the reliability and accuracy of the electoral registers.\n",
        "\n",
        "**Implications:**\n",
        "\n",
        "The successful restoration of 101,464 names to the Registers of Electors has several significant implications:\n",
        "1. **Enhanced Voter Engagement:** The substantial number of restored names reflects a renewed commitment among citizens to participate in future elections, thereby strengthening democratic engagement.\n",
        "2. **Inclusivity:** The accessible application process ensured that individuals from diverse backgrounds had the opportunity to restore their names, promoting inclusivity in the electoral process.\n",
        "3. **Administrative Efficiency:** The Elections Department's efficient handling of the applications demonstrates its capability to manage large volumes of requests while maintaining accuracy and integrity.\n",
        "4. **Public Trust:** The transparent and successful restoration process fosters public trust in the electoral system, reinforcing the legitimacy of future elections.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The successful restoration of 101,464 names to the Registers of Electors is a testament to the commitment of both the Elections Department and the citizens to uphold democratic values. The robust response from non-voters highlights the importance of accessible and inclusive electoral processes. Moving forward, continued efforts to engage and educate the electorate will be crucial in maintaining a vibrant and participatory democracy.\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "To build on this success, the following recommendations are proposed:\n",
        "1. **Ongoing Public Awareness Campaigns:** Continuously educate the public on the importance of voting and the process for restoring names to the Registers of Electors.\n",
        "2. **Enhanced Accessibility:** Further streamline the application process, particularly for online submissions, to accommodate an even broader segment of the population.\n",
        "3. **Regular Updates:** Periodically update the public on the status of the Registers of Electors and any changes to the application process.\n",
        "4. **Feedback Mechanism:** Implement a feedback mechanism to gather insights from applicants on their experience, enabling continuous improvement of the process.\n",
        "\n",
        "By implementing these recommendations, we can ensure that the electoral system remains robust, inclusive, and reflective of the will of the people. Answer: Of the 101,464 non-voters who have applied to restore their names to the Registers of Electors, 101,464 have been approved. # Question: What are the recommendation proposed? #Answer:'''\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\n",
        "            model.base_model.device\n",
        "        )\n",
        "ids = model.medusa_generate(\n",
        "    input_ids,\n",
        "    temperature=0,\n",
        "    max_steps=7000,\n",
        ")\n",
        "for value in ids:\n",
        "    final_output = value\n",
        "time.time()-start_time\n"
      ],
      "metadata": {
        "id": "iA_z2Fk4uUqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731257096363,
          "user_tz": -480,
          "elapsed": 9953,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "08e39712-9105-46a8-87b9-125568d535ca"
      },
      "id": "iA_z2Fk4uUqy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.889328002929688"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7-r2MJb9sUv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731257096364,
          "user_tz": -480,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9d88d0a5-58cc-4cd0-dcac-3e53af247614"
      },
      "id": "i7-r2MJb9sUv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'The Elections Department will continue to engage the public on the importance of voting and the process for restoring names to the Registers of Electors. It will also continue to make the application process as accessible as possible. The public will be updated on the status of the Registers of Electors and any changes to the application process. The Elections Department will also continue to seek feedback from applicants to improve the process. '}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # define GPU id, remove if you want to use all GPUs available\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "import numpy as np\n",
        "from medusa.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM\n",
        "from medusa.model.medusa_model import MedusaModel\n",
        "from medusa.model.kv_cache import *\n",
        "from medusa.model.utils import *\n",
        "from medusa.model.medusa_choices import *\n",
        "import transformers\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def timed(wall_times, key):\n",
        "    start = time.time()\n",
        "    torch.cuda.synchronize()\n",
        "    yield\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    elapsed_time = end - start\n",
        "    wall_times[key].append(elapsed_time)\n",
        "\n",
        "def medusa_forward(input_ids, model, tokenizer, medusa_choices, temperature, posterior_threshold, posterior_alpha, max_steps = 512):\n",
        "    wall_times = {'medusa': [], 'tree': [], 'posterior': [], 'update': [], 'init': []}\n",
        "\n",
        "    with timed(wall_times, 'init'):\n",
        "        if hasattr(model, \"medusa_choices\") and model.medusa_choices == medusa_choices:\n",
        "            # Load the cached medusa buffer\n",
        "            medusa_buffers = model.medusa_buffers\n",
        "        else:\n",
        "            # Initialize the medusa buffer\n",
        "            medusa_buffers = generate_medusa_buffers(\n",
        "                medusa_choices, device=model.base_model.device\n",
        "            )\n",
        "        model.medusa_buffers = medusa_buffers\n",
        "        model.medusa_choices = medusa_choices\n",
        "\n",
        "        # Initialize the past key and value states\n",
        "        if hasattr(model, \"past_key_values\"):\n",
        "            past_key_values = model.past_key_values\n",
        "            past_key_values_data = model.past_key_values_data\n",
        "            current_length_data = model.current_length_data\n",
        "            # Reset the past key and value states\n",
        "            current_length_data.zero_()\n",
        "        else:\n",
        "            (\n",
        "                past_key_values,\n",
        "                past_key_values_data,\n",
        "                current_length_data,\n",
        "            ) = initialize_past_key_values(model.base_model)\n",
        "            model.past_key_values = past_key_values\n",
        "            model.past_key_values_data = past_key_values_data\n",
        "            model.current_length_data = current_length_data\n",
        "\n",
        "        input_len = input_ids.shape[1]\n",
        "        reset_medusa_mode(model)\n",
        "        medusa_logits, logits = initialize_medusa(\n",
        "                input_ids, model, medusa_buffers[\"medusa_attn_mask\"], past_key_values\n",
        "        )\n",
        "    new_token = 0\n",
        "\n",
        "    for idx in range(max_steps):\n",
        "        with timed(wall_times, 'medusa'):\n",
        "            candidates, tree_candidates = generate_candidates(\n",
        "                    medusa_logits,\n",
        "                    logits,\n",
        "                    medusa_buffers[\"tree_indices\"],\n",
        "                    medusa_buffers[\"retrieve_indices\"],\n",
        "                )\n",
        "\n",
        "        with timed(wall_times, 'tree'):\n",
        "            medusa_logits, logits, outputs = tree_decoding(\n",
        "                    model,\n",
        "                    tree_candidates,\n",
        "                    past_key_values,\n",
        "                    medusa_buffers[\"medusa_position_ids\"],\n",
        "                    input_ids,\n",
        "                    medusa_buffers[\"retrieve_indices\"],\n",
        "                )\n",
        "\n",
        "        with timed(wall_times, 'posterior'):\n",
        "            best_candidate, accept_length = evaluate_posterior(\n",
        "                    logits, candidates, temperature, posterior_threshold, posterior_alpha\n",
        "                )\n",
        "\n",
        "        with timed(wall_times, 'update'):\n",
        "            input_ids, logits, medusa_logits, new_token = update_inference_inputs(\n",
        "                    input_ids,\n",
        "                    candidates,\n",
        "                    best_candidate,\n",
        "                    accept_length,\n",
        "                    medusa_buffers[\"retrieve_indices\"],\n",
        "                    outputs,\n",
        "                    logits,\n",
        "                    medusa_logits,\n",
        "                    new_token,\n",
        "                    past_key_values_data,\n",
        "                    current_length_data,\n",
        "                )\n",
        "\n",
        "        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
        "            break\n",
        "\n",
        "    return input_ids, new_token, idx, wall_times\n"
      ],
      "metadata": {
        "id": "nRftnNqaGo7c"
      },
      "id": "nRftnNqaGo7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0.\n",
        "posterior_threshold = 0.09\n",
        "posterior_alpha = 0.3\n",
        "medusa_choices = mc_sim_7b_63\n",
        "\n",
        "import time\n",
        "start_time =time.time()\n",
        "with torch.inference_mode():\n",
        "    input_ids = tokenizer([prompt]).input_ids\n",
        "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
        "                    torch.as_tensor(input_ids).cuda(),\n",
        "                    model,\n",
        "                    tokenizer,\n",
        "                    medusa_choices,\n",
        "                    temperature,\n",
        "                    posterior_threshold,\n",
        "                    posterior_alpha,\n",
        "                )\n",
        "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
        "    print(\"Output length:\", output_ids.size(-1))\n",
        "    print(\"Compression ratio:\", new_token / idx)\n",
        "\n",
        "output = tokenizer.decode(\n",
        "                    output_ids,\n",
        "                    spaces_between_special_tokens=False,\n",
        "                )\n",
        "print(time.time()-start_time)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "2o3txE2U54aA"
      },
      "id": "2o3txE2U54aA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "output_path = 'hansard_answered_questions_llama3_formatted_test_no_response_formatted.json'\n",
        "\n",
        "with open(output_path, \"r\") as outfile:\n",
        "    data = json.load(outfile)\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DnVo0eOfpFt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731323150465,
          "user_tz": -480,
          "elapsed": 17,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "65149aff-bdd7-467f-ed0d-3d7a77825e68"
      },
      "id": "6DnVo0eOfpFt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import time\n",
        "all_outputs = []\n",
        "\n",
        "\n",
        "temperature = 0.\n",
        "posterior_threshold = 0.09\n",
        "posterior_alpha = 0.3\n",
        "medusa_choices = vicuna_7b_stage2\n",
        "\n",
        "start_time = time.time()\n",
        "for prompt in data:\n",
        "  prompt_formatted = prompt.replace(\"# Question:\\nQuestion:\", \" Question: \") + \" # Answer:\"\n",
        "  with torch.inference_mode():\n",
        "      input_ids = tokenizer([prompt_formatted]).input_ids\n",
        "      output_ids, new_token, idx, wall_time = medusa_forward(\n",
        "                      torch.as_tensor(input_ids).cuda(),\n",
        "                      model,\n",
        "                      tokenizer,\n",
        "                      medusa_choices,\n",
        "                      temperature,\n",
        "                      posterior_threshold,\n",
        "                      posterior_alpha,\n",
        "                  )\n",
        "      output_ids = output_ids[0][len(input_ids[0]) :]\n",
        "\n",
        "\n",
        "  output = tokenizer.decode(\n",
        "                      output_ids,\n",
        "                      spaces_between_special_tokens=False,\n",
        "                  )\n",
        "\n",
        "  all_outputs.append(prompt_formatted + \" \"+ output)\n",
        "end_time = time.time() - start_time\n",
        "\n",
        "display(f\"Total Time:{end_time}. Average Time: {end_time/len(data)}\")\n",
        "\n",
        "import json\n",
        "\n",
        "output_path = 'medusa_response.json'\n",
        "\n",
        "with open(output_path, \"w\") as outfile:\n",
        "    json.dump(all_outputs, outfile)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IvMdrr7JdBom",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731325557344,
          "user_tz": -480,
          "elapsed": 2346388,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "87d2ad40-e2db-42eb-dfaf-bf66613d16dc"
      },
      "id": "IvMdrr7JdBom",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Total Time:2346.2848188877106. Average Time: 7.718042167393785'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_outputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "QE1QnTrOr8wB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731325557345,
          "user_tz": -480,
          "elapsed": 10,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "0d4f41e7-0c53-419d-82b4-6856f3a8b161"
      },
      "id": "QE1QnTrOr8wB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Question: Mr Leong Mun Wai asked the Prime Minister (a) since the conclusion of the 2023 Presidential Election, how many non-voters have applied to restore their names to the Registers of Electors; and (b) how many of these applications are successful.\\n\\nSupporting points: **Title: Applications to Restore Names to Registers of Electors**\\n\\n**Executive Summary:**\\n\\nAs of the end of January 2024, a total of 101,464 non-voters from the 2023 Presidential Election have successfully applied to restore their names to the Registers of Electors. This report details the background, process, and implications of these applications, with a focus on ensuring continued electoral engagement and upholding democratic principles.\\n\\n**Background:**\\n\\nThe 2023 Presidential Election saw a significant number of eligible voters abstaining from casting their votes. Consequently, their names were removed from the Registers of Electors in accordance with established electoral regulations. However, recognizing the importance of inclusive participation in democratic processes, the Elections Department provided a mechanism for non-voters to restore their names to the registers.\\n\\n**Application Process:**\\n\\nThe process to restore names to the Registers of Electors was designed to be straightforward and accessible. Non-voters could submit their applications through various channels, including:\\n1. **Online Portal:** A dedicated section on the Elections Department's website allowed individuals to complete and submit their applications electronically.\\n2. **In-Person Submissions:** Local electoral offices across the country were equipped to handle in-person applications, ensuring access for individuals without reliable internet access.\\n3. **Mail-In Applications:** For added convenience, applicants could also restore their names via postal mail by sending completed forms to their respective electoral offices.\\n\\n**Volume of Applications:**\\n\\nBy the end of January 2024, a total of 101,464 individuals had applied to have their names restored. This significant number reflects a robust response from the public, underscoring the desire among citizens to re-engage with the electoral process. The breakdown of applications received through different channels is as follows:\\n- **Online Portal:** 67,892 applications\\n- **In-Person Submissions:** 22,710 applications\\n- **Mail-In Applications:** 10,862 applications\\n\\n**Approval and Verification:**\\n\\nAll 101,464 applications were meticulously reviewed by the Elections Department. The verification process ensured that each applicant met the criteria for restoration, including:\\n- **Verification of Identity:** Confirming the identity of applicants to prevent fraudulent submissions.\\n- **Eligibility Check:** Ensuring that applicants were eligible voters in the 2023 Presidential Election and had not been disqualified for other reasons.\\n- **Data Accuracy:** Cross-referencing application details with existing records to maintain the integrity of the Registers of Electors.\\n\\nUpon successful verification, all applications were approved, and the names of the applicants were restored to the Registers of Electors. This meticulous approach guarantees the reliability and accuracy of the electoral registers.\\n\\n**Implications:**\\n\\nThe successful restoration of 101,464 names to the Registers of Electors has several significant implications:\\n1. **Enhanced Voter Engagement:** The substantial number of restored names reflects a renewed commitment among citizens to participate in future elections, thereby strengthening democratic engagement.\\n2. **Inclusivity:** The accessible application process ensured that individuals from diverse backgrounds had the opportunity to restore their names, promoting inclusivity in the electoral process.\\n3. **Administrative Efficiency:** The Elections Department's efficient handling of the applications demonstrates its capability to manage large volumes of requests while maintaining accuracy and integrity.\\n4. **Public Trust:** The transparent and successful restoration process fosters public trust in the electoral system, reinforcing the legitimacy of future elections.\\n\\n**Conclusion:**\\n\\nThe successful restoration of 101,464 names to the Registers of Electors is a testament to the commitment of both the Elections Department and the citizens to uphold democratic values. The robust response from non-voters highlights the importance of accessible and inclusive electoral processes. Moving forward, continued efforts to engage and educate the electorate will be crucial in maintaining a vibrant and participatory democracy.\\n\\n**Recommendations:**\\n\\nTo build on this success, the following recommendations are proposed:\\n1. **Ongoing Public Awareness Campaigns:** Continuously educate the public on the importance of voting and the process for restoring names to the Registers of Electors.\\n2. **Enhanced Accessibility:** Further streamline the application process, particularly for online submissions, to accommodate an even broader segment of the population.\\n3. **Regular Updates:** Periodically update the public on the status of the Registers of Electors and any changes to the application process.\\n4. **Feedback Mechanism:** Implement a feedback mechanism to gather insights from applicants on their experience, enabling continuous improvement of the process.\\n\\nBy implementing these recommendations, we can ensure that the electoral system remains robust, inclusive, and reflective of the will of the people. # Answer: As of end-January 2024, 101,464 non-voters from the 2023 Presidential Election have successfully applied to restore their names to the Registers of Electors. </s>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medusa without fine-tuning"
      ],
      "metadata": {
        "id": "ifBXWcHZM0Hw"
      },
      "id": "ifBXWcHZM0Hw"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "output_path = 'hansard_answered_questions_llama3_formatted_test_no_response_formatted.json'\n",
        "\n",
        "with open(output_path, \"r\") as outfile:\n",
        "    data = json.load(outfile)\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxCer3l9RZ1I",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731327404202,
          "user_tz": -480,
          "elapsed": 142,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "b958f370-b91b-4c9f-cf0b-04e2413996ae"
      },
      "id": "pxCer3l9RZ1I",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import torch\n",
        "from fastchat.serve.cli import SimpleChatIO, RichChatIO, ProgrammaticChatIO\n",
        "from fastchat.model.model_adapter import get_conversation_template\n",
        "from fastchat.conversation import get_conv_template\n",
        "import json\n",
        "from medusa.model.medusa_model import MedusaModel"
      ],
      "metadata": {
        "id": "wDG0DjuLM7Gz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731327414595,
          "user_tz": -480,
          "elapsed": 9663,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e1d5f941-61b0-441c-ac93-3b5dd6ef54eb"
      },
      "id": "wDG0DjuLM7Gz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-11 12:17:02,218] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'FasterDecoding/medusa-vicuna-7b-v1.3'\n",
        "model = MedusaModel.from_pretrained(\n",
        "    model_name,\n",
        "    #medusa_num_heads = 4,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = model.get_tokenizer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "mUhEN6AAgJJB",
        "executionInfo": {
          "status": "error",
          "timestamp": 1731335729339,
          "user_tz": -480,
          "elapsed": 1617,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "27e28021-6c38-49c5-993d-1991315d4637"
      },
      "id": "mUhEN6AAgJJB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "MedusaModelABC.__init__() got an unexpected keyword argument 'model_max_length'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/medusa_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             return super().from_pretrained(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1060\u001b[0m             \u001b[0;34mf\"Unrecognized model in {pretrained_model_name_or_path}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized model in FasterDecoding/medusa-vicuna-7b-v1.3. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chinese_clip, clap, clip, clipseg, code_llama, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, idefics, imagegpt, informer, instructblip, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mistral, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mpt, mra, mt5, musicgen, mvp, nat, nezha, nllb-moe, nougat, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pe...",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ae4f554c877c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'FasterDecoding/medusa-vicuna-7b-v1.3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = MedusaModel.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#medusa_num_heads = 4,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/medusa_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"llama\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             return MedusaModelLlama.from_pretrained(\n\u001b[0m\u001b[1;32m    398\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/medusa_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mbase_model_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedusa_num_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;31m# TODO: fix the uploaded config (only include 2 heads)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mbase_model_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedusa_num_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedusa_num_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             model = super().from_pretrained(\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3085\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[0;31m# Check first if we are `from_pt`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MedusaModelABC.__init__() got an unexpected keyword argument 'model_max_length'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "input_ids = tokenizer.encode('''# Question:Mr Leong Mun Wai asked the Prime Minister (a) since the conclusion of the 2023 Presidential Election, how many non-voters have applied to restore their names to the Registers of Electors; and (b) how many of these applications are successful.\n",
        "\n",
        "Supporting points: **Title: Applications to Restore Names to Registers of Electors**\n",
        "\n",
        "**Executive Summary:**\n",
        "\n",
        "As of the end of January 2024, a total of 101,464 non-voters from the 2023 Presidential Election have successfully applied to restore their names to the Registers of Electors. This report details the background, process, and implications of these applications, with a focus on ensuring continued electoral engagement and upholding democratic principles.\n",
        "\n",
        "**Background:**\n",
        "\n",
        "The 2023 Presidential Election saw a significant number of eligible voters abstaining from casting their votes. Consequently, their names were removed from the Registers of Electors in accordance with established electoral regulations. However, recognizing the importance of inclusive participation in democratic processes, the Elections Department provided a mechanism for non-voters to restore their names to the registers.\n",
        "\n",
        "**Application Process:**\n",
        "\n",
        "The process to restore names to the Registers of Electors was designed to be straightforward and accessible. Non-voters could submit their applications through various channels, including:\n",
        "1. **Online Portal:** A dedicated section on the Elections Department's website allowed individuals to complete and submit their applications electronically.\n",
        "2. **In-Person Submissions:** Local electoral offices across the country were equipped to handle in-person applications, ensuring access for individuals without reliable internet access.\n",
        "3. **Mail-In Applications:** For added convenience, applicants could also restore their names via postal mail by sending completed forms to their respective electoral offices.\n",
        "\n",
        "**Volume of Applications:**\n",
        "\n",
        "By the end of January 2024, a total of 101,464 individuals had applied to have their names restored. This significant number reflects a robust response from the public, underscoring the desire among citizens to re-engage with the electoral process. The breakdown of applications received through different channels is as follows:\n",
        "- **Online Portal:** 67,892 applications\n",
        "- **In-Person Submissions:** 22,710 applications\n",
        "- **Mail-In Applications:** 10,862 applications\n",
        "\n",
        "**Approval and Verification:**\n",
        "\n",
        "All 101,464 applications were meticulously reviewed by the Elections Department. The verification process ensured that each applicant met the criteria for restoration, including:\n",
        "- **Verification of Identity:** Confirming the identity of applicants to prevent fraudulent submissions.\n",
        "- **Eligibility Check:** Ensuring that applicants were eligible voters in the 2023 Presidential Election and had not been disqualified for other reasons.\n",
        "- **Data Accuracy:** Cross-referencing application details with existing records to maintain the integrity of the Registers of Electors.\n",
        "\n",
        "Upon successful verification, all applications were approved, and the names of the applicants were restored to the Registers of Electors. This meticulous approach guarantees the reliability and accuracy of the electoral registers.\n",
        "\n",
        "**Implications:**\n",
        "\n",
        "The successful restoration of 101,464 names to the Registers of Electors has several significant implications:\n",
        "1. **Enhanced Voter Engagement:** The substantial number of restored names reflects a renewed commitment among citizens to participate in future elections, thereby strengthening democratic engagement.\n",
        "2. **Inclusivity:** The accessible application process ensured that individuals from diverse backgrounds had the opportunity to restore their names, promoting inclusivity in the electoral process.\n",
        "3. **Administrative Efficiency:** The Elections Department's efficient handling of the applications demonstrates its capability to manage large volumes of requests while maintaining accuracy and integrity.\n",
        "4. **Public Trust:** The transparent and successful restoration process fosters public trust in the electoral system, reinforcing the legitimacy of future elections.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The successful restoration of 101,464 names to the Registers of Electors is a testament to the commitment of both the Elections Department and the citizens to uphold democratic values. The robust response from non-voters highlights the importance of accessible and inclusive electoral processes. Moving forward, continued efforts to engage and educate the electorate will be crucial in maintaining a vibrant and participatory democracy.\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "To build on this success, the following recommendations are proposed:\n",
        "1. **Ongoing Public Awareness Campaigns:** Continuously educate the public on the importance of voting and the process for restoring names to the Registers of Electors.\n",
        "2. **Enhanced Accessibility:** Further streamline the application process, particularly for online submissions, to accommodate an even broader segment of the population.\n",
        "3. **Regular Updates:** Periodically update the public on the status of the Registers of Electors and any changes to the application process.\n",
        "4. **Feedback Mechanism:** Implement a feedback mechanism to gather insights from applicants on their experience, enabling continuous improvement of the process.\n",
        "\n",
        "By implementing these recommendations, we can ensure that the electoral system remains robust, inclusive, and reflective of the will of the people. Answer: Of the 101,464 non-voters who have applied to restore their names to the Registers of Electors, 101,464 have been approved. # Question: What are the recommendation proposed? #Answer:''', return_tensors=\"pt\").to(\n",
        "            model.base_model.device\n",
        "        )\n",
        "ids = model.medusa_generate(\n",
        "    input_ids,\n",
        "    temperature=0,\n",
        "    max_steps=7000,\n",
        ")\n",
        "for value in ids:\n",
        "    final_output = value\n",
        "print(time.time()-start_time)\n",
        "final_output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEmWw_IXwjDc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731327551213,
          "user_tz": -480,
          "elapsed": 3983,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "707648f5-ed69-4911-e9df-dcf019c7867c"
      },
      "id": "vEmWw_IXwjDc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.87833833694458\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'The recommendations proposed are:\\n\\n1. **Ongoing Public Awareness Campaigns:** Continuously educate the public on the importance of voting and the process for restoring names to the Registers of Electors.\\n2. **Enhanced Accessibility:** Further streamline the application process, particularly for online submissions, to accommodate an even broader segment of the population.\\n3. **Regular Updates:** Periodically update the public on the status of the Registers of Electors and any changes to the application process.\\n4. **Feedback Mechanism:** Implement a feedback mechanism to gather insights from applicants on their experience, enabling continuous improvement of the process.\\n\\nBy implementing these recommendations, we can ensure that the electoral system remains robust, inclusive, and reflective of the will of the people.'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import time\n",
        "all_outputs = []\n",
        "\n",
        "start_time = time.time()\n",
        "for prompt in data:\n",
        "  prompt_formatted = prompt.replace(\"# Question:\\nQuestion:\", \"# Question: \") + \" # Answer:\"\n",
        "  input_ids = tokenizer.encode(prompt_formatted, return_tensors=\"pt\").to(\n",
        "            model.base_model.device\n",
        "        )\n",
        "  ids = model.medusa_generate(\n",
        "      input_ids,\n",
        "      temperature=0,\n",
        "      max_steps=10000\n",
        "  )\n",
        "  try:\n",
        "    for value in ids:\n",
        "        final_output = value\n",
        "  except:\n",
        "    display(prompt_formatted)\n",
        "\n",
        "  all_outputs.append(prompt_formatted + \" \"+ final_output['text'])\n",
        "\n",
        "\n",
        "end_time = time.time() - start_time\n",
        "\n",
        "display(f\"Total Time:{end_time}. Average Time: {end_time/len(data)}\")\n",
        "\n",
        "import json\n",
        "\n",
        "output_path = 'medusa_response_no_funetuning.json'\n",
        "\n",
        "with open(output_path, \"w\") as outfile:\n",
        "    json.dump(all_outputs, outfile)\n",
        "\n",
        "display(len(all_outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KwF9IJZCR6Bp",
        "executionInfo": {
          "status": "error",
          "timestamp": 1731333502012,
          "user_tz": -480,
          "elapsed": 50070,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "0a92516c-71a3-4507-e015-1616920900a8"
      },
      "id": "KwF9IJZCR6Bp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"# Question: Mr Leong Mun Wai asked the Minister for Transport (a) what are the mitigation measures that have been put in place to reduce the noise from railway track maintenance at night; and (b) how do these measures compare with best practices overseas.\\n\\nSupporting points: **Report: Mitigation Measures to Reduce Noise from Railway Track Maintenance at Night**\\n\\n**Introduction**\\nRailway track maintenance is an essential activity to ensure the safety and efficiency of our rail transport system. However, it can be a significant source of noise, particularly when conducted at night, affecting the quality of life for residents living near above-ground tracks. This report outlines the mitigation measures implemented by rail operators to reduce noise during night-time track maintenance. These measures include improvements in track design, adoption of advanced equipment and tools, and proactive communication with affected residents. Additionally, this report highlights how these practices align with best practices carried out overseas.\\n\\n**Improved Track Design**\\nOne of the primary strategies to mitigate noise during track maintenance is the improvement in track design. Rail operators have invested in state-of-the-art track designs that inherently produce less noise. For instance, the implementation of continuously welded rails has significantly reduced the noise generated by the rail joints. The use of resilient rail pads and ballast mats further dampens the vibrations and noise produced during maintenance activities.\\n\\nAdditionally, rail operators have introduced noise barriers and acoustic screens along the tracks, which effectively block the propagation of sound waves towards residential areas. These barriers are designed to absorb and deflect noise, thereby reducing its impact on nearby communities. The design improvements not only enhance the overall performance of the rail system but also contribute to a quieter environment for residents.\\n\\n**Adoption of New Equipment and Tools**\\nRail operators have also adopted new equipment and tools specifically designed to minimize noise during maintenance operations. Traditional maintenance techniques often involved noisy machinery and processes. In contrast, modern equipment such as rail grinders, tamping machines, and ultrasonic testing devices are engineered to operate more quietly.\\n\\nFor example, rail grinders equipped with advanced noise suppression systems can perform the same tasks with significantly reduced noise levels. Similarly, tamping machines with vibration-dampening features ensure that the maintenance process is both efficient and less disruptive to nearby residents. The use of ultrasonic testing devices allows for non-destructive testing of rails, identifying defects without the need for loud, invasive procedures.\\n\\nMoreover, rail operators are increasingly utilizing electric and battery-powered maintenance vehicles, which produce less noise compared to their diesel counterparts. These vehicles not only contribute to noise reduction but also align with sustainability goals by reducing emissions.\\n\\n**Proactive Communication with Residents**\\nUnderstanding the concerns of residents living near above-ground tracks is crucial for maintaining good community relations. Rail operators have implemented proactive communication strategies to inform residents in advance when noisy maintenance works are required. This approach ensures that residents are not caught off guard and can make necessary arrangements to minimize the impact on their daily lives.\\n\\nCommunication channels include direct mail notifications, community meetings, and digital platforms such as emails and social media updates. Information provided to residents includes the schedule of maintenance activities, expected noise levels, and the duration of the works. In some cases, rail operators also offer temporary accommodation options for residents who may be particularly affected by the noise.\\n\\nFurthermore, dedicated hotlines and customer service teams are available to address any concerns or complaints from residents. This open line of communication fosters trust and demonstrates the rail operators' commitment to minimizing the inconvenience caused by essential maintenance activities.\\n\\n**Alignment with Best Practices Overseas**\\nThe mitigation measures implemented by our rail operators are in line with best practices carried out overseas. Countries with advanced rail systems, such as Japan, Germany, and the United Kingdom, have long recognized the importance of noise reduction in maintaining community relations and ensuring public well-being.\\n\\nFor instance, Japan's Shinkansen (bullet train) network employs sophisticated noise barriers and advanced track designs to minimize noise pollution. Similarly, Germany's Deutsche Bahn has invested in low-noise maintenance equipment and comprehensive communication strategies to keep residents informed. The United Kingdom's Network Rail has also implemented noise mitigation measures, including the use of quieter machinery and proactive engagement with affected communities.\\n\\nBy adopting these best practices, our rail operators not only enhance the effectiveness of their noise reduction efforts but also ensure that our rail system remains competitive on a global scale.\\n\\n**Conclusion**\\nThe implementation of improved track design, adoption of advanced equipment and tools, and proactive communication with residents are key mitigation measures to reduce noise from railway track maintenance at night. These measures align with international best practices and demonstrate the rail operators' commitment to minimizing the impact of essential maintenance activities on nearby communities. Continued investment in these strategies will ensure that our rail system remains safe, efficient, and community-friendly.\\n\\n**Recommendations**\\n- Continued investment in research and development to further improve track design and maintenance equipment.\\n- Regular review and updating of communication strategies to ensure effective engagement with residents.\\n- Collaboration with international rail operators to share knowledge and adopt innovative noise reduction practices.\\n- Monitoring and evaluation of the effectiveness of current mitigation measures to identify areas for improvement.\\n\\nBy implementing these recommendations, we can further enhance our efforts to reduce noise pollution and maintain positive relations with the communities we serve. # Answer:\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "start (1987) + length (64) exceeds dimension size (2048).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7d85d7d6a7f4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/medusa_model.py\u001b[0m in \u001b[0;36mmedusa_generate\u001b[0;34m(self, input_ids, attention_mask, temperature, max_steps, medusa_choices, posterior_threshold, posterior_alpha, top_p, sampling, fast)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# Use tree attention to verify the candidates and get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             medusa_logits, logits, outputs = tree_decoding(\n\u001b[0m\u001b[1;32m    331\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/utils.py\u001b[0m in \u001b[0;36mtree_decoding\u001b[0;34m(model, tree_candidates, past_key_values, medusa_position_ids, input_ids, retrieve_indices)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;31m# The model is expected to return logits for the Medusa structure, original logits, and possibly other outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     tree_medusa_logits, outputs, tree_logits = model(\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mtree_candidates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/medusa_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, output_orig, position_ids, medusa_forward, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# Pass input through the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             outputs = self.base_model.model(\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/modeling_llama_kv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    931\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/modeling_llama_kv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/modeling_llama_kv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/kv_cache.py\u001b[0m in \u001b[0;36mcat\u001b[0;34m(self, tensor, dim)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \"\"\"\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: start (1987) + length (64) exceeds dimension size (2048).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7d85d7d6a7f4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     )    \n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mall_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_formatted\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/medusa_model.py\u001b[0m in \u001b[0;36mmedusa_generate\u001b[0;34m(self, input_ids, attention_mask, temperature, max_steps, medusa_choices, posterior_threshold, posterior_alpha, top_p, sampling, fast)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# Use tree attention to verify the candidates and get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             medusa_logits, logits, outputs = tree_decoding(\n\u001b[0m\u001b[1;32m    331\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mtree_candidates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/utils.py\u001b[0m in \u001b[0;36mtree_decoding\u001b[0;34m(model, tree_candidates, past_key_values, medusa_position_ids, input_ids, retrieve_indices)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# Use the model to decode the tree candidates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;31m# The model is expected to return logits for the Medusa structure, original logits, and possibly other outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     tree_medusa_logits, outputs, tree_logits = model(\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mtree_candidates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0moutput_orig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/medusa_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, output_orig, position_ids, medusa_forward, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# Pass input through the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             outputs = self.base_model.model(\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/modeling_llama_kv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    928\u001b[0m                 )\n\u001b[1;32m    929\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    931\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/modeling_llama_kv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/modeling_llama_kv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# If past_key_value is available, reuse the states for k, v, and self_attention.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# Reset past_key_value to avoid return past_key_value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/kv_cache.py\u001b[0m in \u001b[0;36mcat\u001b[0;34m(self, tensor, dim)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mup\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \"\"\"\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: start (1987) + length (64) exceeds dimension size (2048)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import time\n",
        "all_outputs = []\n",
        "\n",
        "\n",
        "temperature = 0.\n",
        "posterior_threshold = 0.09\n",
        "posterior_alpha = 0.3\n",
        "medusa_choices = vicuna_7b_stage2\n",
        "\n",
        "start_time = time.time()\n",
        "for prompt in data:\n",
        "  prompt_formatted = prompt.replace(\"# Question:\\nQuestion:\", \"# Question: \") + \" # Answer:\"\n",
        "  with torch.inference_mode():\n",
        "      input_ids = tokenizer([prompt_formatted]).input_ids\n",
        "      output_ids, new_token, idx, wall_time = medusa_forward(\n",
        "                      torch.as_tensor(input_ids).cuda(),\n",
        "                      model,\n",
        "                      tokenizer,\n",
        "                      medusa_choices,\n",
        "                      temperature,\n",
        "                      posterior_threshold,\n",
        "                      posterior_alpha,\n",
        "                  )\n",
        "      output_ids = output_ids[0][len(input_ids[0]) :]\n",
        "\n",
        "\n",
        "  output = tokenizer.decode(\n",
        "                      output_ids,\n",
        "                      spaces_between_special_tokens=False,\n",
        "                  )\n",
        "\n",
        "  all_outputs.append(prompt_formatted + \" \"+ output)\n",
        "end_time = time.time() - start_time\n",
        "\n",
        "display(f\"Total Time:{end_time}. Average Time: {end_time/len(data)}\")\n",
        "\n",
        "import json\n",
        "\n",
        "output_path = 'medusa_response_no_funetuning.json'\n",
        "\n",
        "with open(output_path, \"w\") as outfile:\n",
        "    json.dump(all_outputs, outfile)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "ydMhezvBebqm",
        "executionInfo": {
          "status": "error",
          "timestamp": 1731332882916,
          "user_tz": -480,
          "elapsed": 35822,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "a78929bc-a3fc-454c-d05b-152f9fd63efe"
      },
      "id": "ydMhezvBebqm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "start (1987) + length (64) exceeds dimension size (2048).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c4f5935a47d0>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_formatted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       output_ids, new_token, idx, wall_time = medusa_forward(\n\u001b[0m\u001b[1;32m     17\u001b[0m                       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                       \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ac2d6c337f79>\u001b[0m in \u001b[0;36mmedusa_forward\u001b[0;34m(input_ids, model, tokenizer, medusa_choices, temperature, posterior_threshold, posterior_alpha, max_steps)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtimed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwall_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             medusa_logits, logits, outputs = tree_decoding(\n\u001b[0m\u001b[1;32m     77\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mtree_candidates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/utils.py\u001b[0m in \u001b[0;36mtree_decoding\u001b[0;34m(model, tree_candidates, past_key_values, medusa_position_ids, input_ids, retrieve_indices)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# Use the model to decode the tree candidates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;31m# The model is expected to return logits for the Medusa structure, original logits, and possibly other outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     tree_medusa_logits, outputs, tree_logits = model(\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mtree_candidates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0moutput_orig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/medusa_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, output_orig, position_ids, medusa_forward, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# Pass input through the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             outputs = self.base_model.model(\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/modeling_llama_kv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    928\u001b[0m                 )\n\u001b[1;32m    929\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    931\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/modeling_llama_kv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/modeling_llama_kv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# If past_key_value is available, reuse the states for k, v, and self_attention.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# Reset past_key_value to avoid return past_key_value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medusa/model/kv_cache.py\u001b[0m in \u001b[0;36mcat\u001b[0;34m(self, tensor, dim)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mup\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \"\"\"\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: start (1987) + length (64) exceeds dimension size (2048)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLAMMA-2 outputs"
      ],
      "metadata": {
        "id": "IwENnAcIM3Zo"
      },
      "id": "IwENnAcIM3Zo"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxQbv8rbjq-t"
      },
      "id": "nxQbv8rbjq-t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "2b4BCAPb-Wfj"
      },
      "id": "2b4BCAPb-Wfj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "\n",
        "access_token = \"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\"\n",
        "model = \"meta-llama/Llama-2-7b\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model,\n",
        "    token=access_token\n",
        ").to(torch.device(\"cuda:0\"))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model,\n",
        "    token=access_token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "cqROFAbSM6uE",
        "executionInfo": {
          "status": "error",
          "timestamp": 1731418082983,
          "user_tz": -480,
          "elapsed": 844,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "dfa2433c-3d8d-4dce-867e-b5e4ed2f97c3"
      },
      "id": "cqROFAbSM6uE",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Llama-2-7b is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    467\u001b[0m             )\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-673357ec-52acbf3a2f82bbae7a81df40;d36b8584-cd66-42b7-8213-b0b9da86301f)\n\n403 Forbidden: Please enable access to public gated repositories in your fine-grained token settings to view this repository..\nCannot access content at: https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.\nMake sure your token has the correct permissions.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         \u001b[0;31m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m         raise LocalEntryNotFoundError(\n\u001b[0m\u001b[1;32m   1488\u001b[0m             \u001b[0;34m\"An error happened while trying to locate the file on the Hub and we cannot find the requested files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-eeff6836dc2b>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Llama-2-7b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccess_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    676\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this file, couldn't find it in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;34mf\" cached files and it looks like {path_or_repo_id} is not the path to a directory containing a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Llama-2-7b is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_outputs = []\n",
        "import time\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\",\n",
        "    tokenizer=tokenizer,\n",
        "    device=0\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, prompt in enumerate(data):\n",
        "    prompt_formatted = prompt.replace(\"# Question:\\nQuestion:\", \" Question: \") + \" # Answer:\"\n",
        "\n",
        "    sequences = pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    all_outputs.append(prompt_formatted + \" \"+ sequences[0].get(\"generated_text\"))\n",
        "end_time = time.time() - start_time\n",
        "\n",
        "display(f\"Total Time:{end_time}. Average Time: {end_time/len(data)}\")\n",
        "\n",
        "import json\n",
        "\n",
        "output_path = 'llamma2_response.json'\n",
        "\n",
        "with open(output_path, \"w\") as outfile:\n",
        "    json.dump(all_outputs, outfile)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jhVZvEliZZQ2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731085424462,
          "user_tz": -480,
          "elapsed": 375610,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "3ca131e5-ee0c-4f37-effb-a9bd0f88893d"
      },
      "id": "jhVZvEliZZQ2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `device` and `device_map` are specified. `device` will override `device_map`. You will most likely encounter unexpected behavior. Please remove `device` and keep `device_map`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Total Time:375.4614746570587. Average Time: 1.2350706403192722'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHwOWrg0t73j",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731118757768,
          "user_tz": -480,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9fdcc63c-5bf1-40e8-b78a-03acc26fcc2e"
      },
      "id": "bHwOWrg0t73j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "304"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vicuna 7b"
      ],
      "metadata": {
        "id": "b9ppg5n3J0dQ"
      },
      "id": "b9ppg5n3J0dQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\").to(torch.device(\"cuda:0\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "3284b44209d848eca50e2ef6d18c6b87",
            "8cbf6c007ba049da9b0d7ab41e54335c",
            "7c36653daaf444e49e5a864b403446a5",
            "484afec5a9b842f6ab720c4a6b24327c",
            "3487b437b0b44a6b945ce9501b603d68",
            "4e7e0e0a4bd34fb1a06793a46ea5cacb",
            "b3018a84b7fa47aaa242eda885adfe10",
            "2f77bb5aa19f482bb32ae03249819ad0",
            "c28ad206d8964b8ab58e3a79478250a2",
            "2f9586f76960481ea7118bd6f3d1ce71",
            "a094d3476fc2442ebd9b9a00ab03bee6"
          ]
        },
        "id": "uGg1iJn7J2RA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731407847225,
          "user_tz": -480,
          "elapsed": 83562,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "77190885-d1ce-48c0-8470-452ee1d2e24b"
      },
      "id": "uGg1iJn7J2RA",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3284b44209d848eca50e2ef6d18c6b87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=map_location)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "output_path = 'hansard_answered_questions_llama3_formatted_test_no_response_formatted.json'\n",
        "\n",
        "with open(output_path, \"r\") as outfile:\n",
        "    data = json.load(outfile)\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKVvN5ke9SfX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731407981778,
          "user_tz": -480,
          "elapsed": 130,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4475b507-00d2-4cb9-a8a1-99ee1f97b79f"
      },
      "id": "GKVvN5ke9SfX",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "system=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
        "\n",
        "\n",
        "def text_raper(text):\n",
        "    return system + f\"\\nUSER: {text}\\nASSISTANT:\"\n",
        "\n",
        "input_text = \"I love reading books\"\n",
        "input_text = text_raper(input_text)\n",
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(torch.device(\"cuda:0\"))\n",
        "output = model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "7WF8Imfkd6jh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731407884014,
          "user_tz": -480,
          "elapsed": 1781,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "066b4373-fcf8-47ed-c0cc-5fa074a7fb6d"
      },
      "id": "7WF8Imfkd6jh",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
            "USER: I love reading books\n",
            "ASSISTANT: That's great to hear! What kind of books do you enjoy reading?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "system=\"You are a public servant. Your task is to reply to a parliamentary question given a list of supporting points.\"\n",
        "\n",
        "all_outputs = []\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, prompt in enumerate(data):\n",
        "    prompt_formatted = system+prompt.replace(\"# Question:\\nQuestion:\", \"# Question: \") + \" # Answer:\"\n",
        "    prompt_formatted = text_raper(prompt_formatted)\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt_formatted, return_tensors='pt').to(torch.device(\"cuda:0\"))\n",
        "    output = model.generate(input_ids, max_length=10000, do_sample=True, temperature=0.1)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    all_outputs.append(response.replace(system, \"\"))\n",
        "end_time = time.time() - start_time\n",
        "\n",
        "display(f\"Total Time:{end_time}. Average Time: {end_time/len(data)}\")\n",
        "\n",
        "import json\n",
        "\n",
        "output_path = 'vicuna_response.json'\n",
        "\n",
        "with open(output_path, \"w\") as outfile:\n",
        "    json.dump(all_outputs, outfile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LRS1PcuPduXh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731414490183,
          "user_tz": -480,
          "elapsed": 4912773,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "17f1d80c-c822-4505-c23f-d036aecf6fc2"
      },
      "id": "LRS1PcuPduXh",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Total Time:4912.795715093613. Average Time: 16.160512220702675'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "medusa"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a0dc9577a0d469cb5e65c252266acf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55542feba7c34916b5e340bc3726580f",
              "IPY_MODEL_c38543f18d574142ad67b20e341ab09f",
              "IPY_MODEL_a2c52e45335f44cdac88086bebb1e50e"
            ],
            "layout": "IPY_MODEL_d5b0b9924cd846999a8e443b5bf4a42d"
          }
        },
        "55542feba7c34916b5e340bc3726580f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f35cb8198a241c08c81975327986016",
            "placeholder": "​",
            "style": "IPY_MODEL_6dd7e020e2c14322ad95c203d1527e8c",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "c38543f18d574142ad67b20e341ab09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03c5642367524b308f0078e936908d94",
            "max": 1785,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4befcf3c3e324b5e9f11cee5a93bdd35",
            "value": 1785
          }
        },
        "a2c52e45335f44cdac88086bebb1e50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c61da1fb04624f8b97d071e85588c0a4",
            "placeholder": "​",
            "style": "IPY_MODEL_08550ec1cf2c4be788459a3730791372",
            "value": " 1785/1785 [00:00&lt;00:00, 10822.59 examples/s]"
          }
        },
        "d5b0b9924cd846999a8e443b5bf4a42d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f35cb8198a241c08c81975327986016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dd7e020e2c14322ad95c203d1527e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03c5642367524b308f0078e936908d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4befcf3c3e324b5e9f11cee5a93bdd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c61da1fb04624f8b97d071e85588c0a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08550ec1cf2c4be788459a3730791372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "facd529aad4e40a5980496e1b51b266a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e79652d2ab941fba60ccf2f107cfb54",
              "IPY_MODEL_e62a773e9e1040e0a514197f6ea48ed5",
              "IPY_MODEL_72e060e0cfbb4d108c2a8a0a53f4836e"
            ],
            "layout": "IPY_MODEL_cde4d720760e41e9843fcb240151d905"
          }
        },
        "4e79652d2ab941fba60ccf2f107cfb54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5792ac2b58e0458491588d5cb7d0675d",
            "placeholder": "​",
            "style": "IPY_MODEL_a568d1ab0eaa454da3fd683fd303a467",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e62a773e9e1040e0a514197f6ea48ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb67298042164657b4cd25302de9bdf2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd83c13102334183b37b499344d82620",
            "value": 2
          }
        },
        "72e060e0cfbb4d108c2a8a0a53f4836e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c73c5954684475eb095751d67a475a3",
            "placeholder": "​",
            "style": "IPY_MODEL_e17de4d4687e4226851a80a88d894f15",
            "value": " 2/2 [00:17&lt;00:00,  7.95s/it]"
          }
        },
        "cde4d720760e41e9843fcb240151d905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5792ac2b58e0458491588d5cb7d0675d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a568d1ab0eaa454da3fd683fd303a467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb67298042164657b4cd25302de9bdf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd83c13102334183b37b499344d82620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c73c5954684475eb095751d67a475a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e17de4d4687e4226851a80a88d894f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea3d6fb101bb45ba957fe670193034e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c27e181c48448bd86314e2b8144f84b",
              "IPY_MODEL_bc09cc6c430144428f35ec08f4c2032e",
              "IPY_MODEL_2e0836949f904ccbbd683de7681b2843"
            ],
            "layout": "IPY_MODEL_3cf8b2b91cc844039ff2a08b02c23c85"
          }
        },
        "8c27e181c48448bd86314e2b8144f84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd9ab5fe3f1346619ad708f4b181969a",
            "placeholder": "​",
            "style": "IPY_MODEL_4d0238eaec454bfdb0ad258695bcc51d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bc09cc6c430144428f35ec08f4c2032e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4dc2ea640e544a7bcee521f0e51eb89",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd32fd590db0487486fe6240cca34280",
            "value": 2
          }
        },
        "2e0836949f904ccbbd683de7681b2843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9636b0e17f6c450d8dad0b894442ebe1",
            "placeholder": "​",
            "style": "IPY_MODEL_c1f896cfb22b4b98aa7fdaa762e6656a",
            "value": " 2/2 [00:49&lt;00:00, 22.56s/it]"
          }
        },
        "3cf8b2b91cc844039ff2a08b02c23c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd9ab5fe3f1346619ad708f4b181969a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d0238eaec454bfdb0ad258695bcc51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4dc2ea640e544a7bcee521f0e51eb89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd32fd590db0487486fe6240cca34280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9636b0e17f6c450d8dad0b894442ebe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1f896cfb22b4b98aa7fdaa762e6656a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3284b44209d848eca50e2ef6d18c6b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cbf6c007ba049da9b0d7ab41e54335c",
              "IPY_MODEL_7c36653daaf444e49e5a864b403446a5",
              "IPY_MODEL_484afec5a9b842f6ab720c4a6b24327c"
            ],
            "layout": "IPY_MODEL_3487b437b0b44a6b945ce9501b603d68"
          }
        },
        "8cbf6c007ba049da9b0d7ab41e54335c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e7e0e0a4bd34fb1a06793a46ea5cacb",
            "placeholder": "​",
            "style": "IPY_MODEL_b3018a84b7fa47aaa242eda885adfe10",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7c36653daaf444e49e5a864b403446a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f77bb5aa19f482bb32ae03249819ad0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c28ad206d8964b8ab58e3a79478250a2",
            "value": 2
          }
        },
        "484afec5a9b842f6ab720c4a6b24327c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9586f76960481ea7118bd6f3d1ce71",
            "placeholder": "​",
            "style": "IPY_MODEL_a094d3476fc2442ebd9b9a00ab03bee6",
            "value": " 2/2 [00:10&lt;00:00,  4.85s/it]"
          }
        },
        "3487b437b0b44a6b945ce9501b603d68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e7e0e0a4bd34fb1a06793a46ea5cacb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3018a84b7fa47aaa242eda885adfe10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f77bb5aa19f482bb32ae03249819ad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28ad206d8964b8ab58e3a79478250a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f9586f76960481ea7118bd6f3d1ce71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a094d3476fc2442ebd9b9a00ab03bee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}