from __future__ import annotations
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from agno.utils.log import logger
from dataclasses import dataclass, field, asdict
from typing import Dict, List, Set, Literal, Optional


@dataclass
class Parameter:
    """Holds all observed values for a given query parameter name.

    Attributes:
        name (str):
            Parameter name
        values (set):
            Non-exhaustive list of unique values for the Parameter (based on Katana)
    """

    name: str
    values: Set[str] = field(default_factory=set)

    def to_dict(self) -> Dict[str, List[str]]:
        return {self.name: sorted(self.values)}


@dataclass
class Endpoint:
    """Represents a single API endpoint associated with a specific URL path and HTTP method.

    Attributes:
        method (Optional[Literal["GET", "POST", "PUT", "PATCH", "DELETE", "HEAD", "OPTIONS", "TRACE"]]):
            The HTTP method used for this endpoint. Currently only GET and POST are really supported.
            Defaults to None.

        enctype (str):
            The encoding type used for the request body (e.g., 'application/json', 'multipart/form-data').
            Defaults to None.

        parameters (Dict[str, Parameter]):
            A dictionary of parameters accepted by the endpoint, keyed by parameter name.

        parameter_sets (set):
            A set of parameter groupings or combinations that define possible valid configurations of parameters.
    """

    method: Optional[
        Literal["GET", "POST", "PUT", "PATCH", "DELETE", "HEAD", "OPTIONS", "TRACE"]
    ] = None
    enctype: str = None
    parameters: Dict[str, Parameter] = field(default_factory=dict)
    parameter_sets: set = field(default_factory=set)

    def add_parameter(self, key, vals: Optional[List] = None):
        vals = vals or []
        try:
            if not isinstance(key, str):
                key = str(key)
        except:
            logger.error(f"Key {key} just be string serializable")

        if key not in self.parameters:
            self.parameters[key] = Parameter(name=key)
        for v in vals:
            try:
                v = str(v)
            except:
                logger.error(f"Value {v} for key {key} must be string serializable")
            self.parameters[key].values.add(v)

    def to_dict(self):
        return {
            "enctype": self.enctype,
            "parameters": [
                parameter.to_dict() for _, parameter in self.parameters.items()
            ],
            "parameter_sets": sorted(self.parameter_sets),
        }


@dataclass
class PrefixTreeNode:
    """
    A node in the PrefixTreeNode, with the following parameters:
    - children: next path components
    - queries: aggregated QueryParameter objects
    - is_endpoint: true if at least one URL ended here
    - link_type: classification of the URL as one of ['human', 'machine', 'irrelevant']
    - raw_json: relevant fields extracted from Katana JSONL output
    """

    children: Dict[str, PrefixTreeNode] = field(default_factory=dict)
    endpoints: Dict[str, Endpoint] = field(default_factory=dict)
    link_type: Literal["human", "machine", "irrelevant"] = "irrelevant"
    raw_json: Dict = field(default_factory=dict)
    is_endpoint: bool = False

    def _get_or_add_endpoint(
        self,
        method: Literal[
            "GET", "POST", "PUT", "PATCH", "DELETE", "HEAD", "OPTIONS", "TRACE"
        ],
    ):
        """Convenience function that retrieves an endpoint, and creates a new one if it does not exist yet."""
        if method not in self.endpoints:
            self.endpoints[method] = Endpoint(method=method)
        return self.endpoints[method]

    def add_parameter_from_query_string(
        self, raw_query: str, enctype: Optional[str] = None
    ) -> None:
        """Adds parameters to `GET` method under self.endpoints based on provided query string via urllib.parse_qs. Only to be used for GET methods."""

        endpoint = self._get_or_add_endpoint("GET")
        if enctype:
            endpoint.enctype = enctype

        # Deduplicate and index individual query parameters and unique values for each parameter
        parsed = parse_qs(raw_query, keep_blank_values=False)
        for key, vals in parsed.items():
            endpoint.add_parameter(key=key, vals=vals)

        # Deduplicate and index set of query parameters; a set of query parameters is always sorted lexicographically to catch duplicates
        flattened = [(key, v) for key, vals in parsed.items() for v in vals]
        sorted_query = urlencode(sorted(flattened))
        endpoint.parameter_sets.add(sorted_query)

    def add_parameter(
        self,
        method: Literal[
            "GET", "POST", "PUT", "PATCH", "DELETE", "HEAD", "OPTIONS", "TRACE"
        ],
        parameters: List[str],
        enctype: Optional[str] = None,
    ) -> None:
        """Adds parameters to self.endpoints under a specified method.

        Unlike self.add_parameter_from_query_string, this method manually indexes parameters and does not infer them for a query string.
        """

        # Currently only support GET and POST from Katana output to avoid unexpected behavior
        if method in ["GET", "POST"]:
            endpoint = self._get_or_add_endpoint(method)
            if enctype:
                endpoint.enctype = enctype

            for key in parameters:
                endpoint.add_parameter(key=key)
        else:
            logger.error(f"{method} endpoints not supported by PrefixTreeNode yet.")

    def to_dict(self, return_raw_json: bool = False) -> Dict:
        _d = {
            "is_endpoint": self.is_endpoint,
            "link_type": self.link_type,
            "endpoints": {
                method: endpoint.to_dict()
                for method, endpoint in self.endpoints.items()
            },
            "children": {
                part: child.to_dict(return_raw_json)
                for part, child in self.children.items()
            },
        }

        if return_raw_json:
            _d.update({"raw_json": self.raw_json})
        return _d


@dataclass
class PrefixTree:
    """
    Models emuneration of URL paths as a sequence of prefixes, where each PrefixTreeNode is a subsequent path with optional query parameters.

    For example:
    Node(http://example.com)
        ↳ /abc
            ↳/api
            ↳/login
        ↳ /def
            ↳ ...
    """

    root: PrefixTreeNode = field(default_factory=PrefixTreeNode)
    domain: Optional[str] = None

    def insert(self, url: str, raw_json: Dict = None) -> None:
        """
        Insert new URL to the tree based on parsed Katana outputs.

        If a url does not exist in the tree, a new PrefixTreeNode will be created.
        If a query string exists in the url, parameter values will be inferred and indexed using PrefixTreeNode.add_parameter_from_query_string.
        If raw_json contains forms and xhr_requests scraped by Katana, they will be manually indexed using PrefixTreeNode.add_parameter.
        """

        # Helper functions
        def parse_url(url):
            """Helper function to parse and split a url into route components."""
            parsed = urlparse(url)
            parts = [p for p in parsed.path.strip("/").split("/") if p]
            return parsed, parts

        def _insert(parts):
            """Helper function to insert a new url to the tree."""
            node = self.root
            for part in parts:
                if part not in node.children:
                    node.children[part] = PrefixTreeNode()
                node = node.children[part]
            node.is_endpoint = True
            node.link_type = classify_link(
                '/'.join([self.domain] + parts)
            )
            return node

        # Method body

        # Parse url and insert to tree
        raw_json = raw_json or {}
        parsed, parts = parse_url(url)

        # Designate the first seen URL as the domain
        # If there are conflicting domains, this is handled by PrefixForest, which should create a new PrefixTree
        if self.domain is None:
            # Parse scheme+netloc as the full domain
            if parsed.scheme and parsed.netloc:
                self.domain = urlunparse((parsed.scheme, parsed.netloc, "", "", "", ""))
            else:
                raise ValueError(f"Invalid URL: missing scheme or netloc in {url!r}")
            # If self.domain already exists, checks if new url has the same domain
            # For now, assumes all endpoints follow the same scheme
        elif parsed.netloc != urlparse(self.domain).netloc:
            raise ValueError(
                f"URL domain {urlunparse((parsed.scheme, parsed.netloc, '', '', '', ''))!r} "
                f"does not match tree domain {self.domain!r}"
            )
        new_node = _insert(parts)

        # Include raw_json if available
        if raw_json is not None:
            new_node.raw_json.update(raw_json)

        # Indexes GET endpoint for url, along with any query string parameters
        new_node.add_parameter_from_query_string(
            parsed.query, enctype=raw_json.get("enctype")
        )

        # If raw_json contains `forms` or `xhr_requests` scraped by Katana, they are also indexed to the tree
        # Note that form and xhr_request endpoints may point to completely different urls, hence they must be independently indexed as a potentially new node
        form_endpoints = raw_json.get("forms") or []
        xhr_request_endpoints = raw_json.get("xhr_requests") or []
        all_endpoints = form_endpoints + xhr_request_endpoints
        for ep in all_endpoints:
            ep_url = ep.get("action") or ep.get(
                "endpoint"
            )  # `action` is for `forms`, `endpoint` is for `xhr_requests`
            method = ep.get("method")
            parameters = ep.get("parameters", [])

            # if POST request, index using node.add_parameter
            if all([ep_url, method == "POST"]):
                parsed, parts = parse_url(ep_url)
                if parsed.query:
                    parts[-1] += (
                        "?" + parsed.query
                    )  # for POST requests, we reappend the parsed query string since it is likely interpreted as a route, not an actual query string
                new_node = _insert(parts)
                new_node.add_parameter(
                    method=method,
                    parameters=parameters,
                    enctype=ep.get("enctype")
                    or ep.get("headers", {}).get(
                        "Accept"
                    ),  # `enctype` is for `forms`, `headers.Accept` is for `xhr_requests`
                )

            # if GET request, index using node.add_parameter_from_query_string
            if all([ep_url, method == "GET"]):
                parsed, parts = parse_url(ep_url)
                new_node = _insert(parts)
                new_node.add_parameter_from_query_string(
                    parsed.query,
                    enctype=ep.get("enctype") or ep.get("headers", {}).get("Accept"),
                )

    def is_path(self, url: str) -> bool:
        """Checks if a url is a valid path based on the tree's current state."""
        parsed = urlparse(url)
        if parsed.netloc != self.domain:
            return False
        node = self.root
        for part in [p for p in parsed.path.strip("/").split("/") if p]:
            if part not in node.children:
                return False
            node = node.children[part]
        return node.is_endpoint

    def get_endpoints(
        self,
        exclude_link_types: List[Literal["human", "machine", "irrelevant"]] = None,
        return_metadata: bool = False,
    ) -> Dict:
        """Traverse tree and return all endpoints along with metadata. Optionally exclude any of ['human','machine','irrelevant'] links."""

        if exclude_link_types is None:
            exclude_link_types = []

        out = []

        def _recurse(
            node: PrefixTreeNode, path_parts: List[str], return_metadata: bool
        ):
            if node.is_endpoint:
                if node.link_type not in exclude_link_types:
                    _d = {
                        "url": "/".join(path_parts),
                        "endpoints": {
                            k: v.to_dict() for k, v in node.endpoints.items()
                        },
                    }
                    if return_metadata:
                        _d.update({"metadata": node.raw_json})
                    out.append(_d)

            for part, child in node.children.items():
                _recurse(child, path_parts + [part], return_metadata)

        _recurse(self.root, [self.domain], return_metadata)
        return out

    def to_dict(self, return_raw_json: bool = False) -> Dict:
        return {"domain": self.domain, "tree": self.root.to_dict(return_raw_json)}

    def display(self, node=None, prefix="", depth=0):
        """Recursively prints the tree for visualization, including current node."""

        if node is None:
            node = self.root
            prefix = self.domain  # Start with domain as prefix for root

        indent = "   " * depth  # Indentation based on depth
        classification = f"[endpoint, {node.link_type}]" if node.is_endpoint else ""
        print(
            f"{indent}{prefix} {classification}"
        )  # MUST use print, agno logger will not work because of its in-built formatting

        if node.endpoints:
            print(f"{indent}  ↳ Endpoints:")
            for k, v in node.endpoints.items():
                print(f"{indent}    Method: {k}")
                if hasattr(v, "parameters"):
                    values_string = "\n".join(
                        [f"{indent}      {param}" for param in v.parameters]
                    )
                    print(f"{indent}    Values:\n{values_string}")

        for part, child in node.children.items():
            new_prefix = f"{prefix}/{part}"
            self.display(child, new_prefix, depth + 1)


@dataclass
class PrefixForest:
    """
    Wrapper class that manages multiple PrefixTree instances, one per domain.
    """

    trees: Dict[str, PrefixTree] = field(default_factory=dict)

    def insert(self, url: str, raw_json: Dict = None) -> None:
        """
        Insert a URL into the manager. If this domain is new, initialize a PrefixTree for it.
        """
        parsed = urlparse(url)
        domain = parsed.netloc

        # If we find a new domain, create a new tree
        if domain not in self.trees:
            self.trees[domain] = PrefixTree()

        # then insert to the relevant tree
        self.trees[domain].insert(url, raw_json)

    def get_tree(self, domain: str) -> Optional[PrefixTree]:
        """Get a specific tree for a given domain."""
        return self.trees.get(domain)

    def get_domains(self) -> List[str]:
        """Get all indexed domains."""
        return list(self.trees.keys())

    def get_all_endpoints(
        self,
        exclude_link_types: List[Literal["human", "machine", "irrelevant"]] = None,
        return_metadata: bool = False,
    ) -> Dict:
        """
        Get all endpoints from all domains.
        """
        if exclude_link_types is None:
            exclude_link_types = []
        all_eps = []
        for domain, tree in self.trees.items():
            eps = tree.get_endpoints(exclude_link_types, return_metadata)
            all_eps.extend(eps)
        return all_eps

    def to_dict(self) -> Dict[str, Dict]:
        return {domain: tree.to_dict() for domain, tree in self.trees.items()}


def classify_link(href: str) -> str:
    """
    Classify a link into 'human', 'machine', or 'irrelevant'.
    """

    if not href:
        return "irrelevant"

    href = href.lower().strip()

    # Handle special protocols first
    if href.startswith("mailto:") or href.startswith("tel:"):
        return "irrelevant"

    # Remove query and fragment for clean extension checking
    parsed = urlparse(href)
    path = parsed.path

    # Define extension categories
    human_exts = [".html", ".htm"]
    machine_exts = [".json", ".xml", ".js", ".rss", ".atom"]
    irrelevant_exts = [
        ".css",
        ".png",
        ".jpg",
        ".jpeg",
        ".svg",
        ".ico",
        ".pdf",
        ".gif",
        ".webp",
        ".woff",
        ".woff2",
        ".ttf",
        ".eot",
        ".mp4",
        ".mp3",
        ".avi",
    ]

    # Define path patterns for machine-oriented endpoints (more specific patterns)
    machine_path_patterns = [
        "/bundles",
        "/content/css",
        "/content/js",
        "/assets/js",
        "/assets/css",
        "/static/js",
        "/static/css",
        "/dist",
        "/build",
        "/scripts",
        "/stylesheets",
        "/_next",
        "/webpack",
        "/node_modules",
    ]
    
    # Specific filename patterns for machine content
    machine_filename_patterns = [
        "modernizr",
        "msajaxjs",
        "webformsjs", 
        "jsbootstrap",
        "bootstrap",
    ]

    # Define path patterns for irrelevant content
    irrelevant_path_patterns = [
        "/static/images",
        "/images",
        "/img",
        "/fonts",
        "/media",
        "/videos",
        "/static/music",
        "/music",
    ]

    # Check by extension first (most specific)
    for ext in irrelevant_exts:
        if path.endswith(ext):
            return "irrelevant"

    for ext in machine_exts:
        if path.endswith(ext):
            return "machine"

    for ext in human_exts:
        if path.endswith(ext):
            return "human"

    # Check for irrelevant path patterns (before machine patterns to catch static assets)
    for pattern in irrelevant_path_patterns:
        if pattern in path:
            return "irrelevant"

    # Check for machine-oriented path patterns
    for pattern in machine_path_patterns:
        if pattern in path:
            return "machine"
    
    # Check for machine-oriented filename patterns
    filename = path.split("/")[-1] if path else ""
    for pattern in machine_filename_patterns:
        if pattern in filename:
            return "machine"

    # Handle extensionless paths (likely human-readable endpoints)
    if path and not path.endswith('/'):
        last_segment = path.split("/")[-1]
        if last_segment and "." not in last_segment:
            return "human"
    
    # Directory paths (ending with /) are likely human-readable
    if path.endswith('/'):
        return "human"

    # Default fallback
    return "human"
