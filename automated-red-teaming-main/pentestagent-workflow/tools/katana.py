from agno.tools import Toolkit
from agno.utils.functions import cache_result
from agno.utils.log import logger
from tools.katana_utils import PrefixForest
from tools.common import read_jsonl, craft_cookie_header_from_storage_state
from tools.shell_tools.safe_shell_tool import SafeShellTool
import os
import hashlib
from schema.recon import (
    KatanaEndpoint,
    KatanaEndpointMethod,
    KatanaAnalysisOutput,
)
from config.globals import STREAM_RESPONSES
from server.sink import stream_status

@stream_status(
    input_metadata_fields=['target', 'depth'],
    log_output=True
    )
async def run_katana(target: str, storage_state_path: str, depth: int = 1) -> KatanaAnalysisOutput:
    """Run Katana to gather reconnaissance data.
    Args:
        depth (int): Depth of the scan to run with Katana.
    Returns:
        List: List of endpoints extracted from Katana output.

        Output format - list of endpoints with metadata
        [
            {
                "url": "webscraper.io",
                "metadata": {
                    "timestamp": "2025-07-15T15:28:17.050308+08:00",
                    "method": "GET",
                    "endpoint": "https://webscraper.io/",
                    "tag": "a",
                    "attribute":
                    "href",
                    "source": "https://webscraper.io/test-sites",
                    "status": 200,
                    "content_type": "text/html; charset=UTF-8",
                    "content_length": 55581,
                    "cache_control": "max-age=300, public",
                    "hsts": "max-age=47474747; includeSubDomains; preload",
                    "technologies": ["HSTS", "Amazon CloudFront", "Amazon Web Services", "YouTube", "Google Tag Manager", "HTTP/3"]
                },
                "queries": {}
            },
            ...
        ]

    Raises:
        FileNotFoundError: If the Katana output file does not exist.
        ValueError: If the Katana output file is empty.
    """
    
    prefix_forest = PrefixForest()
    # Get the project root directory (assuming this file is in tools/ subdirectory)
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    cache_dir = os.path.join(project_root, ".cache", "katana")
    os.makedirs(cache_dir, exist_ok=True)

    hashed_filename = os.path.join(
        cache_dir, f"{hashlib.sha256(target.encode()).hexdigest()[:10]}.jsonl"
    )  # truncated hash name to map target url to output file

    if not os.path.exists(hashed_filename) or not os.path.getsize(hashed_filename):
        logger.info(
            f"Katana output file {hashed_filename} does not exist or is empty. Running Katana..."
        )
        # Pass command as list instead of f-string to avoid command injections
        katana_cmd = [
            "katana",
            "-u",
            target,  # target url
            "-output",
            hashed_filename,  # output file name
            "-td",  # tech detection
            "-xhr",  # extract xhr request url,method in jsonl output
            "-fx",  # extract form, input, textarea & select elements in jsonl output
            "-jsonl",  # write output in jsonl format; enables verbose output
            "-omit-body",  # omit raw requests/responses from jsonl output
            "-omit-raw",  # omit response body from jsonl output
            "-cos",
            "tools/web_requester/out_of_scope.txt",  # out of scope url regex to be excluded by crawler
            "-delay",
            "5",  # delay between each request
            "-hl",  # headless crawling
            "-nos",  # start headless chrome in --no-sandbox mode
            "-timeout",
            "20",  # time before requests time out; setting too high drastically inflates execution time
            "-d",
            str(depth),  # crawling depth
            "-jc",  # enables javascript file crawling
            "-sf",
            "qurl",  # field to store in per-host output
            "-kf",  # enables crawling of known (default) files
            "-ef",
            "css,png,jpg,bmp,ico,gif,svg,otf,pdf,map,woff,ttf",  # file extensions to ignore
        ]

        # take in cookies
        cookie_header = craft_cookie_header_from_storage_state(target, storage_state_path)
        if cookie_header and cookie_header.header_str:
            katana_cmd.insert(3, "-H")
            katana_cmd.insert(4, cookie_header.header_str)

        logger.info(f"Running Katana command: {' '.join(katana_cmd)}")

        safe_shell_tool = SafeShellTool()
        response = await safe_shell_tool.arun_shell_command(katana_cmd)

        logger.info(f"Response from Katana execution: {response}")

    if not os.path.exists(hashed_filename):
        logger.error(
            f"Katana output file {hashed_filename} does not exist. Check if Katana ran successfully."
        )
        raise FileNotFoundError(f"Katana output file {hashed_filename} does not exist.")

    # If the katana output is empty, return an empty list
    if not os.path.getsize(hashed_filename):
        logger.warning(
            f"Katana output file {hashed_filename} is empty. Returning empty list."
        )
        raise ValueError(f"Katana output file {hashed_filename} is empty.")

    logger.info(f"Reading Katana output from {hashed_filename} and extracting fields.")
    # Read JSONL output from Katana and extract fields
    katana_output = read_jsonl(hashed_filename)
    parsed_katana_output = [extract_katana_fields(row) for row in katana_output]

    # For each row in the katana output, index into our PrefixForest in global state
    for row in parsed_katana_output:
        try:
            prefix_forest.insert(url=row["endpoint"], raw_json=row)
        except Exception as e:
            logger.error(f"Failed to insert {row['endpoint']} with error: {e}")

    # Return list of human-readable endpoints for LLM to analyze
    all_endpoints_with_metadata = prefix_forest.get_all_endpoints(
        exclude_link_types=["machine", "irrelevant"], 
        return_metadata=True
    )

    converted_endpoints_with_metadata = []
    for endpoint in all_endpoints_with_metadata:
        logger.info(f"Sample katana output: {endpoint}")
        # Convert to KatanaAnalysisOutput format
        converted_endpoints_with_metadata.append(
            KatanaEndpoint(
                url=endpoint["url"],
                technologies=endpoint["metadata"].get("technologies", []),
                methods=[
                    KatanaEndpointMethod(
                        method=_method,
                        enctype=_endpoint_info["enctype"],
                        parameters=_endpoint_info["parameters"],
                        parameter_sets=_endpoint_info["parameter_sets"],
                    )
                    for _method, _endpoint_info in endpoint["endpoints"].items()
                ],
            )
        )
            
    return KatanaAnalysisOutput(endpoints=converted_endpoints_with_metadata)


def extract_katana_fields(record):
    FIELD_MAP = {
        "timestamp": "timestamp",
        "method": ("request", "method"),
        "endpoint": ("request", "endpoint"),
        "tag": ("request", "tag"),
        "attribute": ("request", "attribute"),
        "source": ("request", "source"),
        "status": ("response", "status_code"),
        "content_type": ("response", "headers", "content-type"),
        "content_length": ("response", "content_length"),
        "cache_control": ("response", "headers", "cache-control"),
        "hsts": ("response", "headers", "strict-transport-security"),
        "technologies": ("response", "technologies"),
        "forms": ("response", "forms"),
        "xhr_requests": ("response", "xhr_requests"),
    }

    out = {}
    for out_key, path in FIELD_MAP.items():
        if isinstance(path, str):
            out[out_key] = record.get(path)
        else:
            val = record
            for p in path:
                val = val.get(p, {})
            out[out_key] = val or None
    return out


# wrapper to make it a toolkit for use as agent tools
class Katana(Toolkit):
    def __init__(self, **kwargs):
        super().__init__(name="katana_tool", tools=[self.crawl], **kwargs)

    @cache_result()
    def crawl(self, target: str, depth: int = 1) -> KatanaAnalysisOutput:
        """
        Run Katana web crawler to discover endpoints and gather reconnaissance data.

        Args:
            target (str): Target URL to crawl
            depth (int): Depth of the scan to run with Katana (default: 1)

        Returns:
            KatanaAnalysisOutput: Output containing discovered endpoints and metadata
        """
        return run_katana(target, depth)
