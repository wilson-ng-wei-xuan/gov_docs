import os, re, json, random, asyncio, signal, base64, copy
from typing import List, Dict, Optional, Any, Tuple
from urllib.parse import urlparse, urljoin

import aiohttp, contextlib
from aiohttp.client_exceptions import ClientError, ClientConnectionError, ClientResponseError
import urllib3

from agno.tools import Toolkit
from agno.utils.log import logger

from schema.request_response import RequestDetails, ResponseDetails, RequestResponsePair
from tools.web_requester.nordvpn_servers import nordvpn_servers
from tools.shell_tools.safe_shell_tool import SafeShellTool
from tools.web_requester.guardrails.common import WebRequesterGuardrail
from tools.web_requester.guardrails.sqli_guardrail import SQLWebRequesterGuardRail
from tools.web_requester.guardrails.open_redirect_guardrail import OpenRedirectWebRequesterGuardRail
from tools.web_requester.guardrails.xxe_guardrail import XXEWebRequesterGuardRail
from tools.guardrails.base import GuardedToolkit
from tools.common import craft_cookie_header_from_storage_state, tag_summarizer_payload
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Common Helper functions and constants
MAX_WEB_REQUESTER_TIMEOUT = int(os.getenv("MAX_WEB_REQUESTER_TIMEOUT", 120))
MAX_WEB_REQUESTER_INPUT = int(os.getenv("MAX_WEB_REQUESTER_INPUT", 2000))

# Guarded toolkit

class WebRequesterGuardedToolkit(GuardedToolkit):
    """
    Guarded WebRequester toolkit for automated pentesting.

    Wraps a standard WebRequester with specific GuardRails depending on the type of
    vulnerability being tested. GuardRails apply **pre-checks** to validate payloads
    before sending requests and **post-processing** to handle responses safely.

    Args:
        vuln_type (str, optional): Type of vulnerability to test. Determines which
            GuardRail is applied. Options:
            - "general": No specific GuardRail, generic checks only.
            - "sqli": Uses SQLWebRequesterGuardRail.
            - "xss": Placeholder for XSS GuardRail.
            - "xxe": Uses XXEWebRequesterGuardRail.
            - "open_redirect": Uses OpenRedirectWebRequesterGuardRail.
        pre_flags (set[str] | str, optional): Pre-checks to enable (default "all").
            - "all": enable all available pre-check functions for the chosen guardrail.
            - "none": disable all pre-check functions.
            - set of strings: select specific pre-checks.
            - Available pre-checks per vuln_type:
                SQLi:
                    - "file_modification": blocks destructive schema changes (DROP, ALTER, etc.)
                    - "fs_os_interaction": blocks LOAD_FILE, INTO OUTFILE, xp_cmdshell, etc.
                    - "dos": blocks long SLEEP/WAITFOR and recursive CTEs
                XXE:
                    - "rce_check": blocks harmful elements that may attempt RCE
                    - "dos": blocks recursive entity expansions or Billion Laughs
                    - "cookie_use": detects manipulation or exfiltration of cookies/session data
                Open Redirect:
                    - "domain_check": ensures redirects only go to safe domains
                XSS:
                    - Placeholder for future pre-checks
        post_flags (set[str] | str, optional): Post-processing steps to enable (default "all").
            - "all": enable all post-processing functions available for the guardrail.
            - "none": disable all post-processing functions.
            - set of strings: choose specific post-processing steps (currently none are defined for any guardrail).

        timeout (int, optional): Maximum tool timeout in seconds (default MAX_WEB_REQUESTER_TIMEOUT).
        safe_domains (List[str], optional): Only used for open_redirect guardrail to enforce domain whitelist.
        start_with_vpn (bool, optional): If True, the WebRequester starts via VPN.
        use_vpn (bool, optional): If True, all requests are routed through VPN.
        proxies (Optional[str], optional): Optional proxy configuration.
        max_redirects (int, optional): Maximum number of HTTP redirects to follow (default 10).
        *args, **kwargs: Additional parameters passed to the underlying base WebRequesterTool.

    Note:
        - For XXE payloads, pre-checks focus on blocking RCE and DoS while optionally
          allowing file and HTTP(S) exfiltration.
        - For SQLi payloads, pre-checks focus on preventing destructive modifications,
          OS commands, and DoS via sleep/wait commands or recursive CTEs.
        - For OpenRedirect payloads, pre-checks ensure redirection is limited to safe domains.
        - Post-processing hooks are currently not defined but reserved for future response
          validations or sanitization.
    """
    def __init__(
            self,
            vuln_type: str = "general",
            pre_flags: set[str] | str = "all",
            post_flags: set[str] | str = "all",
            timeout: int = MAX_WEB_REQUESTER_TIMEOUT,
            safe_domains: List[str] = None,
            start_with_vpn: bool = False,
            use_vpn: bool = True,
            proxies: Optional[str] = None,
            max_redirects: int = 10,
            session_id: Optional[str] = None,
            *args, **kwargs):


        if vuln_type == "sqli":
            guardrail, suffix_name = SQLWebRequesterGuardRail(pre_flags=pre_flags, post_flags=post_flags), "sqli_payload"
        elif vuln_type == "xss":
            guardrail, suffix_name = WebRequesterGuardrail(pre_flags=pre_flags, post_flags=post_flags), "xss_payload"
        elif vuln_type == "xxe":
            guardrail, suffix_name = XXEWebRequesterGuardRail(pre_flags=pre_flags, post_flags=post_flags), "xxe_payload"
        elif vuln_type == "open_redirect":
            guardrail, suffix_name = OpenRedirectWebRequesterGuardRail(pre_flags=pre_flags, post_flags=post_flags, safe_domains=safe_domains), "open_redirect"
        else:
            guardrail, suffix_name = WebRequesterGuardrail(pre_flags=pre_flags, post_flags=post_flags), "general"

        # TODO: Add timeout after merging with new rebases
        super().__init__(base_tool=WebRequesterTool(use_vpn=use_vpn, start_with_vpn=start_with_vpn, proxies=proxies, max_redirects=max_redirects, session_id=session_id),
                         guardrail=guardrail,
                         custom_name_suffix=suffix_name,
                         tool_timeout=timeout,
                         *args, **kwargs)

# Tool

class WebRequesterTool(Toolkit):
    """
    Async HTTP requester using aiohttp, with retries, manual redirect following,
    WAF/CAPTCHA detection, and optional NordVPN rotation. Cooperates with a
    session Runtime (rs) that may provide stop_event and pids.
    """

    user_agent = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/137.0.0.0 Safari/537.36"
    )

    default_headers: Dict[str, str] = {
        "Sec-Ch-Ua": '"Not)A;Brand";v="8", "Chromium";v="138"',
        "Sec-Ch-Ua-Mobile": "?0",
        "Sec-Ch-Ua-Platform": '"Windows"',
        "Accept-Language": "en-US,en;q=0.9",
        "Upgrade-Insecure-Requests": "1",
        "User-Agent": user_agent,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-User": "?1",
        "Sec-Fetch-Dest": "document",
        "Accept-Encoding": "gzip, deflate, br",
        "Priority": "u=0, i",
    }

    def __init__(
        self,
        *,
        rs=None,                          # optional RuntimeSession with stop_event/pids
        start_with_vpn: bool = False,
        use_vpn: bool = True,
        proxies: Optional[str] = None,    # "host:port" -> will be turned into http://host:port
        max_redirects: int = 10,
        session_id: Optional[str] = None,
        **kwargs,
    ):
        self.rs = rs
        self.result = {}
        self.redirects: List[RequestResponsePair] = []
        self.use_cookies = True
        self.session_id = session_id

        self.nordvpn_started = False
        self.start_with_vpn = start_with_vpn
        self.use_vpn = use_vpn
        self.max_redirects = max_redirects

        # proxy string for aiohttp
        self.proxy: Optional[str] = None
        if proxies:
            self.proxy = f"http://{proxies}"

        # Load WAF signatures
        with open("./tools/web_requester/waf_signatures.json", "r") as f:
            self.WAF_SIGNATURES = json.load(f)

        # optionally init vpn state
        self._init_task: Optional[asyncio.Task] = None
        if self.start_with_vpn:
            # don't block constructor: kick an async init; callers can await arun which will wait if needed
            self._init_task = asyncio.create_task(self.init_nordvpn(True))

        # expose both async and sync tool entrypoints
        super().__init__(name="web_requester", tools=[self.arun_web_requester])

    # ---------- small utils ----------

    @staticmethod
    def normalize_headers(headers: Dict[str, str]) -> Dict[str, str]:
        return {str(k).lower(): str(v).lower() for k, v in headers.items()}

    @staticmethod
    def remove_img_tags(body: Any) -> Any:
        if not isinstance(body, str):
            try:
                body = str(body)
            except Exception:
                return body
        pattern = re.compile(
            r'<img\s[^>]*src=["\']data:image/(?:jpeg|png|gif|svg\+xml|webp);\s*(?:base64|utf-8|utf-16|ascii)[^"\']*["\'][^>]*>',
            re.IGNORECASE,
        )
        return pattern.sub("", body).strip()

    def check_body(self, body: Any, content_type: Optional[str] = None) -> str:
        if body is None:
            return ""
        if isinstance(body, (dict, list)):
            try:
                return json.dumps(body)
            except Exception:
                return str(body)
        if isinstance(body, set):
            try:
                return json.dumps(list(body))
            except Exception:
                return str(body)
        if isinstance(body, bytes):
            if content_type and "multipart/form-data" in content_type:
                return base64.b64encode(body).decode("ascii")
            try:
                return body.decode("utf-8", errors="replace")
            except Exception:
                return base64.b64encode(body).decode("ascii")
        return str(body)

    def detect_block_or_captcha(self, html: str) -> bool:
        html = (html or "").lower()
        block_keywords = [
            "detected unusual traffic from your computer network",
            "www.google.com/sorry",
            "requests, and not a robot",
            "verify you are",
        ]
        return any(k in html for k in block_keywords)

    def detect_waf(self, response_body: str, status_code: int, response_headers: Dict[str, str]):
        matched_wafs = []
        body = (response_body or "").lower()
        headers = self.normalize_headers(response_headers or {})

        for waf_name, waf_data in self.WAF_SIGNATURES.items():
            match_count = 0
            min_score = waf_data["min_score"]

            if waf_data.get("code"):
                for code_pattern in str(waf_data["code"]).split("|"):
                    try:
                        if int(code_pattern) == int(status_code):
                            match_count += 1
                            if match_count >= min_score:
                                break
                    except Exception:
                        continue

            if waf_data.get("headers"):
                for header_pattern in str(waf_data["headers"]).split("|"):
                    header_pattern = header_pattern.strip().lower()
                    # very basic contains check over "k: v"
                    if header_pattern in " ".join([f"{k}: {v}" for k, v in headers.items()]):
                        match_count += 1
                        if match_count >= min_score:
                            break

            if waf_data.get("page"):
                for page_pattern in str(waf_data["page"]).split("|"):
                    page_pattern = page_pattern.strip().lower()
                    if page_pattern in body:
                        match_count += 1
                        if match_count >= min_score:
                            break

            if match_count >= min_score:
                matched_wafs.append(waf_name)

        return matched_wafs if matched_wafs else False

    async def backoff(self, attempt: int, *, base: float = 1.0, cap: float = 30.0) -> None:
        """
        Exponential backoff with optional full jitter, capped at `cap` seconds.
        - attempt: 0,1,2,... (e.g., number of consecutive 429s or retries)
        - base: starting delay (seconds)
        - cap: maximum delay (seconds)
        """
        # exponential delay, capped
        delay = min(cap, base * (2 ** max(0, attempt)))

        logger.info(f"We are likely rate limited. Backing off for {delay:.1f}s (attempt {attempt})...")

        # Cooperative sleep: exit early if a stop is requested
        stop_event = getattr(getattr(self, "rs", None), "stop_event", None)
        if stop_event is not None:
            try:
                await asyncio.wait_for(stop_event.wait(), timeout=delay)
                return  # stop requested
            except asyncio.TimeoutError:
                return  # completed the delay
        else:
            await asyncio.sleep(delay)

    def _stopping(self) -> bool:
        return bool(getattr(self.rs, "stop_event", None) and self.rs.stop_event.is_set())

    # ---------- async process runner for nordvpn control ----------

    async def _spawn(self, *cmd: str, timeout: float = 20.0) -> Tuple[int, str, str]:
        """
        Run a short-lived CLI (nordvpn, chattr, etc.) in its own process group and
        return (rc, stdout, stderr).
        """
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            preexec_fn=os.setsid,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        if self.rs and getattr(self.rs, "pids", None) is not None:
            self.rs.pids.add(proc.pid)
        try:
            try:
                await asyncio.wait_for(proc.wait(), timeout=timeout)
            except asyncio.TimeoutError:
                try:
                    os.killpg(os.getpgid(proc.pid), signal.SIGTERM)
                except ProcessLookupError:
                    pass
                try:
                    await asyncio.wait_for(proc.wait(), 2)
                except asyncio.TimeoutError:
                    try:
                        os.killpg(os.getpgid(proc.pid), signal.SIGKILL)
                    except ProcessLookupError:
                        pass
            out = (await proc.stdout.read()).decode("utf-8", "ignore") if proc.stdout else ""
            err = (await proc.stderr.read()).decode("utf-8", "ignore") if proc.stderr else ""
            return proc.returncode or 0, out, err
        finally:
            if self.rs and getattr(self.rs, "pids", None) is not None:
                self.rs.pids.discard(proc.pid)

    async def check_nordvpn_status(self) -> Optional[bool]:
        rc, out, err = await self._spawn("nordvpn", "status", timeout=15)
        if "Status: Connected" in out:
            server = re.search(r"Server:\s*(.+)", out)
            hostname = re.search(r"Hostname:\s*(.+)", out)
            logger.info(
                f"NordVPN status: Connected to {server.group(1).strip() if server else '?'} "
                f"({hostname.group(1).strip() if hostname else '?'})"
            )
            self.nordvpn_started = True
            return True
        if "Status: Disconnect" in out:
            logger.info("NordVPN status: Disconnected")
            self.nordvpn_started = True
            return False
        logger.error(f"Failed to read NordVPN status.\nSTDOUT: {out}\nSTDERR: {err}")
        return None

    async def init_nordvpn(self, start: bool) -> bool:
        if not self.nordvpn_started:
            await self._spawn("/etc/init.d/nordvpn", "start", timeout=20)
            await asyncio.sleep(1.5)
            self.nordvpn_started = True

        if start:
            await self._spawn("nordvpn", "connect", timeout=30)
        else:
            await self._spawn("nordvpn", "disconnect", timeout=30)

        await self._spawn("chattr", "-i", "/etc/resolv.conf", timeout=10)
        await asyncio.sleep(1.5)
        status = await self.check_nordvpn_status()
        return bool(status) if status is not None else False

    async def rotate_vpn(self) -> bool:
        logger.info("Rotating NordVPN server...")
        try:
            for attempt in range(5):
                if self._stopping():
                    return False
                nordvpn_server = random.choice(nordvpn_servers)
                # try servers until we don't see "does not support"
                while True:
                    rc, out, err = await self._spawn("nordvpn", "connect", nordvpn_server, timeout=45)
                    if "does not support" in out or "does not support" in err:
                        logger.info(f"{nordvpn_server} not supported, choosing another server...")
                        nordvpn_server = random.choice(nordvpn_servers)
                        continue
                    break
                await self._spawn("chattr", "-i", "/etc/resolv.conf", timeout=10)
                for _ in range(4):
                    if self._stopping():
                        return False
                    await asyncio.sleep(1.5)
                    ok = await self.check_nordvpn_status()
                    if ok is True:
                        return True
                logger.warning(f"Failed to connect to {nordvpn_server} quickly; trying another...")
            logger.error("VPN rotation failed after 5 attempts.")
            return False
        except Exception as e:
            logger.exception(f"VPN rotation failed: {e}")
            return False

    # ---------- core async HTTP ----------
    def parse_http_response(self, raw: str) -> Tuple[List[str], str]:
        """
        Parse a raw HTTP response string into header blocks and body.

        Returns:
            header_blocks: list of header sections (str)
            body: the remainder as a string
        """

        # Normalize line endings
        raw = raw.replace("\r\n", "\n")

        header_blocks = []
        remaining = raw

        while remaining.startswith("HTTP/"):
            # Split only at the first empty line
            if "\n\n" not in remaining:
                break
            headers, remaining = remaining.split("\n\n", 1)
            header_blocks.append(headers)

        body = remaining
        return header_blocks, body

    async def fetch_with_curl(
        self,
        url: str,
        method: str = "GET",
        headers: Optional[Dict[str, str]] = None,
        data: Any = None,
    ) -> Tuple[int, str, Dict[str, str], str, str]:
        """
        Use SafeShellTool to perform HTTP request via curl.
        Returns: (status_code, body, headers_dict, resp_url, resp_method)
        """
        # Build curl command
        cmd = [
            "curl", "-sS", "-i", "-L", "-X", method.upper(),
            "--compressed", "--fail-with-body", "--no-remote-name",
        ]

        # Proxy
        if self.proxy:
            cmd.extend(["-x", self.proxy, "--proxytunnel", "-k"])

        # Add headers
        if headers:
            for k, v in headers.items():
                cmd += ["-H", f"{k}: {v}"]

        # Add data
        if data:
            if isinstance(data, (dict, list)):
                cmd += ["-d", json.dumps(data)]
            else:
                cmd += ["-d", str(data)]

        # Target URL
        cmd.append(url)

        try:
            safe_shell = SafeShellTool()
            out_str = await safe_shell.arun_shell_command(cmd)
        except Exception as e:
            logger.info(f"Error when using curl for web_requester: {e}")
            raise

        # --- Manual HTTP parsing ---
        header_blocks, body = self.parse_http_response(out_str)

        # Remove proxy CONNECT blocks if present
        if self.proxy:
            header_blocks = [hb for hb in header_blocks if not hb.startswith("HTTP/1.1 200 Connection established")]

        status_code = 0
        headers_dict: Dict[str, str] = {}
        resp_url = url
        last_url = url

        for header_block in header_blocks:
            lines = header_block.split("\n")
            if not lines or not lines[0].startswith("HTTP/"):
                continue

            # Status code
            try:
                status_code = int(lines[0].split()[1])
            except Exception:
                status_code = 0

            # Headers
            temp_headers = {}
            for h in lines[1:]:
                if ":" in h:
                    k, v = h.split(":", 1)
                    temp_headers[k.strip().lower()] = v.strip()
            headers_dict = temp_headers  # keep final hop headers

            # Redirect handling
            if "location" in temp_headers:
                resp_url = urljoin(last_url, temp_headers["location"])
            last_url = resp_url

        resp_method = method.upper()

        return status_code, body, headers_dict, resp_url, resp_method

    async def _manual_redirect(
        self,
        session: aiohttp.ClientSession,
        url: str,
        method: str,
        headers: Dict[str, str],
        data: Any,
    ) -> Optional[List[RequestResponsePair]]:
        """
        Manually follow redirects up to self.max_redirects, with retry logic and block detection.
        Returns the redirect chain (list of RequestResponsePair) or None on complete failure.
        """
        current_url = url
        redirects: List[RequestResponsePair] = []

        for hop in range(self.max_redirects):
            # retry up to 5 times per hop
            req_headers = dict(headers)
            for attempt in range(5):
                if self._stopping():
                    raise asyncio.CancelledError()

                logger.info(f"[{hop+1}/~{self.max_redirects}] Requesting: {current_url}")

                send_json = None
                send_data = None
                content_type = req_headers.get("Content-Type", "")

                if data is None:
                    send_json = None
                    send_data = None
                elif "application/json" in (content_type or "") and isinstance(data, (dict, list)):
                    send_json = data
                else:
                    # may be urlencoded, multipart, or raw string/bytes
                    send_data = data

                try:
                    # Run the request; allow_redirects=False to handle manually
                    try:
                        async with session.request(
                            method.upper(),
                            current_url,
                            headers=req_headers,
                            json=send_json,
                            data=send_data,
                            allow_redirects=False,
                            proxy=self.proxy,
                            ssl=False,
                            timeout=aiohttp.ClientTimeout(total=30),
                        ) as resp:
                            status_code = resp.status
                            response_headers = {k: v for k, v in resp.headers.items()}
                            # careful with binary; decode best-effort
                            body_bytes = await resp.read()
                            response_body = body_bytes.decode("utf-8", errors="replace")
                    
                    except asyncio.CancelledError:
                        # Propagating cancellation
                        raise
                    
                    except Exception as e:
                        logger.warning(f"aiohttp failed: {e}, falling back to SafeShellTool (curl)...")

                        status_code, response_body, response_headers, resp.url, resp.method = await asyncio.wait_for(self.fetch_with_curl(
                            current_url, method=method, headers=req_headers, data=data
                        ), timeout=30)

                    # Build request details (what got sent)
                    # (aiohttp doesn't expose the fully serialized request body easily;
                    # we log our computed 'data' instead)
                    request_info = RequestDetails(
                        request_url=str(resp.url),
                        request_method=resp.method,
                        request_headers=dict(req_headers),
                        request_body=self.check_body(send_data if send_data is not None else send_json,
                                                        content_type=req_headers.get("Content-Type")),
                    )

                    response_info = ResponseDetails(
                        response_url=str(resp.url),
                        response_status_code=status_code,
                        response_headers=response_headers,
                        response_body=self.remove_img_tags(response_body),
                    )

                    notes: List[str] = []
                    error: List[str] = []

                    # waf / captcha detection
                    matched_wafs = self.detect_waf(response_info.response_body, status_code, response_headers)
                    if matched_wafs:
                        msg = f"Likely WAF(s): {', '.join(matched_wafs)}"
                        logger.warning(msg)
                        notes.append(msg)

                    if self.detect_block_or_captcha(str(response_info.response_body)):
                        logger.warning("Blocked or CAPTCHA detected. Rotating VPN...")
                        notes.append("Blocked or CAPTCHA detected.")
                        if self.use_vpn:
                            await self.rotate_vpn()
                        # retry same hop
                        continue

                    if status_code == 429:
                        logger.warning("HTTP 429 encountered")
                        notes.append("HTTP 429 encountered")
                        if self.use_vpn:
                            await self.rotate_vpn()
                        else:
                            await self.backoff(attempt)
                        continue

                    if status_code == 469:
                        logger.warning("HTTP 469 encountered")
                        notes.append("HTTP 469 encountered")
                        error.append("Encountered status code 469")

                    if status_code == 403:
                        logger.warning("HTTP 403 encountered")
                        notes.append("HTTP 403 encountered")
                        error.append("Encountered status code 403")

                    # record this hop
                    redirects.append(RequestResponsePair(
                        request=request_info,
                        response=response_info,
                        notes=notes,
                        error=error,
                    ))

                    # If an error was recorded (e.g., 469), stop and return
                    if redirects[-1].error:
                        return redirects

                    # handle redirect Location
                    h = self.normalize_headers(response_headers)
                    loc = h.get("location")
                    if resp.is_redirect or int(status_code) in [300, 301, 302, 303, 305, 306, 307, 308] or loc is not None:
                        data = None  # don't forward POST bodies unless you want to
                        if loc:
                            parsed = urlparse(loc)
                            if parsed.hostname:
                                current_url = loc
                            else:
                                current_url = urljoin(current_url, loc)
                        else:
                            # missing location; stop here
                            return redirects
                        # next hop
                        break
                    else:
                        # not a redirect: done
                        return redirects

                except (ClientConnectionError, ClientError) as e:
                    logger.exception(f"aiohttp error: {e}")
                    if self.use_vpn:
                        await self.rotate_vpn()
                    continue
                except asyncio.TimeoutError:
                    logger.warning("Request timed out. Retrying...")
                    if self.use_vpn:
                        await self.rotate_vpn()
                    continue

            else:
                # all retries failed at this hop
                logger.error(f"Failed after retries at {current_url}")
                redirects.append(RequestResponsePair(
                    request=RequestDetails(
                        request_url=current_url,
                        request_method=method,
                        request_headers=req_headers,
                        request_body=self.check_body(data, req_headers.get("Content-Type")),
                    ),
                    response=None,
                    notes=[],
                    error=["Failed after retries"],
                ))
                return redirects

        logger.warning(f"Reached redirect limit of {self.max_redirects}.")
        return redirects if redirects else None

    async def _prepare_headers(self, url: str, method: str, headers: Optional[Dict[str, str]], data: Any) -> Dict[str, str]:
        parsed = urlparse(url)
        hostname = parsed.hostname or ""
        scheme = parsed.scheme or "https"

        merged: Dict[str, str] = {}
        if isinstance(data, dict):
            merged["Content-Type"] = "application/json"
        elif isinstance(data, str):
            merged["Content-Type"] = "application/x-www-form-urlencoded"

        if self.use_cookies:
            storage_state_path = f"{self.session_id}_storage_state.json"
            cookie_header = craft_cookie_header_from_storage_state(url, storage_state_path)
            if cookie_header and cookie_header.header_dict:
                merged["Cookie"] = cookie_header.header_dict.get("Cookie")

        # Referer / Origin defaulting
        if headers:
            ref = next((v for k, v in headers.items() if k.lower() == "referer"), None)
            org = next((v for k, v in headers.items() if k.lower() == "origin"), None)
        else:
            ref = org = None

        merged["Referer"] = ref or f"{scheme}://{hostname}"
        merged["Origin"]  = org or f"{scheme}://{hostname}"

        merged.update(self.default_headers)
        if headers:
            merged.update(headers)  # caller-specified headers override defaults

        return merged

    # ---------- public tool API ----------
    @tag_summarizer_payload(summarizer_type="rolling", payload_type="web_page")
    async def arun_web_requester(
            self,
            url: str,
            method: str = "GET",
            headers: Optional[Dict[str, str]] = None,
            data: Any = None,
    ) -> str:
        """
        Async tool entrypoint. Returns JSON string (list of RequestResponsePair dicts).
        Races the HTTP work against rs.stop_event so a stop cancels immediately.

        Args:
            url (str): Url to send request to
            method (str): Either GET or POST
            headers (Optional[Dict[str, str]]): Headers to send (on top of default headers)
            data (Any): Data to send across if any
        
        Returns:
            str: JSON string (list of RequestResponsePair dicts)
        """

        if self._init_task:
            # ensure any pending VPN init is done (does nothing if None)
            with contextlib.suppress(Exception):
                await self._init_task
            self._init_task = None

        self.redirects = []
        method = (method or "GET").upper()

        if self._stopping():
            raise asyncio.CancelledError()

        merged_headers = await self._prepare_headers(url, method, headers or {}, data)

        timeout = aiohttp.ClientTimeout(total=35)  # per-request cap; manual redirects applied
        async with aiohttp.ClientSession(timeout=timeout) as session:
            main = asyncio.create_task(
                self._manual_redirect(session, url, method, merged_headers, data)
            )

            # Create a stop watcher only if rs.stop_event exists
            stopper = (
                asyncio.create_task(self.rs.stop_event.wait())
                if getattr(self.rs, "stop_event", None)
                else None
            )

            try:
                # Race: HTTP work vs. stop flag
                done, _ = await asyncio.wait(
                    {main, stopper} if stopper else {main},
                    return_when=asyncio.FIRST_COMPLETED,
                )

                # If stop fired first, cancel the main task and raise CancelledError
                if stopper and stopper in done and main not in done:
                    main.cancel()
                    with contextlib.suppress(asyncio.CancelledError):
                        await main  # let its finally/cleanup run
                    raise asyncio.CancelledError()

                # Otherwise, await the main task for its result (or error)
                chain = await main

                if not chain:
                    # payload might be too loud or aggressive
                    request_info = RequestDetails(
                        request_url=url,
                        request_method=method,
                        request_headers=dict(merged_headers),
                        request_body=self.check_body(
                            data, merged_headers.get("Content-Type")
                        ),
                    )
                    self.redirects = [
                        RequestResponsePair(
                            request=request_info,
                            response=None,
                            notes=[],
                            error=[
                                "The payload is likely too loud or aggressive, consider changing the payload"
                            ],
                        )
                    ]
                else:
                    self.redirects = chain

            except asyncio.CancelledError:
                # propagate cancel; caller may be stopping session
                raise
            except Exception as e:
                logger.exception(f"Exception making request: {e}")
                if self.use_vpn:
                    await self.rotate_vpn()
                # final fallback entry
                request_info = RequestDetails(
                    request_url=url,
                    request_method=method,
                    request_headers=dict(merged_headers),
                    request_body=self.check_body(
                        data, merged_headers.get("Content-Type")
                    ),
                )
                self.redirects = [
                    RequestResponsePair(
                        request=request_info,
                        response=None,
                        notes=[],
                        error=["Max retries exceeded or unexpected error"],
                    )
                ]
            finally:
                # Always cancel the stopper if it exists and is still pending
                if stopper and not stopper.done():
                    stopper.cancel()

        # return JSON string compatible with your previous API
        logger.info(f"result: {json.dumps([r.model_dump() for r in self.redirects])}")
        return json.dumps([r.model_dump() for r in self.redirects])

    # sync shim for older tool callers
    @tag_summarizer_payload(summarizer_type="rolling", payload_type="web_page")
    def run_web_requester(self, url: str, method: str = "GET", headers: Optional[Dict[str, str]] = None, data: Any = None) -> str:
        """
        Synchronous tool entrypoint. Returns JSON string (list of RequestResponsePair dicts).
        Races the HTTP work against rs.stop_event so a stop cancels immediately.
        
        Args:
            url (str): Url to send request to
            method (str): Either GET or POST
            headers (Optional[Dict[str, str]]): Headers to send (on top of default headers)
            data (Any): Data to send across if any
        
        Returns:
            str: JSON string (list of RequestResponsePair dicts)
        """

        try:
            loop = asyncio.get_running_loop()
            in_loop = True
        except RuntimeError:
            in_loop = False

        if not in_loop:
            return asyncio.run(self.arun_web_requester(url, method=method, headers=headers, data=data))
        else:
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:
                fut = ex.submit(lambda: asyncio.run(self.arun_web_requester(url, method=method, headers=headers, data=data)))
                return fut.result()


if __name__ == "__main__":
    web_requester = WebRequesterGuardedToolkit(vuln_type="sqli",use_vpn=False, cache_results=True)
    print([fn.__name__ for fn in web_requester.guardrail.pre_checking])
    response = asyncio.run(web_requester.arun(method="POST", url="https://app.tuv-sud-psb.sg/FireExtSearch/", data="WAITFOR DELAY '13:56:00'"))
    print(response)
