import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse

from agno.tools import Toolkit

class WebCrawler(Toolkit):
    def __init__(self, **kwargs):
        self.max_depth = 1
        self.user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91 Safari/537.36"
        self.visited = []
        self.results = {}
        super().__init__(name="web_scraper_tool", tools=[self.crawl], **kwargs)

    # Check robots.txt
    def can_fetch(self, base_url: str, target_url: str):
        """
        Checks if a link is meant for human visit
        
        Args:
            base_url (str): base url of target website.
            target_url (str): target path.
            
        Returns:
            bool value as to whether a link is crawlable
        """
        rp = RobotFileParser()
        rp.set_url(urljoin(base_url, "/robots.txt"))
        try:
            rp.read()
            return rp.can_fetch(self.user_agent, url)
        except:
            return False

    # Configure Selenium headless browser
    def init_browser(self):
        """
        Initialise headless browser
    
        Returns:
            chrome headless browser
        """
        chrome_options = Options()
        chrome_options.add_argument("--headless=new")  # Run in background
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91 Safari/537.36")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        return driver

    def is_human_link(self, href: str):
        """
        Checks if a link is meant for human visit
        
        Args:
            href (str): href link
        Returns:
            bool value as to whether a link is meant for human visit 
        """
        if not href:
            return False
        href = href.lower()
        ignored_exts = [".css", ".js", ".png", ".jpg", ".jpeg", ".svg", ".ico", ".pdf", ".gif", ".webp"]
        return not any(href.endswith(ext) for ext in ignored_exts) and not href.startswith("mailto:")

    def crawl(self, target_url: str):
        """
        Crawls the given website.

        Args:
            target_url (str): url of target website.
        Returns:
            List of endpoints meant for human.
        """
        driver = self.init_browser()

        from urllib.parse import urlparse


        parsed = urlparse(target_url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"

        self._crawl(driver, base_url, target_url, 0)
        driver.quit()
        print(self.results)
        return self.results

    def _crawl(self, driver: webdriver, base_url: str, target_url: str, depth: int):
        """
        Crawls the given website.

        Args:
            driver (webdriver): headless browser driver.
            base_url (str): base url of target website.
            target_url (str): url of target website.
            depth (int): current depth of recursive crawling
        """
        if depth > self.max_depth or target_url in self.visited:
            return None

        parsed_base_url = urlparse(base_url)
        parsed_target_url = urlparse(target_url)
        
        base_domain = parsed_base_url.hostname
        target_domain = parsed_target_url.hostname

        if not base_domain or not target_domain:
            return  # Or raise an error

        base_domain = base_domain.lower()
        target_domain = target_domain.lower()

        if target_domain != base_domain and not target_domain.endswith("." + base_domain):
            return  # Neither exact match nor subdomain
        
        print(f"Crawling: {target_url}")

        try:
            driver.get(target_url)
            random_number = random.randint(1, 10)
            time.sleep(random_number)  # Allow JS to load

            with open(f"tmp/{driver.title}.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)

            self.results[target_url] = f"tmp/{driver.title}.html"
        except Exception as e:
            print(f"{'  '*depth}⚠️ Error loading {target_url}: {e}")
            return

        try:
            self.visited.append(target_url)
            links = driver.find_elements(By.TAG_NAME, "a")
            hrefs = [link.get_attribute("href") for link in links if self.is_human_link(link.get_attribute("href"))]
            hrefs = [urljoin(base_url, href) for href in hrefs if href]

            # Recurse
            for link in hrefs:
                self._crawl(driver, base_url, link, depth + 1)
        except Exception as e:
            print(f"{'  '*depth}⚠️ Error parsing links on {target_url}: {e}")
    