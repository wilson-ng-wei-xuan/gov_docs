import json
import re
import os
import time
from agno.utils.log import logger
from typing import Dict, Any, NamedTuple
from urllib.parse import urlparse
from custom_agents.summarizer import SUMMARIZER_TYPE
from tools.guardrails.common import PAYLOAD_TYPE

##### DECORATORS #####
def tag_summarizer_payload(summarizer_type: str, payload_type: str):
    """
    Decorator that tags the function with __payload_type__ and __summarizer_type__
    for use during tool storage.
    """
    def decorator(func):
        # Convert strings to Enum values
        func.__summarizer_type__ = SUMMARIZER_TYPE[summarizer_type.upper()]
        func.__payload_type__ = PAYLOAD_TYPE[payload_type.upper()]
        return func
    return decorator


##### UTILITY FUNCTIONS #####
def read_jsonl(file_path):
    """
    Reads a JSONL file as a list of parsed JSON objects.
    """
    items = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError as e:
                raise ValueError(f"Error parsing line: {line!r}\n{e}") from e
            items.append(obj)
    return items

def parse_json_from_output(output: str) -> Dict[str, Any]:
    """
    Extract and parse JSON from tool output, handling various formatting patterns.

    Args:
        output (str): Raw output string containing JSON data

    Returns:
        Parsed JSON object or raises ValueError if parsing fails.
    """
    json_match = re.search(r'```json\s*(.*?)\s*```', output, re.DOTALL)
    if json_match:
        content = json_match.group(1).strip()
    else:
        # Fallback: try to extract content between any ``` markers
        fallback_match = re.search(r'```\s*(.*?)\s*```', output, re.DOTALL)
        if fallback_match:
            content = fallback_match.group(1).strip()
        else:
            content = output.strip()

    try:
        parsed_json = json.loads(content)
        return parsed_json
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON from output: {e}")
        raise ValueError(f"Output content does not contain valid JSON: {output}")


class CookieHeader(NamedTuple):
    header_dict: Dict[str, str]   # {"Cookie": "a=1; b=2"}
    header_str: str               # "Cookie: a=1; b=2"

def craft_cookie_header_from_storage_state(url: str, storage_state_path: str) -> CookieHeader | None:
    """
    Retrieve cookies from browser state stored at storage_state_path and format them as a string.

    Args:
        url (str): Target URL.
        storage_state_path (str): Path to storage state.
    Returns:
        CookieHeader: Dictionary of cookies and String of cookie key/value pairs, separated by ;. E.g. 'Cookie: sessionid=abc123; csrftoken=xyz789'
    """
    try:
        with open(storage_state_path, "r", encoding="utf-8") as f:
            content = json.load(f)
    except FileNotFoundError:
        logger.error("storage_state.json does not exist")
        return None

    parsed = urlparse(url)
    scheme = (parsed.scheme or "http").lower()
    host = (parsed.hostname or "").lower()
    req_path = parsed.path or "/"
    now = time.time()

    def domain_match(h: str, d: str) -> bool:
        # Cookie domains may start with a leading dot for subdomains
        d = (d or "").lstrip(".").lower()
        return h == d or h.endswith("." + d)

    def path_match(p: str, cp: str) -> bool:
        cp = cp or "/"
        if not p.startswith("/"):
            p = "/" + p
        # RFC6265 path-match is simple prefix match
        return p.startswith(cp)

    # Filter cookies like a browser would
    candidates = []
    for c in content.get("cookies", []):
        if c.get("secure") and scheme != "https":
            continue
        if c.get("expires") and c["expires"] > 0 and c["expires"] < now:
            continue
        if not domain_match(host, c.get("domain", "")):
            continue
        if not path_match(req_path, c.get("path", "/")):
            continue
        candidates.append(c)

    # Sort by decreasing path length (browsers do this when sending duplicates)
    candidates.sort(key=lambda c: len(c.get("path", "/")), reverse=True)

    if not candidates:
        logger.error(f"No cookies found for {url}")
        return None

    pairs = [f'{c["name"]}={c["value"]}' for c in candidates]

    value = "; ".join(pairs)
    return CookieHeader(header_dict={"Cookie": value}, header_str=f"Cookie: {value}")
