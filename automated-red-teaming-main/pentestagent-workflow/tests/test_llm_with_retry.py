import pytest
import asyncio
import os
from asyncio import Semaphore
from unittest.mock import patch, MagicMock

from custom_models.litellm_with_retry import LiteLLMOpenAIWithRetry
from agno.models.message import Message
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())


@pytest.fixture
def model():
    """Fixture to create a LiteLLMOpenAIWithRetry instance."""
    return LiteLLMOpenAIWithRetry(
        id=os.getenv("MODEL_ID", "gpt-4o"),
        base_url=os.getenv("OPENAI_API_BASE", "https://litellm-stg.aip.gov.sg"),
        api_key=os.getenv("OPENAI_API_KEY", "test-key")
    )


@pytest.fixture
def sample_query():
    """Fixture for test query."""
    return "What is the latest news on cybersecurity threats?"


@pytest.fixture
def sample_messages(sample_query):
    """Fixture for test messages."""
    return [Message(role="user", content=sample_query)]


class TestLiteLLMOpenAIWithRetry:
    """Test class for LiteLLMOpenAIWithRetry functionality."""

    @pytest.mark.asyncio
    async def test_single_async_request(self, model, sample_messages):
        """Test a single async request."""
        response = await model.ainvoke(
            messages=sample_messages,
            response_format=None,
            tools=None,
            tool_choice=None
        )
        
        assert response is not None
        # Add more specific assertions based on expected response structure
        # assert hasattr(response, 'choices')
        # assert len(response.choices) > 0

    @pytest.mark.asyncio
    async def test_concurrent_requests(self, model, sample_messages):
        """Test concurrent async requests with semaphore."""
        semaphore = Semaphore(5)
        num_requests = 10  # Reduced for testing

        async def make_request():
            async with semaphore:
                return await model.ainvoke(
                    messages=sample_messages,
                    response_format=None,
                    tools=None,
                    tool_choice=None
                )

        tasks = [make_request() for _ in range(num_requests)]
        responses = await asyncio.gather(*tasks)

        for i, response in enumerate(responses, 1):
            print(f"Response {i}: {response}")

        assert len(responses) == num_requests
        assert all(response is not None for response in responses)

    @pytest.mark.asyncio
    async def test_concurrent_requests_with_rate_limiting(self, model, sample_messages):
        """Test concurrent requests that might trigger rate limiting."""
        semaphore = Semaphore(2)  # Very low limit to test rate limiting
        num_requests = 5

        async def make_request(request_id):
            async with semaphore:
                try:
                    response = await model.ainvoke(
                        messages=sample_messages,
                        response_format=None,
                        tools=None,
                        tool_choice=None
                    )
                    return {"id": request_id, "response": response, "success": True}
                except Exception as e:
                    return {"id": request_id, "error": str(e), "success": False}

        tasks = [make_request(i+1) for i in range(num_requests)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        assert len(results) == num_requests
        # At least some requests should succeed
        successful_requests = [r for r in results if isinstance(r, dict) and r.get("success")]
        assert len(successful_requests) > 0

    @pytest.mark.asyncio
    async def test_streaming_async(self, model, sample_messages):
        """Test async streaming functionality."""
        chunks = []
        async for chunk in model.ainvoke_stream(
            messages=sample_messages,
            response_format=None,
            tools=None,
            tool_choice=None
        ):
            chunks.append(chunk)
            print(f"Chunk {len(chunks)}: {chunk}")
            # Limit to first few chunks for testing
            if len(chunks) >= 3:
                break
        
        assert len(chunks) > 0
        # Add more specific assertions based on chunk structure
        assert all(hasattr(chunk, 'choices') for chunk in chunks)

    def test_sync_request(self, model, sample_messages):
        """Test synchronous request."""
        response = model.invoke(
            messages=sample_messages,
            response_format=None,
            tools=None,
            tool_choice=None
        )
        
        assert response is not None

    def test_sync_streaming(self, model, sample_messages):
        """Test synchronous streaming."""
        chunks = []
        for chunk in model.invoke_stream(
            messages=sample_messages,
            response_format=None,
            tools=None,
            tool_choice=None
        ):
            chunks.append(chunk)
            # Limit to first few chunks for testing
            if len(chunks) >= 3:
                break
        
        assert len(chunks) > 0

    @pytest.mark.parametrize("num_concurrent", [1, 3, 5])
    @pytest.mark.asyncio
    async def test_different_concurrency_levels(self, model, sample_messages, num_concurrent):
        """Test different levels of concurrency."""
        semaphore = Semaphore(num_concurrent)

        async def make_request():
            async with semaphore:
                return await model.ainvoke(
                    messages=sample_messages,
                    response_format=None,
                    tools=None,
                    tool_choice=None
                )

        tasks = [make_request() for _ in range(num_concurrent * 2)]
        responses = await asyncio.gather(*tasks)
        
        assert len(responses) == num_concurrent * 2
        assert all(response is not None for response in responses)


# @pytest.mark.integration
class TestIntegrationWithRealAPI:
    """Integration tests that require real API access."""

    @pytest.mark.asyncio
    async def test_full_concurrent_load(self, model, sample_messages):
        """Test the original concurrent load scenario."""
        semaphore = Semaphore(5)
        num_requests = 20

        async def make_request(request_id):
            async with semaphore:
                try:
                    response = await model.ainvoke(
                        messages=sample_messages,
                        response_format=None,
                        tools=None,
                        tool_choice=None
                    )
                    return {"id": request_id, "response": response, "success": True}
                except Exception as e:
                    return {"id": request_id, "error": str(e), "success": False}

        tasks = [make_request(i+1) for i in range(num_requests)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        assert len(results) == num_requests
        
        # Count successful vs failed requests
        successful = [r for r in results if isinstance(r, dict) and r.get("success")]
        failed = [r for r in results if isinstance(r, dict) and not r.get("success")]
        
        print(f"Successful requests: {len(successful)}")
        print(f"Failed requests: {len(failed)}")
        
        # At least 50% should succeed (adjust based on your expectations)
        assert len(successful) >= num_requests * 0.5


if __name__ == "__main__":
    # Run tests with pytest
    pytest.main([__file__, "-v", "-s"])


"""
# Run all tests
uv run pytest tests/test_llm_with_retry.py -v

# Run only unit tests (skip integration tests)
uv run pytest tests/test_llm_with_retry.py -v -m "not integration"

# Run with output
uv run pytest tests/test_llm_with_retry.py -v -s

# Run specific test
uv run pytest tests/test_llm_with_retry.py::TestLiteLLMOpenAIWithRetry::test_single_async_request -v
"""