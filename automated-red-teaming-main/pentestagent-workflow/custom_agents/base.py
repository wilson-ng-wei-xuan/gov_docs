import os
from dotenv import load_dotenv
import inspect 

load_dotenv()

from typing import List, Optional, Union
import contextvars
import asyncio
import logging
from textwrap import dedent
from agno.agent import Agent
from agno.run.messages import RunMessages
from agno.utils.log import logger
from agno.run.response import RunResponse
from agno.models.message import Message
from agno.tools.thinking import ThinkingTools
from custom_models.litellm_with_retry import LiteLLMWithRetry

# from tools.user_input import UserInput
from tools.state_management import GLOBAL_STATE_HANDLER
# from tools.web_requester.web_requester import WebRequesterTool
from tools.browser.playwright_toolkit import PlaywrightToolkit
from custom_agents.summarizer import (
    truncate_messages,
    maybe_remove_tools_without_output,
    SUMMARIZER_TYPE,
    get_session_summarizer
)
from copy import deepcopy
from agno.utils.log import log_debug, log_error, log_info, log_warning
from collections.abc import AsyncIterator
from openai.types.chat.chat_completion_chunk import ChatCompletionChunk

from server.sink import StreamType
from server.sink import _push_chunk_to_sink, stream_status
from server.interactions import get_user_input
from contextlib import aclosing
from config.context import current_model, current_sink, current_node, current_session_id
from tools.tool_retrieval.tool_retrieval import ToolRetrieval, output_storage_hook

logger = logging.getLogger(__name__)
default_model = os.getenv(
    "MODEL_ID", "litellm_proxy/csg.anthropic.claude-sonnet-4-20250514-v1:0"
)

current_model: contextvars.ContextVar[str] = contextvars.ContextVar(
    "current_model", default=None
)


class CustomAgent(Agent):
    def __init__(
        self,
        name: str,
        goal: str,
        success_criteria: str,
        instructions: str,
        sess_id: str,
        model_id: Optional[str] = None,
        context: Optional[dict] = None,
        tools: Optional[List] = None,
        debug_mode: bool = True,
        run_without_session: bool = False,
        *args,
        **kwargs,
    ):
        """
        Extend Agno Agent with context-aware run methods and session history saving.

        Args:
            name (str): Name of the agent.
            goal (str): Goal of the agent.
            success_criteria (str): Success criteria of the agent.
            sess_id (str): Unique session ID number.
            model_id (Optional[str]): ID of the LLM model.
            instructions (str): System prompt / instructions.
            context (Optional[str]): Persistent context.
            tools (Optional[List]): Tool list for agent.
            debug_mode (bool): Enable Agno's debug mode for the agent.
            run_without_session (bool): Allows running the agent without providing a valid existing session id. For debugging purposes.
        """

        self.name = name
        self.run_without_session = run_without_session
        # TODO: what is this?
        # self.AGENT_TO_TASK_NAME_MAPPER = {
        #     "dorking_agent": "run_dorking_agent",
        #     "fingerprint_agent": "run_fingerprint_agent",
        #     "recon_agent": "run_recon_agent",
        #     "sql_agent": "run_sql_agent",
        #     "xxs_agent": "run_xxs_agent",
        #     "xxe_agent": "run_xxe_agent",
        #     "ssrf_agent": "run_ssrf_agent",
        #     "attack_mapper_agent": "run_attack_mapper_agent",
        #     "reporter_agent": "run_reporter_agent",
        # }
        self.sess_id = sess_id

        if model_id is not None:
            self.model_id = model_id
        else:
            _current_model = current_model.get()
            _env_model = os.getenv("MODEL_ID")
            if _current_model is not None:
                log_warning(
                    f"Model not specified for {self.__class__.__name__}, defaulting to model specified in context {_current_model}"
                )
                self.model_id = _current_model
            else:
                log_warning(
                    f"Model not specified for {self.__class__.__name__}, defaulting to model specified in env {_env_model}"
                )
                self.model_id = _env_model

        base_context = {
            "user_input_tool_info": "Whenever required, use UserInput tool to get user's input for further clarification or confirmation",
            "web_requester_tool_info": "Whenever required to send requests to a target website, make use of the WebRequesterTool",
            "playwright_toolkit_info": "If authentication is required, call PlaywrightToolkit.alogin()",
            "tool_retrieval_tool_info": "Whenever a tool output has been summarized and you are unsure of the actual results, use this tool to obtain chunks of the raw data",
            "search_chunks": "Whenever you need to search for keywords in the raw output of a tool, use this function to find the relevant chunks",
            "tool calling": "Always call the async versions of tools",
        }
        self.context = {**base_context, **(context or {})}
        self.tools = list(tools) if tools else []
        base_tools = [
            get_user_input,
            PlaywrightToolkit(),
            ToolRetrieval()
            # ThinkingTools(cache_results=True, add_instructions=True),
        ]
        self.tools.extend(base_tools)
        
        super().__init__(
            name=self.name,
            model=LiteLLMWithRetry(
                id=self.model_id,
                api_base=os.getenv("LITELLM_API_BASE"),
                api_key=os.getenv("LITELLM_API_KEY"),
                temperature=int(os.getenv("MODEL_TEMPERATURE", 0)),
                summarizer=get_session_summarizer(summarizer=SUMMARIZER_TYPE.ROLLING)
            ),
            goal=goal,
            success_criteria=success_criteria,
            instructions=instructions,
            context=self.context,
            tools=self.tools,
            add_name_to_instructions=True,
            add_context=True,
            debug_mode=debug_mode,
            tool_hooks=[output_storage_hook],
            *args,
            **kwargs,
        )

    def _get_current_global_state_str(self) -> str:
        """
        Return current global state as a string.
        Returns:
            str: Current global state.
        """
        current_global_state = GLOBAL_STATE_HANDLER.get_session(self.sess_id)
        return current_global_state.model_dump_json()

    # TODO: what is this?
    def _get_previous_task_history(self) -> str:
        """
        Get previous task history.

        Returns:
            str: Previous task history in JSON string format.
        """
        return ""
    #     current_global_state = GLOBAL_STATE.get_session(self.sess_id)
    #     task_name = self.AGENT_TO_TASK_NAME_MAPPER.get(self.name, None)
    #     if not task_name:
    #         return "No previous task history found."
    #     task_history = current_global_state.get_tasks_by_name(task_name)
    #     task_history_list = TaskList(tasks=task_history)
    #     return task_history_list.model_dump_json()

    def log_token_usage(self, obj: Union[RunMessages, RunResponse]):
        """Logs token usage from Agno response objects if available."""

        try:
            if isinstance(obj, RunMessages):
                input_tokens = 0
                output_tokens = 0
                for message in self.run_messages.messages:
                    input_tokens += int(message.metrics.input_tokens)
                    output_tokens += int(message.metrics.output_tokens)

            elif isinstance(obj, RunResponse):
                input_tokens = int(obj.metrics["input_tokens"][-1])
                output_tokens = int(obj.metrics["output_tokens"][-1])
            else:
                input_tokens = 0
                output_tokens = 0
                log_debug(
                    f"log_token_usage expected RunMessages or RunResponse, received {type(obj)}."
                )

            # if not self.run_without_session:
                # add_tokens_in_global_state(
                #     session_id=self.sess_id,
                #     input_tokens=input_tokens,
                #     output_tokens=output_tokens,
                # )
            log_debug(
                f"{input_tokens} input tokens and {output_tokens} output tokens logged from {self.name}."
            )
        except Exception as e:
            log_warning(f"Failed to log token usage for {self.name}; error: {e}")

    def run(
        self,
        prompt: str,
        include_global_state: bool = True,
        summarize_on_fail: bool = True,
        **kwargs,
    ) -> RunResponse:
        if include_global_state and not self.run_without_session:
            current_global_state_str = self._get_current_global_state_str()
            previous_task_history = self._get_previous_task_history()

            modified_prompt = (
                f"{prompt}\n\n The current global state represents actions already taken by other agents and functions in this workflow. "
                f"The current global state is: {current_global_state_str}. "
                f"Previous task history showcases results from previous task runs. "
                f"Previous task history is: {previous_task_history}"
                f"Be sure to use the asynchronous versions of tools"
                f"After every tool call that is not tool_retrieval, try to use the tool_retrieval tool to get all the chunks"
                f"If a tool output has been summarized, use the tool_retrieval tool to obtain chunks of the raw output from the tool"
            )

        else:
            modified_prompt = prompt

        self.model.summarize_on_fail = summarize_on_fail

        try:
            response = super().run(modified_prompt, **kwargs)
            self.log_token_usage(response)

            return response
        
        except asyncio.CancelledError:
            logger.info("arun cancelled")
            raise

        except:
            self.log_token_usage(
                self.run_messages
            )
            logger.warning("Received an unresolved error")
            raise

    async def arun(
        self,
        prompt: str,
        include_global_state: bool = False,
        summarize_on_fail: bool = False,
        stream=False,
        **kwargs,
    ) -> RunResponse:
        """Asynchronously run the agent.

        If stream=True, returns an async generator that streams response chunks. If a sink is specified as a contextvariable,

        """
        
        @stream_status(
            module_name = f"{self.name}", 
            prefix = "[AGENT]",
            START_TYPE=StreamType.AGENT_START,
            END_TYPE=StreamType.AGENT_END,
            input_metadata_fields=['prompt', 'include_global_state','summarize_on_fail', 'stream'],
            )
        async def broadcaster(
            response_stream: AsyncIterator[ChatCompletionChunk],
            **kwargs
        ) -> AsyncIterator[ChatCompletionChunk]:
            """Helper function that wraps async streaming response generator to simultaneously broadcast to sink.

            Exception handling for context window must be handled explicitly within the generator.
            """

            sink = current_sink.get()
            # run_id is is refreshed everytime agent.run/arun is called
            # we track this with every message so that multiple run calls in a single run_x_agent module can be differentiated during logging
            metadata = {
                    'session_id': current_session_id.get(),
                    'node_id': getattr(current_node.get(), 'node_id', None),
                    'run_id': self.run_id, 
                }

            try:
                # At this point, Agno has already appended the default system message (with context, instruction, etc) and initial user prompt to run_messages
                # For completeness, we push these to the sink to be logged downstream if necessary
                for message in self.run_messages.messages:
                    await _push_chunk_to_sink(message, sink, metadata)
                        
                async with aclosing(response_stream) as stream:
                    async for chunk in stream:
                        try:
                            await _push_chunk_to_sink(chunk, sink, metadata)
                        except Exception as e:
                            logger.error(f"Error pushing chunk to sink; chunk {chunk}, error: {e}, metadata:{metadata}")
                            raise
                        
                        yield chunk
            except asyncio.CancelledError:
                raise
            except:
                self.log_token_usage(
                    self.run_messages
                )
                logger.warning(
                    f"Received an unresolved error"
                )
                raise
        
        if include_global_state and not self.run_without_session:
            current_global_state_str = self._get_current_global_state_str()

            modified_prompt = (
                f"{prompt}\n\n The current global state represents actions already taken by other agents and functions in this workflow. "
                f"The current global state is: {current_global_state_str}. "
                f"Be sure to use the asynchronous versions of tools"
                f"If a tool output has been summarized, use the tool_retrieval tool to obtain chunks of the raw output from the tool"
            )

        else:
            modified_prompt = prompt

        self.model.summarize_on_fail = summarize_on_fail
        
        try:
            response = await super().arun(modified_prompt, stream=stream, **kwargs)
            if stream:
                return broadcaster(
                    response,
                    # Include these explicitly for logging purposes for @stream_status 
                    prompt=prompt,
                    include_global_state=include_global_state,
                    summarize_on_fail=summarize_on_fail,
                    stream=stream, 
                    **kwargs
                    )
            else:
                self.log_token_usage(response)
                return response
        except asyncio.CancelledError:
            logger.info("arun cancelled")
            raise
        except:
            self.log_token_usage(
                self.run_messages
            )
            logger.warning("Received an unresolved error")
            raise

    async def safe_call_acontinue_run(self, **kwargs):
        """Runs Agent.acontinue_run() without throwing erors for unknown kwargs."""
        
        # Get the signature of the method
        # Keep only the kwargs that are in the function's signature
        sig = inspect.signature(self.acontinue_run)
        valid_kwargs = {
            k: v for k, v in kwargs.items()
            if k in sig.parameters
        }

        return await self.acontinue_run(
            **valid_kwargs
        )