import asyncio
from enum import Enum
import os
import tiktoken
from copy import deepcopy
from textwrap import dedent
from typing import Any, Dict, List, Optional, Type, Union, Callable
from pydantic import BaseModel, Field
from agno.models.base import Model
from agno.models.message import Message
from agno.utils.log import log_debug, log_error, log_info, log_warning
from agno.utils.prompts import get_json_output_prompt
from tqdm.asyncio import tqdm
from agno.memory.v2.summarizer import SessionSummarizer, SessionSummaryResponse
from textwrap import dedent
from typing import Optional, List
from agno.run.messages import RunMessages
from custom_models.litellm_with_retry import LiteLLMOpenAIWithRetry
from litellm import BadRequestError, APIError
from custom_agents.summary_prompts import SHELL_PROMPTS, WEB_PROMPTS
from tools.guardrails.common import PAYLOAD_TYPE

# Arbitrary reasonable limit on tool output
TRUNCATION_CHAR_LENGTH = 10000
MAX_SUMMARY_ATTEMPT = 5
MAX_TOKENS = int(os.getenv("MAX_TOKENS", 50000))
MAX_CHUNK_SIZE = int(os.getenv("MAX_CHUNK_SIZE", 4000))

SUMMARY_MODEL = LiteLLMOpenAIWithRetry(
    id=os.getenv("SUMMARY_MODEL_ID", "gemini-2.5-pro"),
    base_url="https://litellm-stg.aip.gov.sg/v1",
    api_key=os.getenv("LITELLM_API_KEY"),
    temperature=0,
)

# Helper functions

# Mapping from model names to tiktoken encodings
#TODO: Fix how this encoding is performed
MODEL_ENCODING = {
    "apac.anthropic.claude-3-7-sonnet-20250219-v1:0": "cl100k_base",
    "apac.anthropic.claude-sonnet-4-20250514-v1:0": "cl100k_base",
    "gpt-5-eastus2": "cl100k_base",
    "gpt-5-chat-eastus2": "cl100k_base",
    "gemini-2.5-pro": "cl100k_base",
}

# --- Utility Functions ---

def get_encoding_for_model(model_name: str):
    """
    Returns the tiktoken encoding for a given model.
    
    Args:
        model_name (str): The identifier of the LLM model.

    Returns:
        tiktoken.Encoding: Encoding object for tokenization.
    """

    #TODO: Encoding technique needs to be refined at some point
    model_name = model_name.lower()
    for key, enc_name in MODEL_ENCODING.items():
        if key in model_name:
            return tiktoken.get_encoding(enc_name)
    # fallback
    return tiktoken.get_encoding("cl100k_base")

def normalize_message_content(msg: Union[Message, str, List, Dict, None]) -> str:
    """
    Normalize an Agno Message (or its content) into a plain string.

    Handles:
    - Message.content as str
    - Message.content as list of parts (e.g. [{"type": "text", "text": "..."}, {"type": "image_url", ...}])
    - Message.content as dict
    - None
    """
    if isinstance(msg, Message):
        content = msg.content
    else:
        content = msg

    if content is None:
        return ""

    if isinstance(content, str):
        return content

    if isinstance(content, list):
        # Concatenate parts into a single string
        parts = []
        for c in content:
            if isinstance(c, dict):
                if c.get("type") == "text":
                    parts.append(c.get("text", ""))
                elif c.get("type") == "image_url":
                    parts.append(f"[image: {c.get('image_url', {}).get('url', '')}]")
                else:
                    parts.append(str(c))
            else:
                parts.append(str(c))
        return " ".join(p for p in parts if p)

    if isinstance(content, dict):
        if "text" in content:
            return content["text"]
        return str(content)

    return str(content)

def truncate_messages(run_messages: RunMessages):
    """Truncates output of all tool calls except for `think` tool."""

    messages = deepcopy(run_messages)
    for message in messages:
        # Idea is that even if we truncate tool outputs, we try to retain as much logic as possible by preserving model reasoning outputs
        if message.role == "tool" and "think" not in message.tool_name:
            message.content = message.content[:TRUNCATION_CHAR_LENGTH]
            log_debug(
                f"Tool call {message.tool_call_id} [id: {message.tool_name}] output is longer than {TRUNCATION_CHAR_LENGTH}; truncating..."
            )
    return messages

def maybe_remove_tools_without_output(run_messages: RunMessages):
    """
    Remove assistant tool calls that do not have a corresponding tool output.

    A tool call is considered to have output if there exists a `role='tool'`
    message whose `tool_call_id` matches the tool call's `id` (or `tool_call_id`
    fallback) from the assistant message.
    """
    messages_copy = deepcopy(run_messages)

    # Collect all tool_call_ids that actually produced tool outputs
    tool_output_ids = set()
    for msg in messages_copy:
        if getattr(msg, "role", None) == "tool":
            tcid = getattr(msg, "tool_call_id", None)
            if tcid:
                tool_output_ids.add(tcid)

    # Walk assistant messages and prune orphan tool_calls
    for msg in messages_copy:
        if getattr(msg, "role", None) != "assistant":
            continue
        tool_calls = getattr(msg, "tool_calls", None)
        if not tool_calls:
            continue

        pruned_calls = []
        removed = 0
        for tc in tool_calls:
            # Support both dict-style and attribute-style tool_call objects
            if isinstance(tc, dict):
                tc_id = tc.get("id") or tc.get("tool_call_id")
                tool_name = (tc.get("function") or {}).get("name")
            else:
                tc_id = getattr(tc, "id", None) or getattr(tc, "tool_call_id", None)
                fn = getattr(tc, "function", None)
                tool_name = getattr(fn, "name", None) if fn else None

            if tc_id and tc_id in tool_output_ids:
                pruned_calls.append(tc)
            else:
                removed += 1
                try:
                    log_warning(
                        f"Removing tool call without output: id={tc_id}, tool={tool_name}"
                    )
                except Exception:
                    pass

        if removed:
            # Assign the filtered list back
            msg.tool_calls = pruned_calls

    return messages_copy

def extract_message_fields(message: Message) -> Optional[List[str]]:
    """Extract and format fields for each Message in conversation history as a string."""

    role_templates = {
        "tool": """
            <tool_output>
                <tool>{tool_name}</tool>
                <tool_id>{tool_call_id}</tool_id>
                <content>{content}</content>
            </tool_output>
        """,
        "user": """
            <user>
                <content>{content}</content>
            </user>
        """,
        "system": """
            <system>
                <content>{content}</content>
            </system>
        """,
        "assistant_content": """
            <assistant>
                <content>{content}</content>
            </assistant>
        """,
        "assistant_tool": """
            <assistant>
                <tool_call>
                    <tool>{tool}</tool>
                    <tool_id>{tool_id}</tool_id>
                    <arguments>{arguments}</arguments>
                </tool_call>
            </assistant>
        """,
    }

    if message.role == "tool":
        return dedent(role_templates["tool"]).format(
            tool_name=message.tool_name,
            tool_call_id=message.tool_call_id,
            content=message.content,
        )

    elif message.role in ("user", "system"):
        return dedent(role_templates[message.role]).format(content=message.content)

    elif message.role == "assistant":
        if getattr(message, "tool_calls", None):
            tool_calls = message.tool_calls
            for tool_call in tool_calls:
                return dedent(role_templates["assistant_tool"]).format(
                    tool=tool_call["function"]["name"],
                    tool_id=tool_call["id"],
                    arguments=tool_call["function"]["arguments"],
                )
        else:
            return dedent(role_templates["assistant_content"]).format(
                content=message.content
            )
    else:
        return ""

# Reduction functions
def reduce_raw(chunks: List[str]) -> str:
    """
    Reduce a list of text chunks into a concatenated string with tags.
    
    Args:
        chunks (List[str]): Text fragments.
    
    Returns:
        str: Concatenated chunked representation.
    """
    res = ""
    for i in range(len(chunks)):
        res += f"<chunk_{i}>\n\t{chunks[i]}\n<chunk_{i}>\n"
    
    return res

# --- Summarizer Types ---

class SUMMARIZER_TYPE(Enum):
    STANDARD = "standard"
    ROLLING = "rolling"
    MAP_REDUCE = "map_reduce"

# --- Tool Summarizer ---

class ToolSummarizer:
    """Custom class that manages summarization of tool outputs with multiple strategies."""

    def __init__(self, 
                 model: Optional[Any] = SUMMARY_MODEL,
                 summarizer_type: SUMMARIZER_TYPE = SUMMARIZER_TYPE.STANDARD,
                 payload_type: PAYLOAD_TYPE = PAYLOAD_TYPE.TEXT,
                 batch_size: int = 15):
        
        self.model = model
        self.summarizer_type = summarizer_type
        self.payload_type = payload_type
        self.batch_size = batch_size

        if payload_type == PAYLOAD_TYPE.TEXT:
            self.reducing_function = reduce_raw
            self.summary_prompt = SHELL_PROMPTS
            
        elif payload_type == PAYLOAD_TYPE.WEB_PAGE:
            self.reducing_function = reduce_raw
            self.summary_prompt = WEB_PROMPTS

        elif payload_type == PAYLOAD_TYPE.SHELL_OUTPUT:
            self.reducing_function = reduce_raw
            self.summary_prompt = SHELL_PROMPTS
        
        else:
            raise ValueError("Summarization parameters are not defined yet")

    async def make_llm_call(self, system_prompt: str, user_prompt: str, prompt_input: str, session_summary: str) -> str:
        """
        Construct prompts and make a call to the LLM with retry.
        
        Args:
            system_prompt (str): System-level instructions.
            user_prompt (str): User-facing prompt template.
            prompt_input (str): Input content to summarize.
            session_summary (str): Running session summary.
        
        Returns:
            str: LLM-generated summarization output.
        """

        session_summary = f"""
            \n\nThis is the summary of the session so far. You should use it to guide the summarization process and identify what
            are the important details that need to be kept.

            <session_summary>
            {session_summary}
            </session_summary>
        """

        messages = [
            Message(
                role="system",
                content=system_prompt + session_summary,
            ),
            Message(
                role="user",
                content=user_prompt.format(content=prompt_input),
            )
        ]

        response = await self.model.call_llm_with_retry_async(messages=messages)
        return response.choices[0].message.content

    # Define summarization functions
    async def arun(self, raw_output: str, session_summary: str) -> str:
        """
        Route summarization request based on configured summarizer_type.
        
        Args:
            raw_output (str): Raw text/tool output.
            session_summary (str): Ongoing session summary.
        
        Returns:
            str: Summarized output.
        """

        # Specific forms of summarization
        if self.summarizer_type == SUMMARIZER_TYPE.MAP_REDUCE:
            return await self.map_reduce(raw_output=raw_output, session_summary=session_summary)
        elif self.summarizer_type == SUMMARIZER_TYPE.ROLLING:
            return await self.rolling(raw_output=raw_output, session_summary=session_summary)
        else:
            return await self.divide_and_conquer(raw_output=raw_output, session_summary=session_summary)

    async def map_reduce(self, raw_output: str, session_summary: str) -> str:
        """Perform summarization using a MapReduce approach with retries."""
        try:
            summary = await self.make_llm_call(system_prompt=self.summary_prompt["system_chunk"], user_prompt=self.summary_prompt["user_chunk"], prompt_input=raw_output, session_summary=session_summary)
            return summary
        except (BadRequestError, APIError):
            curr_output = raw_output

            # Initialize summarization
            level = 0
            chunking_function = self.payload_type.get_chunking_function()
            
            while level < MAX_SUMMARY_ATTEMPT and len(curr_output) > MAX_TOKENS:
                if level == 0:
                    chunks = chunking_function(raw_output=curr_output)
                    curr_prompt = "chunk"
                else:
                    new_chunks = []
                    for i in range(0, len(chunks), self.batch_size):
                        new_chunks.append(self.reducing_function(chunks=chunks[i:min(len(chunks), i + self.batch_size)]))
                    chunks = new_chunks
                    curr_prompt = "intermediate"

                semaphore = asyncio.Semaphore(10)

                async def process_chunk_concurrent(chunk):
                    async with semaphore:
                        return await self.make_llm_call(system_prompt=self.summary_prompt["system_" + curr_prompt], user_prompt=self.summary_prompt["user_" + curr_prompt], prompt_input=chunk, session_summary=session_summary)

                tasks = [process_chunk_concurrent(chunk) for chunk in chunks]
                chunks = []
                for task in tqdm.as_completed(tasks, desc="Processing chunks", total=len(tasks)):
                    result = await task
                    chunks.append(result)
                
                # Perform reduction
                curr_output = self.reducing_function(chunks=chunks)

                level += 1

            if level == MAX_SUMMARY_ATTEMPT:
                raise RecursionError
            
            return curr_output
        except:
            raise


    async def rolling(self, raw_output: str, session_summary: str) -> str:
        """Perform summarization incrementally across chunks (Rolling)."""
        try:
            summary = await self.make_llm_call(system_prompt=self.summary_prompt["system_chunk"], user_prompt=self.summary_prompt["user_chunk"], prompt_input=raw_output, session_summary=session_summary)
            return summary
        except (BadRequestError, APIError):
            chunking_function = self.payload_type.get_chunking_function()

            chunks = chunking_function(raw_output=raw_output)
            curr_summary = "<current_summary>No chunk has been summarized yet.</current_summary>\n"
            current_chunk = 1

            for i in tqdm(range(len(chunks))):
                chunk = chunks[i]
                curr_summary += f"<chunk_{current_chunk}>\n\t{chunk}\n</chunk_{current_chunk}>"

                if len(curr_summary) > MAX_TOKENS or i == len(chunks) - 1:
                    current_chunk = 1
                    curr_summary = await self.make_llm_call(system_prompt=self.summary_prompt["system_rolling"], user_prompt=self.summary_prompt["user_rolling"], prompt_input=raw_output, session_summary=session_summary)
                    curr_summary = f"<current_summary>\n\t{curr_summary}\n</current_summary>\n"
            
            return curr_summary
        
        except:
            raise

    async def divide_and_conquer(self, raw_output: str, session_summary: str) -> str:
        """Perform summarization by recursively splitting data (Divide-and-Conquer)."""
        try:
            summary = await self.make_llm_call(system_prompt=self.summary_prompt["system_divide_conquer"], user_prompt=self.summary_prompt["user_divide_conquer"], prompt_input=raw_output, session_summary=session_summary)
            return summary
        except (BadRequestError, APIError):
            chunking_function = self.payload_type.get_chunking_function()

            chunks = chunking_function(raw_output=raw_output)
            left_half = reduce_raw(chunks=chunks[0:len(chunks)//2])
            right_half = reduce_raw(chunks=chunks[len(chunks)//2: len(chunks)])

            semaphore = asyncio.Semaphore(2)

            async def process_half_concurrent(chunk):
                async with semaphore:
                    return await self.divide_and_conquer(raw_output=chunk, session_summary=session_summary)
                
            tasks = [process_half_concurrent(chunk) for chunk in [left_half, right_half]]
            results = []
            for task in tasks:
                result = await task

                results.append(result)
            
            return reduce_raw(results)
        except:
            raise
        
# Session Summarizer Functions
class CustomSessionSummarizer(SessionSummarizer):
    """Subclass of SessionSummarizer, Agno's summarization class, that implements a custom system prompt to summarize the conversation so far, with a focus on tool outputs."""

    def get_system_message(
        self,
        conversation: List[Message],
        response_format: Union[Dict[str, Any], Type[BaseModel]],
    ) -> Message:
        """
        Construct the system message prompt for session summarization.
        
        Args:
            conversation (List[Message]): Full conversation history.
            response_format (Union[Dict[str, Any], Type[BaseModel]]): Expected output format.
        
        Returns:
            Message: System prompt as a message object.
        """

        if self.system_message is not None:
            return Message(role="system", content=self.system_message)

        system_prompt = dedent("""\
        Analyze the following conversation between a user and an assistant, and extract the following details:
          - Summary (str): Provide a concise summary of the session, focusing on important information that would be helpful for future interactions.
          - Topics (Optional[List[str]]): List the topics discussed in the session.
        Keep the summary concise and to the point. Only include relevant information.

        <conversation>
        """)
        system_prompt += "\n".join([extract_message_fields(m) for m in conversation])
        system_prompt += "</conversation>"

        if self.additional_instructions:
            system_prompt += "\n" + self.additional_instructions

        if response_format == {"type": "json_object"}:
            system_prompt += "\n" + get_json_output_prompt(SessionSummaryResponse)

        return Message(role="system", content=system_prompt)

class MapReduceSessionSummarizer(CustomSessionSummarizer):
    """Custom session summarizer with fallback to MapReduce summarization."""

    def __init__(self, chunking_function: Callable[[List[Message]], List[List[Message]]], *args, **kwargs):
        """
        Takes in a chunking function. This allows for modularity based on the content type so that relationships in the content are not arbitrarily split based simply on size
        """

        self.chunking_function = chunking_function

        super().__init__(*args, **kwargs)
    
    def get_system_message(self, conversation, response_format):
        default_message = super().get_system_message(conversation, response_format)

        # Add on the necessary addendum
        addendum = dedent("""
            \n\n Note that this is a MapReduce Session Summarizer. As such, each conversation
            you summarize is going to be either a chunk from the initial conversation or a 
            set of messages containing summaries. As such, when you summarize the conversation,
            you will attempt to. relate the different messages to each other.
        """)

        default_message.content = default_message + addendum
        return default_message

    async def arun(self, conversation: List[Message], recursion_level: int = 0):
        """Attempt summarization, fallback to chunked MapReduce on failure."""
        try:
            return await super().arun(conversation=conversation)
        
        except (BadRequestError, APIError) as e:
            # Recursion level is too deep
            if recursion_level == MAX_SUMMARY_ATTEMPT:
                raise
            
            # Chunk it appropriately using the given function
            chunks = self.chunking_function(conversation)

            # Perform concurrent map reduction
            partial_summaries = []
            semaphore = asyncio.Semaphore(5)

            async def process_chunk_concurrent(chunk, idx):
                async with semaphore:
                    return (await self.arun(conversation=chunk, recursion_level=recursion_level+1), idx)
                
            tasks = [process_chunk_concurrent(chunk, idx) for chunk, idx in enumerate(chunks)]
            for task in tqdm.as_completed(tasks, desc="Processing chunks", total=len(tasks)):
                result, idx = await task

                partial_summaries.append(
                    Message(
                        role="user",
                        content=result,
                        metadata={
                            "chunk_id": str(idx)
                        }
                    )
                )

            return await self.arun(conversation=partial_summaries, recursion_level=recursion_level)

class RollingSessionSummarizer(CustomSessionSummarizer):
    """Custom session summarizer with fallback to Rolling summarization."""

    def get_system_message(self, conversation, response_format):
        default_message = super().get_system_message(conversation, response_format)

        # Add on the necessary addendum
        addendum = dedent("""
            \n\n Note that this is a Rolling Session Summarizer. As such, the first message in the conversation
            will either be a message in the initial conversation or the summary that has been built so far. As such,
            you will use the other messages to continuously refine this summary, add relevant information, and build a
            summary of the full conversation.
        """)

        default_message.content = default_message.content + addendum
        return default_message
    
    async def arun(self, conversation: List[Message], recursion_level: int = 0):
        """Attempt summarization, fallback to rolling chunk summarization on failure."""
        try:
            return await super().arun(conversation=conversation)
        
        except (BadRequestError, APIError) as e:
            # Recursion level. is too deep
            if recursion_level == MAX_SUMMARY_ATTEMPT:
                raise

            curr_msg = []
            curr_window = 0
            enc = get_encoding_for_model(SUMMARY_MODEL.id)

            for msg in tqdm.as_completed(conversation, desc="Processing chunks", total=len(conversation)):
                tokens = enc.encode(normalize_message_content(msg.content))
                curr_msg.append(msg)
                curr_window += len(tokens)
                
                if curr_window > MAX_TOKENS:
                    summary = await self.arun(conversation=curr_msg, recursion_level=recursion_level+1)
                    curr_msg = [
                        Message(
                            role="user",
                            content=summary.summary
                        )
                    ]

                    curr_window = len(enc.encode(normalize_message_content(summary.summary)))
            
            return await self.arun(conversation=curr_msg, recursion_level=recursion_level+1)

def get_session_summarizer(summarizer: SUMMARIZER_TYPE = SUMMARIZER_TYPE.STANDARD, chunking_function: Optional[Callable] = None, *args, **kwargs):
    """
    Factory function to get a session summarizer by type.
    
    Args:
        summarizer (SUMMARIZER_TYPE): Type of summarizer.
        chunking_function (Optional[Callable[[List[Message]], List[List[Message]]]): Optional chunking function for MapReduce.
    
    Returns:
        SessionSummarizer: Configured summarizer instance.
    """
    if summarizer == SUMMARIZER_TYPE.STANDARD:
        return CustomSessionSummarizer(
            model=SUMMARY_MODEL,
            *args,
            **kwargs
        )
    
    elif summarizer == SUMMARIZER_TYPE.ROLLING:
        return RollingSessionSummarizer(
            model=SUMMARY_MODEL,
            *args,
            **kwargs
        )
    elif summarizer == SUMMARIZER_TYPE.MAP_REDUCE:
        return MapReduceSessionSummarizer(
            model=SUMMARY_MODEL,
            chunking_function=chunking_function,
            *args, **kwargs
        )
    else:
        raise ValueError("Current summarizer request is not supported")


