import asyncio
import json
import os
import re
from typing import List, Dict, Any
from dataclasses import dataclass, asdict
from tqdm.asyncio import tqdm
import tiktoken
from agno.utils.log import logger


from openai import AsyncOpenAI, RateLimitError
from dotenv import load_dotenv
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Import prompts from separate file
from custom_agents.recon.web_page_analysis_prompts import (
    CHUNK_ANALYSIS_SYSTEM_PROMPT,
    CHUNK_ANALYSIS_USER_PROMPT,
    FINAL_SUMMARY_SYSTEM_PROMPT,
    FINAL_SUMMARY_USER_PROMPT,
    INTERMEDIATE_SUMMARY_SYSTEM_PROMPT,
    INTERMEDIATE_SUMMARY_USER_PROMPT
)

load_dotenv()
enc = tiktoken.get_encoding("cl100k_base")


@dataclass
class WebPageElement:
    """Represents a web page element that could be vulnerable."""
    element_type: str  # form, link, parameter, etc.
    location: str  # URL or xpath
    method: str  # GET, POST, etc.
    parameters: List[str]
    parameter_sets: List[str]
    vulnerability_indicators: Dict[str, List[str]]  # vuln_type -> indicators


@dataclass
class ChunkAnalysis:
    """Analysis result for a single chunk."""
    chunk_id: str
    forms: List[Dict[str, Any]]
    parameters: List[Dict[str, Any]]
    endpoints: List[Dict[str, Any]]
    vulnerability_surfaces: Dict[str, List[Dict[str, Any]]]  # vuln_type -> surfaces
    content_indicators: Dict[str, Any]


@dataclass
class IntermediateSummary:
    """Intermediate summary for a batch of chunks."""
    batch_id: str
    chunk_count: int
    top_forms: List[Dict[str, Any]]
    top_parameters: List[Dict[str, Any]]
    top_endpoints: List[Dict[str, Any]]
    high_confidence_vulnerabilities: Dict[str, List[Dict[str, Any]]]
    batch_insights: List[str]


@dataclass
class WebPageAnalysis:
    """Final analysis result for the entire web page."""
    total_chunks: int
    forms_found: List[Dict[str, Any]]
    parameters_found: List[Dict[str, Any]]
    endpoints_found: List[Dict[str, Any]]
    vulnerability_surfaces: Dict[str, List[Dict[str, Any]]]
    recon_summary: Dict[str, Any]
    key_findings: List[str]
    processing_method: str = "direct"  # "direct", "two_level", "hierarchical"


class WebPageAnalyzer:
    def __init__(self, chunk_size: int = 8000, overlap: int = 500, model: str = "azure/gpt-4o"):
        self.client = AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE"),
        )
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.model = model
        
        # Context length thresholds
        self.direct_threshold = 15000  # tokens
        self.two_level_threshold = 50000  # tokens
        self.batch_size = 15  # chunks per batch for hierarchical processing
    
    # use tiktoken to estimate token count
    def estimate_token_count(self, data: Any) -> int:
        """Estimate token count for data structure (rough approximation)."""
        if isinstance(data, str):
            return len(enc.encode(data))
        elif isinstance(data, dict):
            return len(enc.encode(json.dumps(data)))
        elif isinstance(data, list):
            return len(enc.encode(json.dumps(data)))
        else:
            return len(enc.encode(str(data)))

    def estimate_final_prompt_size(self, chunk_analyses: List[ChunkAnalysis]) -> int:
        """Estimate the token count for the final summary prompt."""
        # Combine all data to estimate size
        all_forms = []
        all_parameters = []
        all_endpoints = []
        combined_vuln_surfaces = {'sqli': [], 'xss': [], 'xxe': [], 'open_redirect': []}
        
        for analysis in chunk_analyses:
            all_forms.extend(analysis.forms)
            all_parameters.extend(analysis.parameters)
            all_endpoints.extend(analysis.endpoints)
            
            for vuln_type, surfaces in analysis.vulnerability_surfaces.items():
                combined_vuln_surfaces[vuln_type].extend(surfaces)
        
        # Estimate tokens for each component
        system_prompt_tokens = self.estimate_token_count(FINAL_SUMMARY_SYSTEM_PROMPT)
        summary_data_tokens = self.estimate_token_count({
            'total_chunks': len(chunk_analyses),
            'forms_count': len(all_forms),
            'parameters_count': len(all_parameters),
            'endpoints_count': len(all_endpoints),
            'vulnerability_surfaces': combined_vuln_surfaces
        })
        forms_sample_tokens = self.estimate_token_count(all_forms[:5])
        parameters_sample_tokens = self.estimate_token_count(all_parameters[:10])
        vuln_surfaces_sample_tokens = self.estimate_token_count({
            k: v[:3] for k, v in combined_vuln_surfaces.items()
        })
        
        total_estimated = (system_prompt_tokens + summary_data_tokens + 
                          forms_sample_tokens + parameters_sample_tokens + 
                          vuln_surfaces_sample_tokens + 500)  # buffer
        
        return total_estimated
        
    def chunk_text_with_overlap(self, text: str) -> List[str]:
        """Split text into overlapping chunks, trying to preserve HTML structure."""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end]
            
            # If this isn't the last chunk, try to break at HTML boundaries
            if end < len(text):
                # Look for closing tags in the latter half of the chunk
                tag_patterns = [r'</\w+>', r'</form>', r'</div>', r'</section>', r'</body>']
                best_break = -1
                
                for pattern in tag_patterns:
                    matches = list(re.finditer(pattern, chunk[self.chunk_size // 2:]))
                    if matches:
                        last_match = matches[-1]
                        potential_break = self.chunk_size // 2 + last_match.end()
                        if potential_break > best_break:
                            best_break = potential_break
                
                if best_break > 0:
                    chunk = chunk[:best_break]
                    end = start + best_break
            
            chunks.append(chunk)
            
            # Move start position with overlap
            if end >= len(text):
                break
            start = end - self.overlap
            
        return chunks

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=2, min=4, max=60),
        retry=retry_if_exception_type(RateLimitError)
    )
    async def llm_call_async(self, prompt: str, system_prompt: str = None) -> str:
        """Make async LLM call with retry logic for rate limits only."""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            response = await self.client.chat.completions.create(
                messages=messages,
                model=self.model,
                temperature=0.1,
                timeout=120.0,
            )
            return response.choices[0].message.content
        except RateLimitError as e:
            logger.error(f"Rate limit hit, retrying: {e}")
            raise  # This will trigger the retry
        except Exception as e:
            logger.error(f"Non-retryable error in LLM call: {e}")
            raise  # This will not trigger retry, will fail immediately

    async def analyze_chunk(self, chunk: str, chunk_id: str) -> ChunkAnalysis:
        """Analyze a single chunk of web page content for recon purposes."""
        
        user_prompt = CHUNK_ANALYSIS_USER_PROMPT.format(chunk=chunk)

        try:
            response = await self.llm_call_async(user_prompt, CHUNK_ANALYSIS_SYSTEM_PROMPT)
            
            # Clean up the response to extract JSON
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)
                
                return ChunkAnalysis(
                    chunk_id=chunk_id,
                    forms=data.get('forms', []),
                    parameters=data.get('parameters', []),
                    endpoints=data.get('endpoints', []),
                    vulnerability_surfaces=data.get('vulnerability_surfaces', {
                        'sqli': [],
                        'xss': [],
                        'xxe': [],
                        'open_redirect': []
                    }),
                    content_indicators=data.get('content_indicators', {})
                )
            else:
                # Fallback: create empty analysis
                return ChunkAnalysis(
                    chunk_id=chunk_id,
                    forms=[],
                    parameters=[],
                    endpoints=[],
                    vulnerability_surfaces={'sqli': [], 'xss': [], 'xxe': [], 'open_redirect': []},
                    content_indicators={}
                )

        except Exception as e:
            logger.error(f"Error analyzing chunk {chunk_id}: {e}")
            return ChunkAnalysis(
                chunk_id=chunk_id,
                forms=[],
                parameters=[],
                endpoints=[],
                vulnerability_surfaces={'sqli': [], 'xss': [], 'xxe': [], 'open_redirect': []},
                content_indicators={}
            )

    async def generate_intermediate_summary(self, chunk_analyses: List[ChunkAnalysis], batch_id: str) -> IntermediateSummary:
        """Generate intermediate summary for a batch of chunk analyses."""
        
        user_prompt = INTERMEDIATE_SUMMARY_USER_PROMPT.format(
            batch_id=batch_id,
            chunk_count=len(chunk_analyses),
            chunk_analyses=json.dumps([asdict(analysis) for analysis in chunk_analyses], indent=2)
        )

        try:
            response = await self.llm_call_async(user_prompt, INTERMEDIATE_SUMMARY_SYSTEM_PROMPT)
            
            # Clean up the response to extract JSON
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)
                
                return IntermediateSummary(
                    batch_id=batch_id,
                    chunk_count=len(chunk_analyses),
                    top_forms=data.get('top_forms', []),
                    top_parameters=data.get('top_parameters', []),
                    top_endpoints=data.get('top_endpoints', []),
                    high_confidence_vulnerabilities=data.get('high_confidence_vulnerabilities', {
                        'sqli': [], 'xss': [], 'xxe': [], 'open_redirect': []
                    }),
                    batch_insights=data.get('batch_insights', [])
                )
            else:
                # Fallback: create basic summary from chunk data
                all_forms = []
                all_parameters = []
                all_endpoints = []
                
                for analysis in chunk_analyses:
                    all_forms.extend(analysis.forms)
                    all_parameters.extend(analysis.parameters)
                    all_endpoints.extend(analysis.endpoints)
                
                return IntermediateSummary(
                    batch_id=batch_id,
                    chunk_count=len(chunk_analyses),
                    top_forms=all_forms[:5],  # Top 5 forms
                    top_parameters=all_parameters[:10],  # Top 10 parameters
                    top_endpoints=all_endpoints[:5],  # Top 5 endpoints
                    high_confidence_vulnerabilities={'sqli': [], 'xss': [], 'xxe': [], 'open_redirect': []},
                    batch_insights=[f"Processed {len(chunk_analyses)} chunks in batch {batch_id}"]
                )

        except Exception as e:
            logger.error(f"Error generating intermediate summary for batch {batch_id}: {e}")
            # Return minimal summary
            return IntermediateSummary(
                batch_id=batch_id,
                chunk_count=len(chunk_analyses),
                top_forms=[],
                top_parameters=[],
                top_endpoints=[],
                high_confidence_vulnerabilities={'sqli': [], 'xss': [], 'xxe': [], 'open_redirect': []},
                batch_insights=[f"Error processing batch {batch_id}"]
            )

    async def generate_final_summary_from_intermediates(self, intermediate_summaries: List[IntermediateSummary]) -> WebPageAnalysis:
        """Generate final summary from intermediate summaries instead of raw chunk analyses."""
        
        # Combine all intermediate data
        all_forms = []
        all_parameters = []
        all_endpoints = []
        combined_vuln_surfaces = {'sqli': [], 'xss': [], 'xxe': [], 'open_redirect': []}
        all_insights = []
        total_chunks = 0
        
        for summary in intermediate_summaries:
            all_forms.extend(summary.top_forms)
            all_parameters.extend(summary.top_parameters)
            all_endpoints.extend(summary.top_endpoints)
            all_insights.extend(summary.batch_insights)
            total_chunks += summary.chunk_count
            
            for vuln_type, surfaces in summary.high_confidence_vulnerabilities.items():
                combined_vuln_surfaces[vuln_type].extend(surfaces)
        
        # Deduplicate
        unique_forms = self._deduplicate_items(all_forms, ['action', 'method'])
        unique_parameters = self._deduplicate_items(all_parameters, ['name', 'location'])
        unique_endpoints = self._deduplicate_items(all_endpoints, ['url', 'method'])
        
        for vuln_type in combined_vuln_surfaces:
            combined_vuln_surfaces[vuln_type] = self._deduplicate_items(
                combined_vuln_surfaces[vuln_type], 
                ['parameter', 'location', 'reason']
            )
        
        # Generate final summary using LLM
        summary_data = {
            'total_chunks': total_chunks,
            'intermediate_batches': len(intermediate_summaries),
            'forms_count': len(unique_forms),
            'parameters_count': len(unique_parameters),
            'endpoints_count': len(unique_endpoints),
            'vulnerability_surfaces': combined_vuln_surfaces,
            'batch_insights': all_insights
        }
        
        summary_prompt = FINAL_SUMMARY_USER_PROMPT.format(
            summary_data=json.dumps(summary_data, indent=2),
            forms_sample=json.dumps(unique_forms[:5], indent=2),
            parameters_sample=json.dumps(unique_parameters[:10], indent=2),
            vuln_surfaces_sample=json.dumps({
                k: v[:3] for k, v in combined_vuln_surfaces.items()
            }, indent=2)
        )

        try:
            summary_response = await self.llm_call_async(summary_prompt, FINAL_SUMMARY_SYSTEM_PROMPT)
            json_start = summary_response.find('{')
            json_end = summary_response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                summary_json = json.loads(summary_response[json_start:json_end])
                key_findings = summary_json.get('key_findings', ["Analysis completed with extracted data"])
                recon_summary = summary_json.get('recon_summary', {})
            else:
                key_findings = ["Analysis completed with extracted data"]
                recon_summary = {}
        except Exception as e:
            logger.error(f"Error generating final summary: {e}")
            key_findings = ["Analysis completed with extracted data"]
            recon_summary = {}

        return WebPageAnalysis(
            total_chunks=total_chunks,
            forms_found=unique_forms,
            parameters_found=unique_parameters,
            endpoints_found=unique_endpoints,
            vulnerability_surfaces=combined_vuln_surfaces,
            recon_summary=recon_summary,
            key_findings=key_findings,
            processing_method="hierarchical"
        )

    async def adaptive_reduce_strategy(self, chunk_analyses: List[ChunkAnalysis]) -> WebPageAnalysis:
        """Choose the appropriate reduce strategy based on the estimated context length."""
        
        estimated_tokens = self.estimate_final_prompt_size(chunk_analyses)
        logger.info(f"Estimated final prompt size: {estimated_tokens:,} tokens")
        
        if estimated_tokens <= self.direct_threshold:
            logger.info("Using direct processing (small dataset)")
            analysis = await self.generate_final_summary(chunk_analyses)
            analysis.processing_method = "direct"
            return analysis
            
        elif estimated_tokens <= self.two_level_threshold:
            logger.info("Using two-level hierarchical processing (medium dataset)")
            return await self.two_level_reduce(chunk_analyses)
            
        else:
            logger.info("Using multi-level hierarchical processing (large dataset)")
            return await self.hierarchical_reduce(chunk_analyses)

    async def two_level_reduce(self, chunk_analyses: List[ChunkAnalysis]) -> WebPageAnalysis:
        """Two-level reduce: chunks -> intermediate summaries -> final summary."""
        
        # Split chunks into batches
        batches = [chunk_analyses[i:i + self.batch_size] 
                  for i in range(0, len(chunk_analyses), self.batch_size)]

        logger.info(f"Processing {len(batches)} batches of ~{self.batch_size} chunks each")

        # Generate intermediate summaries
        intermediate_summaries = []
        for i, batch in enumerate(batches):
            batch_id = f"batch_{i:03d}"
            logger.info(f"Generating intermediate summary for {batch_id} ({len(batch)} chunks)")
            intermediate = await self.generate_intermediate_summary(batch, batch_id)
            intermediate_summaries.append(intermediate)
        
        # Generate final summary from intermediates
        logger.info("Generating final summary from intermediate summaries")
        analysis = await self.generate_final_summary_from_intermediates(intermediate_summaries)
        analysis.processing_method = "two_level"
        return analysis

    async def hierarchical_reduce(self, chunk_analyses: List[ChunkAnalysis]) -> WebPageAnalysis:
        """Multi-level hierarchical reduce for very large datasets."""
        
        # First level: chunks -> intermediate summaries
        batches = [chunk_analyses[i:i + self.batch_size] 
                  for i in range(0, len(chunk_analyses), self.batch_size)]

        logger.info(f"Level 1: Processing {len(batches)} batches of ~{self.batch_size} chunks each")

        intermediate_summaries = []
        for i, batch in enumerate(batches):
            batch_id = f"L1_batch_{i:03d}"
            intermediate = await self.generate_intermediate_summary(batch, batch_id)
            intermediate_summaries.append(intermediate)
        
        # If we still have too many intermediate summaries, create another level
        if len(intermediate_summaries) > self.batch_size:
            logger.info(f"Level 2: Too many intermediate summaries ({len(intermediate_summaries)}), creating second level")
            
            # Group intermediate summaries into super-batches
            super_batches = [intermediate_summaries[i:i + self.batch_size] 
                           for i in range(0, len(intermediate_summaries), self.batch_size)]
            
            final_intermediates = []
            for i, super_batch in enumerate(super_batches):
                # Convert intermediate summaries back to a format we can process
                # This is a simplified approach - in practice, you might want a more sophisticated method
                combined_batch_data = []
                for intermediate in super_batch:
                    # Create a pseudo-chunk analysis from intermediate data
                    pseudo_analysis = ChunkAnalysis(
                        chunk_id=intermediate.batch_id,
                        forms=intermediate.top_forms,
                        parameters=intermediate.top_parameters,
                        endpoints=intermediate.top_endpoints,
                        vulnerability_surfaces=intermediate.high_confidence_vulnerabilities,
                        content_indicators={"batch_insights": intermediate.batch_insights}
                    )
                    combined_batch_data.append(pseudo_analysis)
                
                super_batch_id = f"L2_super_batch_{i:03d}"
                super_intermediate = await self.generate_intermediate_summary(combined_batch_data, super_batch_id)
                final_intermediates.append(super_intermediate)
            
            intermediate_summaries = final_intermediates
        
        # Final level: generate summary from final intermediate summaries
        logger.info("Final level: Generating summary from intermediate summaries")
        analysis = await self.generate_final_summary_from_intermediates(intermediate_summaries)
        analysis.processing_method = "hierarchical"
        return analysis

    async def generate_final_summary(self, chunk_analyses: List[ChunkAnalysis]) -> WebPageAnalysis:
        """Generate final summary from all chunk analyses."""
        
        # Combine all data
        all_forms = []
        all_parameters = []
        all_endpoints = []
        combined_vuln_surfaces = {'sqli': [], 'xss': [], 'xxe': [], 'open_redirect': []}
        
        for analysis in chunk_analyses:
            all_forms.extend(analysis.forms)
            all_parameters.extend(analysis.parameters)
            all_endpoints.extend(analysis.endpoints)
            
            for vuln_type, surfaces in analysis.vulnerability_surfaces.items():
                combined_vuln_surfaces[vuln_type].extend(surfaces)
                
        # Deduplicate forms, parameters, endpoints based on key attributes
        unique_forms = self._deduplicate_items(all_forms, ['action', 'method'])
        unique_parameters = self._deduplicate_items(all_parameters, ['name', 'location'])
        unique_endpoints = self._deduplicate_items(all_endpoints, ['url', 'method'])
        
        # Deduplicate vulnerability surfaces
        for vuln_type in combined_vuln_surfaces:
            combined_vuln_surfaces[vuln_type] = self._deduplicate_items(
                combined_vuln_surfaces[vuln_type], 
                ['parameter', 'location', 'reason']
            )
        
        # Generate recon summary using LLM
        summary_data = {
            'total_chunks': len(chunk_analyses),
            'forms_count': len(unique_forms),
            'parameters_count': len(unique_parameters),
            'endpoints_count': len(unique_endpoints),
            'vulnerability_surfaces': combined_vuln_surfaces
        }
        
        summary_prompt = FINAL_SUMMARY_USER_PROMPT.format(
            summary_data=json.dumps(summary_data, indent=2),
            forms_sample=json.dumps(unique_forms[:5], indent=2),
            parameters_sample=json.dumps(unique_parameters[:10], indent=2),
            vuln_surfaces_sample=json.dumps({
                k: v[:3] for k, v in combined_vuln_surfaces.items()
            }, indent=2)
        )

        try:
            summary_response = await self.llm_call_async(summary_prompt, FINAL_SUMMARY_SYSTEM_PROMPT)
            json_start = summary_response.find('{')
            json_end = summary_response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                summary_json = json.loads(summary_response[json_start:json_end])
                key_findings = summary_json.get('key_findings', ["Analysis completed with extracted data"])
                recon_summary = summary_json.get('recon_summary', {})
            else:
                key_findings = ["Analysis completed with extracted data"]
                recon_summary = {}
        except Exception as e:
            logger.error(f"Error generating summary: {e}")
            key_findings = ["Analysis completed with extracted data"]
            recon_summary = {}

        return WebPageAnalysis(
            total_chunks=len(chunk_analyses),
            forms_found=unique_forms,
            parameters_found=unique_parameters,
            endpoints_found=unique_endpoints,
            vulnerability_surfaces=combined_vuln_surfaces,
            recon_summary=recon_summary,
            key_findings=key_findings
        )

    def _deduplicate_items(self, items: List[Dict], key_fields: List[str]) -> List[Dict]:
        """Deduplicate items based on key fields."""
        seen = set()
        unique_items = []
        
        for item in items:
            # Create a key from the specified fields
            key_values = tuple(str(item.get(field, '')) for field in key_fields)
            if key_values not in seen:
                seen.add(key_values)
                unique_items.append(item)
        
        return unique_items

    async def process_content(self, content: str) -> WebPageAnalysis:        
        # Chunk the content
        chunks = self.chunk_text_with_overlap(content)
        logger.info(f"Split into {len(chunks)} chunks")
        
        # Process chunks concurrently with progress tracking
        semaphore = asyncio.Semaphore(5)  # Limit concurrent requests
        
        async def process_chunk_with_semaphore(chunk, idx):
            async with semaphore:
                result = await self.analyze_chunk(chunk, f"chunk_{idx:03d}")
                return result
        
        # Create tasks for all chunks
        tasks = [process_chunk_with_semaphore(chunk, i) for i, chunk in enumerate(chunks)]
        
        # Process with progress bar
        logger.info("Analyzing chunks...")
        chunk_analyses = []
        for task in tqdm.as_completed(tasks, desc="Processing chunks", total=len(tasks)):
            result = await task
            chunk_analyses.append(result)
        
        logger.info(f"Analyzed {len(chunk_analyses)} chunks")
        
        # Use adaptive reduce strategy to handle context length intelligently
        logger.info("Generating final summary using adaptive strategy...")
        analysis = await self.adaptive_reduce_strategy(chunk_analyses)
        
        return analysis



