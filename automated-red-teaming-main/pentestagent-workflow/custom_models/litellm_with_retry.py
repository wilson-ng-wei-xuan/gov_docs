from collections.abc import AsyncIterator
from copy import deepcopy
from dataclasses import dataclass
from textwrap import dedent
from typing import Any, Dict, Iterator, List, Optional, Type, Union, Mapping
import asyncio
import os

from pydantic import BaseModel

from agno.exceptions import ModelProviderError
from agno.models.litellm.chat import LiteLLM
from agno.models.litellm import LiteLLMOpenAI
from agno.models.message import Message
from agno.utils.log import log_error, log_info, log_warning
from tenacity import (
    retry,
    wait_exponential,
    stop_after_attempt,
    retry_if_exception_type,
)

try:
    from openai import APIConnectionError, APIStatusError, RateLimitError
    from openai.types.chat.chat_completion import ChatCompletion
    from openai.types.chat.chat_completion_chunk import (
        ChatCompletionChunk,
    )
except (ImportError, ModuleNotFoundError):
    raise ImportError(
        "`openai` not installed. Please install using `pip install openai`"
    )

import litellm
import tiktoken
from litellm import BadRequestError, APIError

litellm.drop_params = True

MAX_ATTEMPTS = 5
def log_retry_attempt(retry_state):
    """Log retry attempts"""
    log_info(
        f"Retrying {retry_state.fn.__name__} attempt {retry_state.attempt_number} after {retry_state.outcome.exception()}"
    )


def log_retry_stop(retry_state):
    """Log when retries stop"""
    if retry_state.outcome.failed:
        log_info(
            f"Failed after {retry_state.attempt_number} attempts: {retry_state.outcome.exception()}"
        )
    else:
        log_info(f"Succeeded after {retry_state.attempt_number} attempts")

@dataclass
class LiteLLMWithRetry(LiteLLM):
    """A class for interacting with LiteLLM with retry capabilities.

    This class extends LiteLLM to handle retries for API requests.
    Unlike LiteLLMOpenAIWithRetry, this supports tool calling together with stream=True.
    """

    def __init__(self, summarizer: Optional[Any] = None, *args, **kwargs):
        # Takes in any summarizer if there should be context window issues
        self.summarizer = summarizer
        self.summarize_on_fail = True
        super().__init__(*args, **kwargs)

    @retry(
        stop=stop_after_attempt(MAX_ATTEMPTS),
        wait=wait_exponential(multiplier=10, min=10, max=600),
        retry=retry_if_exception_type(RateLimitError),
        before_sleep=log_retry_attempt,
        after=log_retry_stop,
    )
    def invoke(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> Mapping[str, Any]:
        """Sends a chat completion request to the LiteLLM API."""
        completion_kwargs = self.get_request_params(tools=tools)
        completion_kwargs = self.get_request_params(tools=tools)
        completion_kwargs["messages"] = self._format_messages(messages)
        return self.get_client().completion(
            additional_drop_params=["tool_choice", "temperature"], **completion_kwargs
        )

    @retry(
        stop=stop_after_attempt(MAX_ATTEMPTS),
        wait=wait_exponential(multiplier=10, min=10, max=600),
        retry=retry_if_exception_type(RateLimitError),
        before_sleep=log_retry_attempt,
        after=log_retry_stop,
    )
    def invoke_stream(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> Iterator[Mapping[str, Any]]:
        """Sends a streaming chat completion request to the LiteLLM API."""
        completion_kwargs = self.get_request_params(tools=tools)
        completion_kwargs["messages"] = self._format_messages(messages)
        completion_kwargs["stream"] = True
        return self.get_client().completion(
            additional_drop_params=["tool_choice", "temperature"], **completion_kwargs
        )

    @retry(
        stop=stop_after_attempt(MAX_ATTEMPTS),
        wait=wait_exponential(multiplier=10, min=10, max=600),
        retry=retry_if_exception_type(RateLimitError),
        before_sleep=log_retry_attempt,
        after=log_retry_stop,
    )
    async def ainvoke(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> Mapping[str, Any]:
        """Sends an asynchronous chat completion request to the LiteLLM API."""
        completion_kwargs = self.get_request_params(tools=tools)
        completion_kwargs["messages"] = self._format_messages(messages)
        return await self.get_client().acompletion(
            drop_params=True,
            additional_drop_params=["tool_choice", "temperature"],
            **completion_kwargs,
        )

    async def ainvoke_stream(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> AsyncIterator[Any]:
        """Sends an asynchronous streaming chat request to the LiteLLM API."""
        completion_kwargs = self.get_request_params(tools=tools)
        completion_kwargs["messages"] = self._format_messages(messages)
        completion_kwargs["stream"] = True

        # Tenacity decorator does not work with streaming because it does not catch errors raised during generator iteration, only during generation creation
        # Manually implementing the retry mechanism here
        for attempt in range(MAX_ATTEMPTS):
            completion_kwargs["messages"] = self._format_messages(messages)

            try:
                async_stream = await self.get_client().acompletion(
                    drop_params=True,
                    additional_drop_params=["tool_choice", "temperature"],
                    **completion_kwargs,
                )

                async for chunk in async_stream:
                    yield chunk
                
                return

            except RateLimitError as e:
                if attempt == MAX_ATTEMPTS:
                    log_error(
                        f"Max retries ({MAX_ATTEMPTS}) exceeded for streaming response"
                    )
                    raise

                # Calculate wait time using exponential backoff
                wait_time = min(10 * (2**attempt), 600)
                log_warning(
                    f"Retrying ainvoke_stream, attempt {attempt + 1}/{MAX_ATTEMPTS} after {e}"
                )
                await asyncio.sleep(wait_time)

            except (BadRequestError, APIError) as e:
                raise

            except Exception as e:
                log_error(f"Error in streaming response: {e}")
                raise

    # Model functions implemented with fallback behavior (summarization)
    # TODO: Maybe consider maintaining a rolling summary
    def response(
        self,
        messages: List[Message],
        *args,
        **kwargs
    ):
        last_exception = None
        for _ in range(MAX_ATTEMPTS):
            # Try this for MAX_ATTEMPTS times
            try:
                return super().response(messages=messages, *args, **kwargs)
            
            except (BadRequestError, APIError) as e:
                last_exception = e

                # Implement fallback behavior
                
                if self.summarizer is None or not self.summarize_on_fail:
                    raise

                # Summarize since possible
                summary = asyncio.run(self.summarizer.arun(conversation=deepcopy(messages)))

                # Modify messages to be [initial system message, initial user message, summary]
                system_message = messages[0]

                messages.clear()
                messages.extend([
                        system_message,
                        Message(
                            role="user",
                            content=dedent(f"""
                                The conversation exceed the maximum context window. The following is a summary obtained from the conversation thus far. Continue your action based on this summmary
                                
                                <summmary>
                                    {summary.summary}
                                </summary>
                                """)
                        )
                    ])
                
            except Exception as e:
                raise

        raise last_exception

    async def aresponse(
        self,
        messages: List[Message],
        *args,
        **kwargs
    ):
        last_exception = None
        for _ in range(MAX_ATTEMPTS):
            # Try this for MAX_ATTEMPTS times
            try:
                return await super().aresponse(messages=messages, *args, **kwargs)
            
            except (BadRequestError, APIError) as e:
                last_exception = e

                # Implement fallback behavior
                
                if self.summarizer is None or not self.summarize_on_fail:
                    raise

                # Summarize since possible
                summary = await self.summarizer.arun(conversation=deepcopy(messages))

                # Modify messages to be [initial system message, initial user message, summary]
                system_message = messages[0]

                messages.clear()
                messages.extend([
                        system_message,
                        Message(
                            role="user",
                            content=dedent(f"""
                                The conversation exceed the maximum context window. The following is a summary obtained from the conversation thus far. Continue your action based on this summmary
                                
                                <summmary>
                                    {summary.summary}
                                </summary>
                                """)
                        )
                    ])
                
            except Exception as e:
                raise
            
        raise last_exception

    def response_stream(
        self,
        messages: List[Message],
        *args,
        **kwargs
    ):  
        last_exception = None
        for _ in range(MAX_ATTEMPTS):
            # Try this for MAX_ATTEMPTS times
            try:
                yield from super().response_stream(messages, *args, **kwargs)

                # If everything is successfully yielded then return
                return
            
            except (BadRequestError, APIError) as e:
                last_exception = e

                # Implement fallback behavior
                
                if self.summarizer is None or not self.summarize_on_fail:
                    raise

                # Summarize since possible
                summary = asyncio.run(self.summarizer.arun(conversation=deepcopy(messages)))

                # Modify messages to be including the initial system message, and the summary as a user message
                system_message = messages[0]
                assert system_message.role == "system", "Error in extracting system_message"

                messages.clear()
                messages.extend([
                        system_message,
                        Message(
                            role="user",
                            content=dedent(f"""
                                The conversation exceed the maximum context window. The following is a summary obtained from the conversation thus far. Continue your action based on this summmary
                                
                                <summmary>
                                    {summary.summary}
                                </summary>
                                """)
                        )
                    ])
                
            except Exception as e:
                raise

        raise last_exception

    async def aresponse_stream(
        self,
        messages: List[Message],
        *args,
        **kwargs,
    ):  
        last_exception = None
        for _ in range(MAX_ATTEMPTS):
            # Try this for MAX_ATTEMPTS times
            try:
                response_stream = super().aresponse_stream(messages, *args, **kwargs)
                async for response in response_stream:
                    yield response

                # If everything is successfully yielded then return
                return
            
            except (BadRequestError, APIError) as e:
                last_exception = e

                # Implement fallback behavior
                
                if self.summarizer is None or not self.summarize_on_fail:
                    raise

                # Summarize since possible
                summary = await self.summarizer.arun(conversation=deepcopy(messages))

                # Modify messages to be [initial system message, initial user message, summary]
                system_message = messages[0]
                assert system_message.role == "system", "Error in extracting system_message"

                messages.clear()
                messages.extend([
                        system_message,
                        Message(
                            role="user",
                            content=dedent(f"""
                                The conversation exceed the maximum context window. The following is a summary obtained from the conversation thus far. Continue your action based on this summmary
                                
                                <summmary>
                                    {summary.summary}
                                </summary>
                                """)
                        )
                    ])
                
            except Exception as e:
                raise

        raise last_exception


@dataclass
class LiteLLMOpenAIWithRetry(LiteLLMOpenAI):
    """A class for interacting with LiteLLMOpenAI with retry capabilities.

    This class extends LiteLLMOpenAI to handle retries for API requests.
    Note that LiteLLMOpenAI does not support stream=True when tools are supplied.
    """

    @retry(
        stop=stop_after_attempt(MAX_ATTEMPTS),
        wait=wait_exponential(multiplier=10, min=10, max=600),
        retry=retry_if_exception_type(RateLimitError),
        before_sleep=log_retry_attempt,
        after=log_retry_stop,
    )
    def call_llm_with_retry(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> ChatCompletion:
        return self.get_client().chat.completions.create(
            model=self.id,
            messages=[self._format_message(m) for m in messages],  # type: ignore
            **self.get_request_params(
                response_format=response_format, tools=tools, tool_choice=tool_choice
            ),
        )

    @retry(
        stop=stop_after_attempt(MAX_ATTEMPTS),
        wait=wait_exponential(multiplier=10, min=10, max=600),
        retry=retry_if_exception_type(RateLimitError),
        before_sleep=log_retry_attempt,
        after=log_retry_stop,
    )
    async def call_llm_with_retry_async(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> ChatCompletion:
        return await self.get_async_client().chat.completions.create(
            model=self.id,
            messages=[self._format_message(m) for m in messages],  # type: ignore
            **self.get_request_params(
                response_format=response_format, tools=tools, tool_choice=tool_choice
            ),
        )

    @retry(
        stop=stop_after_attempt(MAX_ATTEMPTS),
        wait=wait_exponential(multiplier=10, min=10, max=600),
        retry=retry_if_exception_type(RateLimitError),
        before_sleep=log_retry_attempt,
        after=log_retry_stop,
    )
    def stream_llm_with_retry(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> Iterator[ChatCompletionChunk]:
        yield from self.get_client().chat.completions.create(
            model=self.id,
            messages=[self._format_message(m) for m in messages],  # type: ignore
            stream=True,
            stream_options={"include_usage": True},
            **self.get_request_params(
                response_format=response_format, tools=tools, tool_choice=tool_choice
            ),
        )

    @retry(
        stop=stop_after_attempt(MAX_ATTEMPTS),
        wait=wait_exponential(multiplier=10, min=10, max=600),
        retry=retry_if_exception_type(RateLimitError),
        before_sleep=log_retry_attempt,
        after=log_retry_stop,
    )
    async def stream_llm_with_retry_async(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> AsyncIterator[ChatCompletionChunk]:
        async_stream = await self.get_async_client().chat.completions.create(
            model=self.id,
            messages=[self._format_message(m) for m in messages],  # type: ignore
            stream=True,
            stream_options={"include_usage": True},
            **self.get_request_params(
                response_format=response_format, tools=tools, tool_choice=tool_choice
            ),
        )

        async for chunk in async_stream:
            yield chunk

    def invoke(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> ChatCompletion:
        """
        Send a chat completion request to the OpenAI API.

        Args:
            messages (List[Message]): A list of messages to send to the model.

        Returns:
            ChatCompletion: The chat completion response from the API.
        """
        try:
            return self.call_llm_with_retry(
                messages=messages,
                response_format=response_format,
                tools=tools,
                tool_choice=tool_choice,
            )
        except RateLimitError as e:
            log_error(f"Rate limit error from OpenAI API: {e}")
            error_message = e.response.json().get("error", {})
            error_message = (
                error_message.get("message", "Unknown model error")
                if isinstance(error_message, dict)
                else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model_name=self.name,
                model_id=self.id,
            ) from e
        except APIConnectionError as e:
            log_error(f"API connection error from OpenAI API: {e}")
            raise ModelProviderError(
                message=str(e), model_name=self.name, model_id=self.id
            ) from e
        except APIStatusError as e:
            log_error(f"API status error from OpenAI API: {e}")
            try:
                error_message = e.response.json().get("error", {})
            except Exception:
                error_message = e.response.text
            error_message = (
                error_message.get("message", "Unknown model error")
                if isinstance(error_message, dict)
                else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model_name=self.name,
                model_id=self.id,
            ) from e
        except Exception as e:
            log_error(f"Error from OpenAI API: {e}")
            raise ModelProviderError(
                message=str(e), model_name=self.name, model_id=self.id
            ) from e

    async def ainvoke(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> ChatCompletion:
        """
        Sends an asynchronous chat completion request to the OpenAI API.

        Args:
            messages (List[Message]): A list of messages to send to the model.

        Returns:
            ChatCompletion: The chat completion response from the API.
        """
        try:
            return await self.call_llm_with_retry_async(
                messages=messages,
                response_format=response_format,
                tools=tools,
                tool_choice=tool_choice,
            )
        except RateLimitError as e:
            log_error(f"Rate limit error from OpenAI API: {e}")
            error_message = e.response.json().get("error", {})
            error_message = (
                error_message.get("message", "Unknown model error")
                if isinstance(error_message, dict)
                else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model_name=self.name,
                model_id=self.id,
            ) from e
        except APIConnectionError as e:
            log_error(f"API connection error from OpenAI API: {e}")
            raise ModelProviderError(
                message=str(e), model_name=self.name, model_id=self.id
            ) from e
        except APIStatusError as e:
            log_error(f"API status error from OpenAI API: {e}")
            try:
                error_message = e.response.json().get("error", {})
            except Exception:
                error_message = e.response.text
            error_message = (
                error_message.get("message", "Unknown model error")
                if isinstance(error_message, dict)
                else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model_name=self.name,
                model_id=self.id,
            ) from e
        except Exception as e:
            log_error(f"Error from OpenAI API: {e}")
            raise ModelProviderError(
                message=str(e), model_name=self.name, model_id=self.id
            ) from e

    def invoke_stream(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> Iterator[ChatCompletionChunk]:
        """
        Send a streaming chat completion request to the OpenAI API.

        Args:
            messages (List[Message]): A list of messages to send to the model.

        Returns:
            Iterator[ChatCompletionChunk]: An iterator of chat completion chunks.
        """

        try:
            yield from self.stream_llm_with_retry(
                messages=messages,
                response_format=response_format,
                tools=tools,
                tool_choice=tool_choice,
            )
        except RateLimitError as e:
            log_error(f"Rate limit error from OpenAI API: {e}")
            error_message = e.response.json().get("error", {})
            error_message = (
                error_message.get("message", "Unknown model error")
                if isinstance(error_message, dict)
                else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model_name=self.name,
                model_id=self.id,
            ) from e
        except APIConnectionError as e:
            log_error(f"API connection error from OpenAI API: {e}")
            raise ModelProviderError(
                message=str(e), model_name=self.name, model_id=self.id
            ) from e
        except APIStatusError as e:
            log_error(f"API status error from OpenAI API: {e}")
            try:
                error_message = e.response.json().get("error", {})
            except Exception:
                error_message = e.response.text
            error_message = (
                error_message.get("message", "Unknown model error")
                if isinstance(error_message, dict)
                else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model_name=self.name,
                model_id=self.id,
            ) from e
        except Exception as e:
            log_error(f"Error from OpenAI API: {e}")
            raise ModelProviderError(
                message=str(e), model_name=self.name, model_id=self.id
            ) from e

    async def ainvoke_stream(
        self,
        messages: List[Message],
        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> AsyncIterator[ChatCompletionChunk]:
        """
        Sends an asynchronous streaming chat completion request to the OpenAI API.

        Args:
            messages (List[Message]): A list of messages to send to the model.

        Returns:
            AsyncIterator[ChatCompletionChunk]: An asynchronous iterator of chat completion chunks.
        """
        try:
            async for chunk in self.stream_llm_with_retry_async(
                messages=messages,
                response_format=response_format,
                tools=tools,
                tool_choice=tool_choice,
            ):
                yield chunk
        except RateLimitError as e:
            log_error(f"Rate limit error from OpenAI API: {e}")
            error_message = e.response.json().get("error", {})
            error_message = (
                error_message.get("message", "Unknown model error")
                if isinstance(error_message, dict)
                else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model_name=self.name,
                model_id=self.id,
            ) from e
        except APIConnectionError as e:
            log_error(f"API connection error from OpenAI API: {e}")
            raise ModelProviderError(
                message=str(e), model_name=self.name, model_id=self.id
            ) from e
        except APIStatusError as e:
            log_error(f"API status error from OpenAI API: {e}")
            try:
                error_message = e.response.json().get("error", {})
            except Exception:
                error_message = e.response.text
            error_message = (
                error_message.get("message", "Unknown model error")
                if isinstance(error_message, dict)
                else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model_name=self.name,
                model_id=self.id,
            ) from e
        except Exception as e:
            log_error(f"Error from OpenAI API: {e}")
            raise ModelProviderError(
                message=str(e), model_name=self.name, model_id=self.id
            ) from e
