import time
import os
import asyncio
import json
from typing import Any, Optional, Dict, Literal, Union, List
import uuid
from db.database import Database
from db.dynamo import ExceedDynamoSizeLimit
from botocore.exceptions import ClientError
from config.context import current_node
from config.globals import S3
from schema.state import PentestNode
from server.sink.schema import StreamType, StreamEvent, StreamSink
import logging

logger = logging.getLogger(__name__)


class QueueSink:
    """Pushes messages to an asyncio.Queue to be consumed by a separate async task."""

    def __init__(self, q: asyncio.Queue[StreamEvent]) -> None:
        self._q = q
        self._closed = False

    async def send(self, event: StreamEvent) -> None:
        if not self._closed:
            await self._q.put(event)

    async def close(self) -> None:
        self._closed = True
        # await self._q.put(StreamEvent(type="end", data="Stream Close"))

    async def get(self) -> StreamEvent:
        evt = await self._q.get()
        return evt
    
class DBSink:
    """Pushes messages to an asyncio.Queue to be consumed by a separate async task."""

    def __init__(self, DB:Database, use_s3_fallback=False) -> None:
        self._closed = False
        self.db: Database = DB
        self.use_s3_fallback = use_s3_fallback

    async def send(self, event: StreamEvent) -> None:
        if self._closed:
            return
        else:
            curr_node: PentestNode|None = current_node.get()
            retry_count = getattr(curr_node, 'retry_counter', 0)

            # Only create a new db entry for the node if we receive an INITIATE event and it is on its first try
            if (event.type == StreamType.INITIATE) and (retry_count == 0):
                # Case when we begin a new module (e.g. run_x_agent), which is represented as a new node
                meta = {**event.metadata} if event.metadata else {}
                if 'node_id' in meta:
                    del meta['node_id']
                if 'session_id' in meta:
                    del meta['session_id']
                item = {
                    self.db.pk: event.metadata.get('session_id'),
                    self.db.sk: event.metadata.get('node_id'),
                    **meta,
                    'history': [
                        {
                            'type': event.type.value,
                            'data': event.data,
                            'ts': event.ts
                        }
                    ]
                }
                await self.db.acreate_item(item=item)
            
            elif (event.type == StreamType.INITIATE) and (retry_count > 0):
                logger.info(
                    f"{event.metadata.get('node_id')} is being retried (retry count: {retry_count}); appending to DB history instead of creating new entry."
                )

                # Case when we are retrying a node; don't create a new db entry but log the INITIATE event to history
                pk = event.metadata.get('session_id')
                sk = event.metadata.get('node_id')
                meta = {**event.metadata} if event.metadata else {}
                if 'node_id' in meta:
                    del meta['node_id']
                if 'session_id' in meta:
                    del meta['session_id']
                # Add a RETRY_BOUNDARY event to demarcate the retry clearly 
                await self.db.aappend_to_list(pk=pk, sk=sk, attr='history', values=[
                        {
                            'type': 'RETRY_BOUNDARY',
                            'data': '',
                            'metadata': {'retry_count': retry_count},
                            'ts': event.ts
                        },
                        {
                            'type': event.type.value,
                            'data': event.data,
                            'metadata': meta,
                            'ts': event.ts
                        }
                    ])
            elif event.type == StreamType.COMPLETE:
                # Case when we end a module (e.g. run_x_agent), which is represented as a new node
                
                pk = event.metadata.get('session_id')
                sk = event.metadata.get('node_id')
                final_output = event.metadata.get('final_output')
                meta = {**event.metadata} if event.metadata else {}
                
                if 'node_id' in meta:
                    del meta['node_id']
                if 'session_id' in meta:
                    del meta['session_id']
                if 'final_output' in meta:
                    del meta['final_output']
                    
                await self.db.aappend_to_list(pk=pk, sk=sk, attr='history', values=[
                        {
                            'type': event.type.value,
                            'data': event.data,
                            'metadata': meta,
                            'ts': event.ts
                        }
                    ])
                
                await self.db.aupdate_item(pk=pk, sk=sk, updates={'final_output': final_output})
                
            elif event.type == StreamType.RUN_COMPLETE:
                # Case when we complete pentest; this is handled by updating the global state and does not need to be recorded in history
                pass
            
            elif event.type == StreamType.USERINPUT:
                # Case when we request user input; this already creates a tool call and the StreamType.USERINPUT message is just for frontend interaction 
                pass
                
            else:
                # All other events generated within a module (e.g. model response stream, tool calls, diagnostics); appended to the node history
                
                pk = event.metadata.get('session_id')
                sk = event.metadata.get('node_id')
                meta = {**event.metadata} if event.metadata else {}
                if 'node_id' in meta:
                    del meta['node_id']
                if 'session_id' in meta:
                    del meta['session_id']
                try:
                    await self.db.aappend_to_list(pk=pk, sk=sk, attr='history', values=[
                        {
                            'type': event.type.value,
                            'data': event.data,
                            'metadata': meta,
                            'ts': event.ts
                        }
                    ])
                
                except (ExceedDynamoSizeLimit, ClientError) as e: 
                    logger.error(f'DBSink error appending to history due to size limit: {e}')

                    if self.use_s3_fallback and \
                        (e == ExceedDynamoSizeLimit or (e.response["Error"]["Code"] == "ValidationException" and \
                                                      "size" in e.response["Error"]["Message"])):
                        blob_id = str(uuid.uuid4())
                        bucket_name = os.getenv("S3_BUCKET_NAME", "maya-apt")
                        blob_name = f's3://{bucket_name}/logs/{blob_id}.json'
                        await S3.awrite(
                            s3_uri=blob_name,
                            content=json.dumps(meta),
                        )

                        await self.db.aappend_to_list(pk=pk, sk=sk, attr='history', values=[
                             {
                                'type': event.type.value,
                                'data': event.data,
                                'metadata': {
                                    's3_uri': blob_name
                                },
                                'ts': event.ts
                            }
                        ])
                        logger.info(f'Offloaded large event metadata for {event.type.value} to S3 at {blob_name}')
                    else:
                        raise 

                except Exception as e:
                    logger.error(f'DBSink error appending to history: {e}; Event details: type={event.type}, pk={pk}, sk={sk}')
                    raise Exception(f"Failed to append event to database history: {e}") from e
                
    async def close(self) -> None:
        self._closed = True
        # await self._q.put(StreamEvent(type="end", data="Stream Close"))

    async def get(self) -> StreamEvent:
        pass


class StorageSink:
    """Sink that writes events to a file, one per line."""

    def __init__(self, file_path: str) -> None:
        self._file_path = file_path
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        self._file = open(file_path, "a", encoding="utf-8")
        self._closed = False

    async def send(self, event: StreamEvent) -> None:
        if self._closed:
            raise RuntimeError("Cannot send to a closed StorageSink.")
        json_line = event.to_json()
        self._file.write(json_line + "\n")
        self._file.flush()

    async def close(self) -> None:
        if not self._closed:
            self._file.close()
            self._closed = True

    async def get(self):
        pass


class MultiSink:
    """Pushes messages to multiple sinks. 
    
    Optionally provides a buffer for 'RunResponseContent' events (streamed model responses) to moderate consumer load. 
    The buffer accumulates messages until self.buffer_size is reached or a non-RunResponseContent event occurs.
    """

    def __init__(self, sinks: Dict[str, StreamSink], buffer_size=None, allow_exceptions=False) -> None:
        self._closed = False
        self._sinks = sinks
        self.buffer_size = buffer_size
        self._buffer = []
        self.allow_exceptions = allow_exceptions
        self.dropped_events = 0

    async def send(self, event: StreamEvent) -> None:
        t = getattr(event, "type", None)
        if not self._closed:
            
            # If buffer is inactive, dispatch events right away 
            if self.buffer_size is None:
                errors = []
                for sink_name, _sink in self._sinks.items():
                    try:
                        await _sink.send(event)
                    except Exception as e:
                        errors.append(f"Sink failure '{sink_name}': {e}")
                        self.dropped_events += 1  
                
                if self.allow_exceptions:
                    if errors: logger.warning(f'Sink has allow_exceptions=True; continuing despite errors. Dropped messages: {self.dropped_events}')
                    for err in errors:
                        logger.error(err)
                else:
                    if errors:
                        raise Exception(f"One or more sinks failed: {'; '.join(errors)}")
                    
            else:
                # If buffer is active, dispatch all buffered events if we hit a non-text chunk or if we exceed the buffer size
                # We do this to avoid overwhelming DB or file sinks with too many small writes
                if t != StreamType.RUN_RESPONSE_CONTENT or len(self._buffer) > self.buffer_size:
                    # Combine all the buffered text and send as one event 
                    if self._buffer:
                        accumulated_buffer_text = "".join([x.data for x in self._buffer])
                        buffer_event = self._buffer[0]
                        buffer_event.data = accumulated_buffer_text
                        errors = []
                        for sink_name, _sink in self._sinks.items():
                            try:
                                await _sink.send(buffer_event)
                            except Exception as e:
                                errors.append(f"Sink '{sink_name}': {e}")
                                self.dropped_events += 1  

                        if self.allow_exceptions:
                            if errors: logger.warning(f'Sink has allow_exceptions=True; continuing despite errors. Dropped messages: {self.dropped_events}')
                            for err in errors:
                                logger.error(err)
                        else:
                            if errors:
                                raise Exception(f"One or more sinks failed during buffer purge: {'; '.join(errors)}")

                        self._buffer.clear()
                        
                    # Then send the new event after, to preserve chronology
                    errors = []
                    for sink_name, _sink in self._sinks.items():
                        try:
                            await _sink.send(event)
                        except Exception as e:
                            errors.append(f"Sink '{sink_name}': {e}")
                            self.dropped_events += 1  
                    
                    if self.allow_exceptions:
                        if errors: logger.warning(f'Sink has allow_exceptions=True; continuing despite errors. Dropped messages: {self.dropped_events}')
                        for err in errors:
                            logger.error(err)
                    else:
                        if errors:
                            raise Exception(f"One or more sinks failed: {'; '.join(errors)}")
                    
                # If not add to the buffer
                else:
                    self._buffer.append(event)


    async def close(self, sink_name: Optional[str] = None) -> None:
        if sink_name is not None and sink_name in self._sinks:
            self._sinks[sink_name].close()
        else:
            for name in self._sinks:
                try:
                    await self._sinks[name].close()
                except Exception as e:
                    logger.error(f'Failed to close sink {name}; error: {e}')
            self._closed = True

    async def get(self, sink_name: str) -> StreamEvent:
        if sink_name not in self._sinks:
            return None
        else:
            evt = await self._sinks[sink_name].get()
            return evt

