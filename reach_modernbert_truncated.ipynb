{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-storage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvxQZ-DJknq7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746840840126,
          "user_tz": -480,
          "elapsed": 2808,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "67f47c21-bea3-4092-96c9-6d208a7bca11"
      },
      "id": "tvxQZ-DJknq7",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.27.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (1.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.12.14)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.51.3 datasets peft accelerate bitsandbytes wandb deepspeed lion-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwHjybkC9rxv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746840842856,
          "user_tz": -480,
          "elapsed": 2733,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "cfd6aa81-f26e-476a-a46d-cd59e01ce587"
      },
      "id": "nwHjybkC9rxv",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.10/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.6.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.5)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.16.7)\n",
            "Requirement already satisfied: lion-pytorch in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.51.3) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (69.5.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed) (0.8.0)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.575.51)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.11)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (1.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.51.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.51.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.51.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.51.3) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj3GnrvWCuwZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746840845480,
          "user_tz": -480,
          "elapsed": 2626,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "03b4533b-9a87-493d-f5db-9ec32014893d"
      },
      "id": "Tj3GnrvWCuwZ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --pre triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-GU8-lBKg7u",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746840849799,
          "user_tz": -480,
          "elapsed": 4321,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "85b26d26-3e73-4dcf-ba16-18a2944d2296"
      },
      "id": "L-GU8-lBKg7u",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton) (69.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data"
      ],
      "metadata": {
        "id": "s2Tx2-NfkrK8"
      },
      "id": "s2Tx2-NfkrK8"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "def save_json(data, filename):\n",
        "    # Get the directory from the filename\n",
        "    directory = os.path.dirname(filename)\n",
        "\n",
        "    # Check if the directory exists, if not, create it\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Save the data to the file\n",
        "    with open(filename, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "\n",
        "def list_files_in_bucket(bucket_name, prefix=\"\"):\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=prefix)\n",
        "\n",
        "    # Print the list of file names in the bucket\n",
        "    print(\"Files in the bucket:\")\n",
        "    for blob in blobs:\n",
        "        print(blob.name)\n",
        "\n",
        "def load_json_from_gcs(bucket_name, file_name):\n",
        "    from google.cloud import storage\n",
        "    import json\n",
        "\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    if not file_name.endswith('.jsonl'):  # Ensure it's a JSONL file\n",
        "        raise ValueError(f\"The specified file '{file_name}' is not a JSONL file.\")\n",
        "\n",
        "    concatenated_data = []  # To accumulate JSON objects\n",
        "    try:\n",
        "        # Download and decode the file content\n",
        "        content = blob.download_as_string().decode('utf-8')\n",
        "        # Split content by lines and load each line as a separate JSON object\n",
        "        for line in content.splitlines():\n",
        "            if line.strip():  # Only parse non-empty lines\n",
        "                json_obj = json.loads(line)\n",
        "                concatenated_data.append(json_obj)  # Add JSON object to the list\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON in {file_name}: {e}\")\n",
        "\n",
        "    # Return the JSON as a string for output\n",
        "    return json.dumps(concatenated_data, indent=4)  # Prettify the JSON output\n",
        "\n",
        "def load_csv_from_gcs(bucket_name, file_name):\n",
        "    from google.cloud import storage\n",
        "    import pandas as pd\n",
        "    from io import StringIO  # Corrected import for StringIO\n",
        "\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    if not file_name.endswith('.csv'):  # Ensure it's a CSV file\n",
        "        raise ValueError(f\"The specified file '{file_name}' is not a CSV file.\")\n",
        "\n",
        "    try:\n",
        "        # Download CSV content and load it into a pandas DataFrame\n",
        "        content = blob.download_as_string().decode('utf-8')\n",
        "        data = pd.read_csv(StringIO(content))  # Use StringIO to parse the CSV content\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV file '{file_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_csv_to_gcs(bucket_name, file_name, dataframe):\n",
        "    from google.cloud import storage\n",
        "    import pandas as pd\n",
        "\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    if not file_name.endswith('.csv'):\n",
        "        raise ValueError(f\"The specified file '{file_name}' is not a CSV file.\")\n",
        "\n",
        "    try:\n",
        "        # Convert the DataFrame to CSV and upload it to GCS\n",
        "        csv_content = dataframe.to_csv(index=False)  # Convert DataFrame to CSV string\n",
        "        blob.upload_from_string(csv_content, content_type='text/csv')\n",
        "        print(f\"File '{file_name}' successfully saved to bucket '{bucket_name}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving CSV file '{file_name}': {e}\")"
      ],
      "metadata": {
        "id": "eMEeoVSoktAy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949639522,
          "user_tz": -480,
          "elapsed": 130,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "eMEeoVSoktAy",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = load_csv_from_gcs(\"mddi-reach-conversation\", \"modernbert/modernbert_train_truncated.csv\")\n",
        "augmented_full_df = load_csv_from_gcs(\"mddi-reach-conversation\", \"modernbert/modernbert_augmented_truncated.csv\")\n",
        "test_data = load_csv_from_gcs(\"mddi-reach-conversation\", \"modernbert/modernbert_test_truncated.csv\").rename({'key_point': 'stance', 'person_id': 'user'}, axis=1)\n",
        "vali_data = load_csv_from_gcs(\"mddi-reach-conversation\", \"modernbert/modernbert_val_truncated.csv\").rename({'key_point': 'stance', 'person_id': 'user'}, axis=1)\n",
        "print(augmented_full_df.shape)\n",
        "augmented_full_df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "Bkz7dCWYku1L",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949661461,
          "user_tz": -480,
          "elapsed": 20129,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c6df886c-3649-4ebe-af4f-b7efa0643ea6"
      },
      "id": "Bkz7dCWYku1L",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4943, 7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      chat_group_id  contributor  \\\n",
              "4938           1279          333   \n",
              "4939            457          728   \n",
              "4940            757          130   \n",
              "4941            637          708   \n",
              "4942           1928          619   \n",
              "\n",
              "                                                 stance  \\\n",
              "4938  Contributors raised strong concerns regarding ...   \n",
              "4939  Contributors expressed concern over the govern...   \n",
              "4940  Contributors have expressed concern over the r...   \n",
              "4941  Contributors expressed concern over the rising...   \n",
              "4942  Contributors highlighted the importance of inc...   \n",
              "\n",
              "                                         content_concat  label human_label  \\\n",
              "4938  Contributor955: Not to forget, the dynamics of...      0     neutral   \n",
              "4939  Contributor775: While affordable housing is cr...      0     neutral   \n",
              "4940  Contributor508: Perhaps we could consider that...      1       agree   \n",
              "4941  Contributor556: Access to knowledge is the beg...      1       agree   \n",
              "4942  Contributor288: What about the pressure it mig...      0    disagree   \n",
              "\n",
              "                                                 prompt  \n",
              "4938  <s>[INST]Determine whether Contributor333 hold...  \n",
              "4939  <s>[INST]Determine whether Contributor728 hold...  \n",
              "4940  <s>[INST]Determine whether Contributor130 hold...  \n",
              "4941  <s>[INST]Determine whether Contributor708 hold...  \n",
              "4942  <s>[INST]Determine whether Contributor619 hold...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9aabeef0-cf78-439b-9274-d2f47693555a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chat_group_id</th>\n",
              "      <th>contributor</th>\n",
              "      <th>stance</th>\n",
              "      <th>content_concat</th>\n",
              "      <th>label</th>\n",
              "      <th>human_label</th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4938</th>\n",
              "      <td>1279</td>\n",
              "      <td>333</td>\n",
              "      <td>Contributors raised strong concerns regarding ...</td>\n",
              "      <td>Contributor955: Not to forget, the dynamics of...</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>&lt;s&gt;[INST]Determine whether Contributor333 hold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4939</th>\n",
              "      <td>457</td>\n",
              "      <td>728</td>\n",
              "      <td>Contributors expressed concern over the govern...</td>\n",
              "      <td>Contributor775: While affordable housing is cr...</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>&lt;s&gt;[INST]Determine whether Contributor728 hold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4940</th>\n",
              "      <td>757</td>\n",
              "      <td>130</td>\n",
              "      <td>Contributors have expressed concern over the r...</td>\n",
              "      <td>Contributor508: Perhaps we could consider that...</td>\n",
              "      <td>1</td>\n",
              "      <td>agree</td>\n",
              "      <td>&lt;s&gt;[INST]Determine whether Contributor130 hold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4941</th>\n",
              "      <td>637</td>\n",
              "      <td>708</td>\n",
              "      <td>Contributors expressed concern over the rising...</td>\n",
              "      <td>Contributor556: Access to knowledge is the beg...</td>\n",
              "      <td>1</td>\n",
              "      <td>agree</td>\n",
              "      <td>&lt;s&gt;[INST]Determine whether Contributor708 hold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4942</th>\n",
              "      <td>1928</td>\n",
              "      <td>619</td>\n",
              "      <td>Contributors highlighted the importance of inc...</td>\n",
              "      <td>Contributor288: What about the pressure it mig...</td>\n",
              "      <td>0</td>\n",
              "      <td>disagree</td>\n",
              "      <td>&lt;s&gt;[INST]Determine whether Contributor619 hold...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9aabeef0-cf78-439b-9274-d2f47693555a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9aabeef0-cf78-439b-9274-d2f47693555a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9aabeef0-cf78-439b-9274-d2f47693555a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-916dc0aa-2551-4142-b0c6-ce046e262796\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-916dc0aa-2551-4142-b0c6-ce046e262796')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-916dc0aa-2551-4142-b0c6-ce046e262796 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"augmented_full_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"chat_group_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 596,\n        \"min\": 457,\n        \"max\": 1928,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          457,\n          1928,\n          757\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contributor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 261,\n        \"min\": 130,\n        \"max\": 728,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          728,\n          619,\n          130\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stance\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Contributors expressed concern over the government's recent decision to reduce the foreign worker quota in various sectors, arguing that this could lead to labor shortages and increased operational costs for small businesses, ultimately affecting job opportunities for local workers as firms may restructure to cope with higher wages.\",\n          \"Contributors highlighted the importance of increased mental health support services for youth in Singapore, calling for more accessible resources to address rising anxiety and depression among adolescents.\",\n          \"Contributors have expressed concern over the rising cost of living, urging the government to implement more targeted financial assistance programs for low- and middle-income families to alleviate their economic burdens.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content_concat\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Contributor775: While affordable housing is crucial, we also need to consider sustainable urban planning to ensure that these developments dont just offer lower prices but also quality living conditions for families.\\nContributor679: I don\\u2019t agree leh. Cost of living high, but government also doing their part to help us. Like the PASE (Progressive Wage Model) and other initiatives to upskill workers. Not only tuition fees or house prices increase lah, but lifestyle too. Must learn to manage money better also, can help ourselves. Otherwise keep blaming the government, how? Think they can solve everything ah? Better to look at what we can do, like improving ourselves, then can earn more.\\nContributor232: Perhaps we should also consider how the shifting job market influences wage dynamics and the type of jobs available to residents.\\nContributor677: Perhaps we should also consider how individual financial literacy can empower households to better navigate these challenges.\\nContributor232: While the governments support for low-income households is crucial, it might also be worth considering how individual financial literacy and personal budgeting can play significant roles in managing household expenses amidst rising costs.\\nContributor576: While I understand the concerns about rising costs, it\\u2019s essential to recognize that many households have adapted by finding innovative ways to budget and maximize their spending, such as embracing community initiatives that provide shared resources.\\nContributor232: Are we considering the role of private sectors and businesses in wage growth and support for low-income households as part of the solution?\\nContributor679: Great to see initiatives like SkillsFuture that empower individuals to upskill and adapt to the changing job market, which can help mitigate rising costs in the long run.\\nContributor836: Perhaps exploring more sustainable economic growth strategies could be the key to addressing this ongoing concern.\\nContributor543: Have we considered the potential role of private sector initiatives in addressing wage growth and supporting low-income households alongside government measures?\\nContributor266: While government measures are important, we should also consider the role of personal financial planning and how individuals can adapt to rising costs.\\nContributor843: While I understand the concerns about the cost of living, one might also consider how increased spending can actually drive local businesses and stimulate job creation, benefiting the economy as a whole.\\nContributor838: I strongly believe that expanding the SkillsFuture initiative to include more targeted training programs for digital skills is essential. In todays job market, many industries are rapidly shifting towards technology-driven operations. For example, if we can provide training for data analytics or cybersecurity, it will empower both employees and employers to adapt to these changes. It\\u2019s not just about basic computer literacy; advanced tech skills can lead to job security and career growth. Offering subsidies for these specific courses can attract more individuals to upgrade their skills., Additionally, I think its important to have greater access to SkillsFuture credits for older workers who may have been in the same job for many years. A tailored program that helps them transition into new roles could alleviate some employment issues we see in the senior demographic. For instance, vocational training for in-demand skills in healthcare or renewable energy can diversify their career options. Its about making sure that no one is left behind in this evolving job landscape.\\nContributor848: While investing in skills is essential, one must consider how accessible these opportunities are for everyone, as not all individuals may have the same ability to take advantage of them.\\nContributor376: While I understand the value placed on SkillsFuture, Ive seen many people who still struggle to utilize the skills theyve acquired in real-world scenarios, almost like buying a fancy kitchen appliance that just sits there unused.\\nContributor838: Its interesting to think about how my uncle used SkillsFuture to upskill in digital marketing, but I wonder if we should also prioritize making these resources more accessible to those who may not be tech-savvy or have time to learn.\\nContributor140: While SkillsFuture is essential, we must ensure that the training programs align closely with the actual needs of the job market.\\nContributor548: While its vital to support lifelong learning, could we consider how we might better tailor these programs to the specific needs of different industries?\\nContributor636: While lifelong learning is important, its equally crucial to assess whether the current programs truly address the skills gap in our specific industry needs.\\nContributor632: Aiyoh, but not everybody can benefit from SkillsFuture lah, some people really need other types of support to find their way in the job market, you know?\\nContributor838: Yet, we should also consider the accessibility and effectiveness of these programs for all demographics, as not everyone benefits equally from such initiatives.\\nContributor372: While the SkillsFuture initiative is indeed valuable, one must consider whether sufficient awareness and utilization of these resources are being achieved across all demographics, particularly among those who may not be actively seeking to upskill.\\nContributor140: While lifelong learning is essential, we should also consider how accessible resources are for those who may have financial or time constraints, as not all individuals can equally benefit from expanded initiatives like SkillsFuture.\\nContributor554: I must express my strong disagreement with the notion that reducing the foreign worker quota will inevitably lead to labor shortages and negatively impact local job opportunities. In fact, this decision can be seen as a long-overdue catalyst for innovation and efficiency within our local businesses.A key point to consider is the potential for increased automation and technological adoption in response to a reduced foreign workforce. Companies may accelerate their investments in technology-driven solutions, streamlining operations, and improving productivity. For instance, small businesses might adopt tools like Artificial Intelligence and robotics to manage tasks that were previously performed by foreign workers. This shift not only helps mitigate labor shortages but also positions local enterprises to become more competitive in global markets.Moreover, with a tighter labor market, businesses may be compelled to offer more attractive salary packages and better working conditions to retain local talent. This can enhance job satisfaction and improve worker retention rates. Ultimately, this could lead to a more skilled and motivated workforce, creating a positive cycle that benefits local communities.Let\\u2019s also consider the importance of nurturing local talent. By prioritizing local hires and reducing reliance on foreign workers, we can invest in training and upskilling programs for Singaporeans. This investment in human capital helps ensure that local workers are better equipped for a rapidly changing job landscape, ultimately benefiting our economy.In conclusion, the anticipation of labor shortages and increased costs need not overshadow the potential benefits of this policy decision. Rather than viewing the reduction of foreign worker quotas as a setback, we should embrace it as an opportunity for growth, innovation, and development in our workforce and economy.\\nContributor295: While labor shortages could pose challenges for some businesses, it might also encourage local workers to upskill and fill those roles, potentially leading to a more competitive job market.\\nContributor728: Thats an interesting point; I often think about how adapting to change can sometimes lead to unexpected innovations in business practices.\",\n          \"Contributor288: What about the pressure it might put on teachers, though? They already have so much on their plates.\\nContributor11: While early intervention is important, we might be overemphasizing institutional support instead of empowering families to address mental health issues at home.\\nContributor412: While early intervention is crucial, one must consider that the greatest weapon against stress is our ability to choose one thought over another, highlighting the importance of empowering students with coping strategies alongside resources.\\nContributor133: But have we considered how we can ensure that the training for teachers is robust enough to genuinely identify and support students with mental health issues?\\nContributor1000: While early intervention is crucial, we must also consider how these initiatives can be integrated with existing curricula without overwhelming students.But hor, sometimes all these initiatives also need to consider how to involve the parents more, right?\\nContributor412: But sia, we also need to consider how training teachers to spot these issues might be just as important as the resources, you know?\\nContributor455: What if we considered the role of parents in supporting mental health at schools?\\nContributor360: Perhaps we should consider that some students may prefer to seek help outside of school, where they feel a bit more comfortable and less scrutinized by their peers.\\nContributor921: Its important to consider whether existing resources are being utilized effectively before introducing new initiatives.\\nContributor913: What about the potential impact on teachers who might feel overwhelmed by these additional responsibilities?\\nContributor11: While early intervention is important, theres also a risk of over-pathologizing normal childhood experiences, which could lead to unnecessary stigma around mental health.\\nContributor557: I wholeheartedly support the Medisave Care Scheme, particularly for its focus on chronic illness management. Having a family member who suffers from diabetes, Ive seen firsthand how overwhelming the medical costs can be. The financial relief this scheme provides could allow families like mine to focus more on treatment and well-being rather than worrying about bills. However, Ive also noticed a distinct lack of awareness about this scheme among my peers. More campaigns, especially in community centres or online platforms, would empower eligible citizens to take full advantage of the benefits available to them. Its crucial that everyone knows about the resources that could alleviate their burdens.\\nContributor663: What we really need to consider is how access to care can be compromised by the complexity of the scheme itself\\u2014like how some low-income families might still struggle to navigate the system effectively, regardless of the funding available.I appreciate the sentiment, but isn\\u2019t it true that the road to hell is paved with good intentions? Awareness alone might not address deeper systemic issues.\\nContributor275: Have people considered how different chronic conditions might affect the uptake of programs like this?\\nContributor714: Awareness campaigns are great, but they often overlook the fact that many people may still find it confusing to navigate the system, which can deter them from even trying to access these benefits.\\nContributor599: However, one could consider whether there might be alternative solutions that could also address chronic illness management without relying solely on government programs.\\nContributor714: While addressing the financial burdens can be important, focusing solely on schemes like Medisave Care might overlook the need for broader systemic reforms that tackle healthcare access and quality for all citizens.\\nContributor28: Awareness is key, just like how my aunt needed to know about benefits for elderly care but didnt until someone else told her.\\nContributor861: It\\u2019s great that awareness campaigns are being suggested, but I wonder if the programs structure might also need some adjustments to truly reach all the citizens it aims to help.\\nContributor243: While the Medisave Care Scheme is a step in the right direction, its crucial to examine how its eligibility criteria might exclude some individuals who truly need assistance, as seen with similarly structured programs where eligibility has often posed a significant barrier for many potential beneficiaries.\\nContributor706: While awareness is important, as the saying goes, A fool with a plan is better than a genius without one\\u2014we need to ensure that the program itself is truly effective and sustainable before pushing for broader awareness.\\nContributor627: It\\u2019s interesting to consider how other countries have tackled chronic illness management through community health initiatives, which might also complement what the Medisave Care Scheme aims to achieve.\\nContributor419: While its important to support citizens, one must consider that the road to hell is paved with good intentions, and without a thorough evaluation of the Medisave Care Schemes long-term sustainability, we might inadvertently create more problems than we solve.\\nContributor860: It might also be worth considering how we can increase access to not just awareness, but also the actual usage of medical facilities under this scheme, as some might still face barriers in that regard.While awareness is critical, I wonder if the scheme does enough to address the needs of those most vulnerable.\\nContributor815: While the Medisave Care Scheme is indeed beneficial, one might wonder if the ease of access to digital tools could further streamline the application process, making it even simpler for citizens to utilize it effectively.\\nContributor924: However, its also worth considering if the Medisave Care Scheme adequately addresses the specific needs of diverse communities and their varying health management practices.\\nContributor872: But sometimes hor, people might still not know how to navigate the system, so maybe need easier ways for them to find info leh!\\nContributor194: Actually, more than just awareness, maybe we should also look at simplifying the application process so its easier for everyone to use the Medisave Care Scheme ah.\\nContributor714: While awareness is important, I think we also need to consider how effectively these funds are allocated to truly address the needs of those with chronic illnesses, as some may still struggle with out-of-pocket expenses despite the scheme.\\nContributor597: Its interesting how we often overlook the role of community support in managing chronic illnesses, as sometimes having a strong network can complement any financial schemes.\\nContributor557: Awareness is important, but as the saying goes, knowledge is of no value unless you put it into practice.\\nContributor531: While its acknowledged that the youth face increasing pressures, we might also consider that the greatest weapon against stress is our ability to choose one thought over another, as William James said. Are we perhaps overemphasizing the need for external resources rather than fostering internal resilience?\\nContributor153: Maybe must also consider how to engage parents better, so they know how to support their kids too, right?\\nContributor591: Maybe we could also explore how schools can integrate mental wellness activities into their curricula, like mindfulness exercises or workshops, to promote resilience among students.\\nContributor916: Absolutely, and perhaps we could also explore how integrating mental health education into school curriculums might help destigmatize these issues from an early age.\\nContributor318: While its crucial to increase resources, we also need to remember what Einstein said: A calm and modest life brings more happiness than the pursuit of success combined with constant restlessness.\\nContributor619: But why we must keep adding more services when parents can just spend more time with kids?\\n\\n\\nContributor253: Sometimes, it might be useful to explore the role of peer support systems alongside professional services, as they can create a more holistic support environment for youth.\\nContributor747: Have we considered the role of family engagement and communication in supporting youth mental health, rather than solely focusing on external resources?\\nContributor969: When I was in school, I found that focusing on extracurricular activities really helped to distract from stress rather than relying on support services.\\nContributor958: I think we should focus more on resilience-building programs rather than solely increasing support services.\\nContributor233: Maybe we should focus more on building resilience in our youth rather than just increasing support services; teaching them coping mechanisms could be more effective in the long run.\\nContributor958: Maybe we should focus on strengthening resilience and coping skills instead of just providing more support services.\\nContributor778: I remember when I was in school, the pressure to perform well was immense, so while more resources are great, we also need to consider how we can change expectations and create a more supportive environment.\\nContributor619: But isnt it also about teaching resilience and coping skills rather than just providing more services?\",\n          \"Contributor508: Perhaps we could consider that increased out-of-pocket expenses might drive more innovation in healthcare solutions, similar to how rising costs in other sectors push for advancements in technology and efficiency.\\nContributor104: While I understand the concerns about rising expenses, I believe that investing in one\\u2019s health is ultimately beneficial, as similar to how I prioritize spending on nutritious food, it pays off in the long run by reducing potential medical costs later on.\\nContributor318: But you know, maybe it\\u2019s also about how we can better manage our spending on healthcare, lah; like explore preventive measures so we don\\u2019t end up in hospital so often?Its important to consider whether there are alternative models that could alleviate the pressure on middle-income families while still ensuring adequate coverage for everyone.\\nContributor756: I think it\\u2019s important to consider how we can find balance, just like how we adapt our spending on things we enjoy, like choosing a simple yet fulfilling meal that suits our budget without compromising on quality.\\nContributor649: It could be interesting to consider how some families adapt their budgets to account for these changes, much like how we plan for unexpected expenses in our monthly finances.\\nContributor375: Its interesting to consider how some families might explore additional insurance options or health savings plans as potential ways to manage these rising costs more effectively.\\nContributor559: I strictly disagree with the notion that enhancing digital literacy programs is a pressing necessity for citizens. As Albert Einstein famously said, The measure of intelligence is the ability to change. We need to adapt our education system to focus more on critical thinking and creativity rather than just digital skills., Moreover, theres an inherent danger in emphasizing digital literacy as if it\\u2019s the sole solution. As Martin Luther King Jr. once remarked, Intelligence plus character\\u2014that is the goal of true education. We should be fostering well-rounded individuals capable of ethical decision-making rather than just digitally savvy ones.\\nContributor87: Its interesting how sometimes people equate digital literacy solely with technical skills, whereas understanding online etiquette and critical thinking is equally vital in this digital age.\\nContributor567: While enhancing digital literacy programs is one approach, we should also consider leveraging existing resources and partnerships to provide a more immediate impact on citizens skills and confidence in a digital economy.While digital literacy is important, we should also consider investing in hands-on practical experience and mentorship programs, as they can often provide more immediate and applicable skills for individuals in their daily lives.\\nContributor837: Perhaps we should also consider how access to technology itself might impact the effectiveness of digital literacy programs.\\nContributor668: Maybe instead of focusing solely on digital literacy, we should consider the importance of critical thinking skills to help people assess the information more effectively.\\nContributor567: What about focusing on fostering innovation and entrepreneurship instead? Many successful startups in Singapore emerged without formal digital literacy programs, driven by resourcefulness and creativity in leveraging existing skills.\\u201cDigital literacy is important, but as Albert Einstein once said, \\u2018The measure of intelligence is the ability to change.\\u2019 Perhaps we should focus on fostering adaptability rather than just enhancing skills.\\u201d\\nContributor428: Its interesting to consider how different digital tools might complement existing literacy programs, offering hands-on experiences that can engage learners more effectively.\\nContributor665: While enhancing digital literacy is crucial, we might also want to consider how access to technology itself plays a role in this empowerment.\\nContributor837: Its interesting to think about how many traditional skills have also been transformed by technology, like cooking or gardening, and these could be integrated into digital literacy to make learning more relatable.\\nContributor87: I wonder if we should also consider how different demographics might require tailored digital literacy initiatives to be truly effective in bridging the skills gap.\\nContributor665: While digital literacy is crucial, I think we should also focus on providing hands-on experience in real-world applications, like internships or workshops, to truly equip citizens for the digital job market.\\nContributor671: I understand the concern over the rising cost of living, but I disagree with the idea of implementing more targeted financial assistance programs. Instead, I believe enhancing opportunities for economic growth and education might yield better long-term results. For example, when I was a young adult, I found that investing in my skills and pursuing higher education not only improved my job prospects but also helped me manage financial challenges more effectively.Moreover, I think we should focus on fostering a culture of self-reliance rather than increasing dependence on government assistance. Like many others, I\\u2019ve faced tough financial times, yet I learned to budget and save wisely. The skills I developed during those times have served me well and instilled a sense of financial responsibility that I would prefer to see promoted over handouts.Furthermore, there are many successful initiatives that drive entrepreneurship and innovation within our communities. For instance, look at the hawker culture in Singapore; many families have thrived by creating their own small businesses. Supporting these entrepreneurial ventures could be a more effective avenue for addressing economic burdens rather than more government financial aid.I respect the viewpoint that immediate assistance might seem necessary, but I believe a holistic approach, considering the root causes of the economic pressures, would yield better outcomes. Encouraging a robust economic environment would allow individuals to lift themselves up with their capabilities and tenacity.\\nContributor641: But ah, maybe the focus should also be on creating more job opportunities loh, so people can earn more instead of just relying on assistance?\\nContributor253: The effectiveness of financial assistance programs may also depend on how well they are communicated to those who need them the most.\\nContributor148: While financial assistance is essential, enhancing employment opportunities and job training programs could also significantly support families in the long run.\\nContributor253: Perhaps instead of solely increasing financial aid, we could also focus on enhancing job training programs to help families improve their income potential in the long run.\\nContributor805: Its interesting to consider that while targeted financial assistance is essential, perhaps we should also explore initiatives that promote long-term financial literacy and empowerment for these families.But hor, maybe we should also think about how to boost wages for these families leh, so they can keep up without always relying on assistance.\\nContributor730: Could exploring alternative solutions, such as community support initiatives, also play a role in addressing the financial pressures faced by families?\\nContributor985: While I understand the concern for targeted financial assistance, I wonder if focusing solely on temporary relief might overlook the need for more sustainable solutions, like job creation or skills training, similar to how a garden needs consistent care rather than just occasional watering to flourish.\\nContributor540: Perhaps enhancing job opportunities and skills training could be a more effective approach.\\nContributor130: While financial assistance is important, perhaps we should also consider investing in skills training and education programs for these families to help them secure better-paying jobs in the long run.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"human_label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"neutral\",\n          \"agree\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"<s>[INST]Determine whether Contributor728 holds the same view as this statement: 'Contributors expressed concern over the government's recent decision to reduce the foreign worker quota in various sectors, arguing that this could lead to labor shortages and increased operational costs for small businesses, ultimately affecting job opportunities for local workers as firms may restructure to cope with higher wages.\\u2019? Based on the following conversation summary, respond with \\u20181\\u2019 if they share the view, or \\u20180\\u2019 otherwise. Even if it is implicit, consider it a match. Do not include any additional text. The following are messages by contributor 728 and other contributors: \\n Contributor775: While affordable housing is crucial, we also need to consider sustainable urban planning to ensure that these developments dont just offer lower prices but also quality living conditions for families.\\nContributor679: I don\\u2019t agree leh. Cost of living high, but government also doing their part to help us. Like the PASE (Progressive Wage Model) and other initiatives to upskill workers. Not only tuition fees or house prices increase lah, but lifestyle too. Must learn to manage money better also, can help ourselves. Otherwise keep blaming the government, how? Think they can solve everything ah? Better to look at what we can do, like improving ourselves, then can earn more.\\nContributor232: Perhaps we should also consider how the shifting job market influences wage dynamics and the type of jobs available to residents.\\nContributor677: Perhaps we should also consider how individual financial literacy can empower households to better navigate these challenges.\\nContributor232: While the governments support for low-income households is crucial, it might also be worth considering how individual financial literacy and personal budgeting can play significant roles in managing household expenses amidst rising costs.\\nContributor576: While I understand the concerns about rising costs, it\\u2019s essential to recognize that many households have adapted by finding innovative ways to budget and maximize their spending, such as embracing community initiatives that provide shared resources.\\nContributor232: Are we considering the role of private sectors and businesses in wage growth and support for low-income households as part of the solution?\\nContributor679: Great to see initiatives like SkillsFuture that empower individuals to upskill and adapt to the changing job market, which can help mitigate rising costs in the long run.\\nContributor836: Perhaps exploring more sustainable economic growth strategies could be the key to addressing this ongoing concern.\\nContributor543: Have we considered the potential role of private sector initiatives in addressing wage growth and supporting low-income households alongside government measures?\\nContributor266: While government measures are important, we should also consider the role of personal financial planning and how individuals can adapt to rising costs.\\nContributor843: While I understand the concerns about the cost of living, one might also consider how increased spending can actually drive local businesses and stimulate job creation, benefiting the economy as a whole.\\nContributor838: I strongly believe that expanding the SkillsFuture initiative to include more targeted training programs for digital skills is essential. In todays job market, many industries are rapidly shifting towards technology-driven operations. For example, if we can provide training for data analytics or cybersecurity, it will empower both employees and employers to adapt to these changes. It\\u2019s not just about basic computer literacy; advanced tech skills can lead to job security and career growth. Offering subsidies for these specific courses can attract more individuals to upgrade their skills., Additionally, I think its important to have greater access to SkillsFuture credits for older workers who may have been in the same job for many years. A tailored program that helps them transition into new roles could alleviate some employment issues we see in the senior demographic. For instance, vocational training for in-demand skills in healthcare or renewable energy can diversify their career options. Its about making sure that no one is left behind in this evolving job landscape.\\nContributor848: While investing in skills is essential, one must consider how accessible these opportunities are for everyone, as not all individuals may have the same ability to take advantage of them.\\nContributor376: While I understand the value placed on SkillsFuture, Ive seen many people who still struggle to utilize the skills theyve acquired in real-world scenarios, almost like buying a fancy kitchen appliance that just sits there unused.\\nContributor838: Its interesting to think about how my uncle used SkillsFuture to upskill in digital marketing, but I wonder if we should also prioritize making these resources more accessible to those who may not be tech-savvy or have time to learn.\\nContributor140: While SkillsFuture is essential, we must ensure that the training programs align closely with the actual needs of the job market.\\nContributor548: While its vital to support lifelong learning, could we consider how we might better tailor these programs to the specific needs of different industries?\\nContributor636: While lifelong learning is important, its equally crucial to assess whether the current programs truly address the skills gap in our specific industry needs.\\nContributor632: Aiyoh, but not everybody can benefit from SkillsFuture lah, some people really need other types of support to find their way in the job market, you know?\\nContributor838: Yet, we should also consider the accessibility and effectiveness of these programs for all demographics, as not everyone benefits equally from such initiatives.\\nContributor372: While the SkillsFuture initiative is indeed valuable, one must consider whether sufficient awareness and utilization of these resources are being achieved across all demographics, particularly among those who may not be actively seeking to upskill.\\nContributor140: While lifelong learning is essential, we should also consider how accessible resources are for those who may have financial or time constraints, as not all individuals can equally benefit from expanded initiatives like SkillsFuture.\\nContributor554: I must express my strong disagreement with the notion that reducing the foreign worker quota will inevitably lead to labor shortages and negatively impact local job opportunities. In fact, this decision can be seen as a long-overdue catalyst for innovation and efficiency within our local businesses.A key point to consider is the potential for increased automation and technological adoption in response to a reduced foreign workforce. Companies may accelerate their investments in technology-driven solutions, streamlining operations, and improving productivity. For instance, small businesses might adopt tools like Artificial Intelligence and robotics to manage tasks that were previously performed by foreign workers. This shift not only helps mitigate labor shortages but also positions local enterprises to become more competitive in global markets.Moreover, with a tighter labor market, businesses may be compelled to offer more attractive salary packages and better working conditions to retain local talent. This can enhance job satisfaction and improve worker retention rates. Ultimately, this could lead to a more skilled and motivated workforce, creating a positive cycle that benefits local communities.Let\\u2019s also consider the importance of nurturing local talent. By prioritizing local hires and reducing reliance on foreign workers, we can invest in training and upskilling programs for Singaporeans. This investment in human capital helps ensure that local workers are better equipped for a rapidly changing job landscape, ultimately benefiting our economy.In conclusion, the anticipation of labor shortages and increased costs need not overshadow the potential benefits of this policy decision. Rather than viewing the reduction of foreign worker quotas as a setback, we should embrace it as an opportunity for growth, innovation, and development in our workforce and economy.\\nContributor295: While labor shortages could pose challenges for some businesses, it might also encourage local workers to upskill and fill those roles, potentially leading to a more competitive job market.\\nContributor728: Thats an interesting point; I often think about how adapting to change can sometimes lead to unexpected innovations in business practices. [/INST]0</s>\",\n          \"<s>[INST]Determine whether Contributor619 holds the same view as this statement: 'Contributors highlighted the importance of increased mental health support services for youth in Singapore, calling for more accessible resources to address rising anxiety and depression among adolescents.\\u2019? Based on the following conversation summary, respond with \\u20181\\u2019 if they share the view, or \\u20180\\u2019 otherwise. Even if it is implicit, consider it a match. Do not include any additional text. The following are messages by contributor 619 and other contributors: \\n Contributor288: What about the pressure it might put on teachers, though? They already have so much on their plates.\\nContributor11: While early intervention is important, we might be overemphasizing institutional support instead of empowering families to address mental health issues at home.\\nContributor412: While early intervention is crucial, one must consider that the greatest weapon against stress is our ability to choose one thought over another, highlighting the importance of empowering students with coping strategies alongside resources.\\nContributor133: But have we considered how we can ensure that the training for teachers is robust enough to genuinely identify and support students with mental health issues?\\nContributor1000: While early intervention is crucial, we must also consider how these initiatives can be integrated with existing curricula without overwhelming students.But hor, sometimes all these initiatives also need to consider how to involve the parents more, right?\\nContributor412: But sia, we also need to consider how training teachers to spot these issues might be just as important as the resources, you know?\\nContributor455: What if we considered the role of parents in supporting mental health at schools?\\nContributor360: Perhaps we should consider that some students may prefer to seek help outside of school, where they feel a bit more comfortable and less scrutinized by their peers.\\nContributor921: Its important to consider whether existing resources are being utilized effectively before introducing new initiatives.\\nContributor913: What about the potential impact on teachers who might feel overwhelmed by these additional responsibilities?\\nContributor11: While early intervention is important, theres also a risk of over-pathologizing normal childhood experiences, which could lead to unnecessary stigma around mental health.\\nContributor557: I wholeheartedly support the Medisave Care Scheme, particularly for its focus on chronic illness management. Having a family member who suffers from diabetes, Ive seen firsthand how overwhelming the medical costs can be. The financial relief this scheme provides could allow families like mine to focus more on treatment and well-being rather than worrying about bills. However, Ive also noticed a distinct lack of awareness about this scheme among my peers. More campaigns, especially in community centres or online platforms, would empower eligible citizens to take full advantage of the benefits available to them. Its crucial that everyone knows about the resources that could alleviate their burdens.\\nContributor663: What we really need to consider is how access to care can be compromised by the complexity of the scheme itself\\u2014like how some low-income families might still struggle to navigate the system effectively, regardless of the funding available.I appreciate the sentiment, but isn\\u2019t it true that the road to hell is paved with good intentions? Awareness alone might not address deeper systemic issues.\\nContributor275: Have people considered how different chronic conditions might affect the uptake of programs like this?\\nContributor714: Awareness campaigns are great, but they often overlook the fact that many people may still find it confusing to navigate the system, which can deter them from even trying to access these benefits.\\nContributor599: However, one could consider whether there might be alternative solutions that could also address chronic illness management without relying solely on government programs.\\nContributor714: While addressing the financial burdens can be important, focusing solely on schemes like Medisave Care might overlook the need for broader systemic reforms that tackle healthcare access and quality for all citizens.\\nContributor28: Awareness is key, just like how my aunt needed to know about benefits for elderly care but didnt until someone else told her.\\nContributor861: It\\u2019s great that awareness campaigns are being suggested, but I wonder if the programs structure might also need some adjustments to truly reach all the citizens it aims to help.\\nContributor243: While the Medisave Care Scheme is a step in the right direction, its crucial to examine how its eligibility criteria might exclude some individuals who truly need assistance, as seen with similarly structured programs where eligibility has often posed a significant barrier for many potential beneficiaries.\\nContributor706: While awareness is important, as the saying goes, A fool with a plan is better than a genius without one\\u2014we need to ensure that the program itself is truly effective and sustainable before pushing for broader awareness.\\nContributor627: It\\u2019s interesting to consider how other countries have tackled chronic illness management through community health initiatives, which might also complement what the Medisave Care Scheme aims to achieve.\\nContributor419: While its important to support citizens, one must consider that the road to hell is paved with good intentions, and without a thorough evaluation of the Medisave Care Schemes long-term sustainability, we might inadvertently create more problems than we solve.\\nContributor860: It might also be worth considering how we can increase access to not just awareness, but also the actual usage of medical facilities under this scheme, as some might still face barriers in that regard.While awareness is critical, I wonder if the scheme does enough to address the needs of those most vulnerable.\\nContributor815: While the Medisave Care Scheme is indeed beneficial, one might wonder if the ease of access to digital tools could further streamline the application process, making it even simpler for citizens to utilize it effectively.\\nContributor924: However, its also worth considering if the Medisave Care Scheme adequately addresses the specific needs of diverse communities and their varying health management practices.\\nContributor872: But sometimes hor, people might still not know how to navigate the system, so maybe need easier ways for them to find info leh!\\nContributor194: Actually, more than just awareness, maybe we should also look at simplifying the application process so its easier for everyone to use the Medisave Care Scheme ah.\\nContributor714: While awareness is important, I think we also need to consider how effectively these funds are allocated to truly address the needs of those with chronic illnesses, as some may still struggle with out-of-pocket expenses despite the scheme.\\nContributor597: Its interesting how we often overlook the role of community support in managing chronic illnesses, as sometimes having a strong network can complement any financial schemes.\\nContributor557: Awareness is important, but as the saying goes, knowledge is of no value unless you put it into practice.\\nContributor531: While its acknowledged that the youth face increasing pressures, we might also consider that the greatest weapon against stress is our ability to choose one thought over another, as William James said. Are we perhaps overemphasizing the need for external resources rather than fostering internal resilience?\\nContributor153: Maybe must also consider how to engage parents better, so they know how to support their kids too, right?\\nContributor591: Maybe we could also explore how schools can integrate mental wellness activities into their curricula, like mindfulness exercises or workshops, to promote resilience among students.\\nContributor916: Absolutely, and perhaps we could also explore how integrating mental health education into school curriculums might help destigmatize these issues from an early age.\\nContributor318: While its crucial to increase resources, we also need to remember what Einstein said: A calm and modest life brings more happiness than the pursuit of success combined with constant restlessness.\\nContributor619: But why we must keep adding more services when parents can just spend more time with kids?\\n\\n\\nContributor253: Sometimes, it might be useful to explore the role of peer support systems alongside professional services, as they can create a more holistic support environment for youth.\\nContributor747: Have we considered the role of family engagement and communication in supporting youth mental health, rather than solely focusing on external resources?\\nContributor969: When I was in school, I found that focusing on extracurricular activities really helped to distract from stress rather than relying on support services.\\nContributor958: I think we should focus more on resilience-building programs rather than solely increasing support services.\\nContributor233: Maybe we should focus more on building resilience in our youth rather than just increasing support services; teaching them coping mechanisms could be more effective in the long run.\\nContributor958: Maybe we should focus on strengthening resilience and coping skills instead of just providing more support services.\\nContributor778: I remember when I was in school, the pressure to perform well was immense, so while more resources are great, we also need to consider how we can change expectations and create a more supportive environment.\\nContributor619: But isnt it also about teaching resilience and coping skills rather than just providing more services? [/INST]0</s>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns, test_data.columns, augmented_full_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otUk_i8z8Cj0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949661462,
          "user_tz": -480,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e469e32c-188d-46cb-fb12-12a36b52293b"
      },
      "id": "otUk_i8z8Cj0",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Index(['group', 'user', 'stance', 'topic_group', 'human_label', 'agreement',\n",
              "        'group_user', 'label', 'id', 'content_concat'],\n",
              "       dtype='object'),\n",
              " Index(['discussion_id', 'stance', 'contributor_name', 'key_point_id',\n",
              "        'contributor_id', 'expressed_view', 'user', 'topic', 'topic_statement',\n",
              "        'label', 'content_concat', 'prompt', 'translated_content'],\n",
              "       dtype='object'),\n",
              " Index(['chat_group_id', 'contributor', 'stance', 'content_concat', 'label',\n",
              "        'human_label', 'prompt'],\n",
              "       dtype='object'))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.concat([\n",
        "    train_data[[ 'label', 'stance', 'content_concat', 'human_label', 'topic_group', 'agreement', 'user', 'group_user']],\n",
        "    augmented_full_df[[  'stance', 'content_concat', 'label']],\n",
        "    ]\n",
        ").reset_index(drop=True)\n",
        "print(train_df.shape)\n",
        "train_df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "0qvuc4XM4o8p",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949662281,
          "user_tz": -480,
          "elapsed": 821,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ed50e86f-2c86-447f-8390-3e79f2413af0"
      },
      "id": "0qvuc4XM4o8p",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7348, 8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                             stance  \\\n",
              "0      0  Contributor shared different perspectives on p...   \n",
              "1      0  Contributor expressed differing levels of trus...   \n",
              "\n",
              "                                      content_concat human_label  \\\n",
              "0  Contributor926: Huh! They spoil market!\\nDo yo...  no opinion   \n",
              "1  Contributor926: Huh! They spoil market!\\nDo yo...    disagree   \n",
              "\n",
              "    topic_group  agreement                                  user  \\\n",
              "0  national_day  unanimous  04195570-f8b3-4eab-866f-32808d77d8e1   \n",
              "1  national_day   majority  04195570-f8b3-4eab-866f-32808d77d8e1   \n",
              "\n",
              "                              group_user  \n",
              "0  104195570-f8b3-4eab-866f-32808d77d8e1  \n",
              "1  104195570-f8b3-4eab-866f-32808d77d8e1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47e28b8e-1827-4944-916d-4ee7a8215853\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>stance</th>\n",
              "      <th>content_concat</th>\n",
              "      <th>human_label</th>\n",
              "      <th>topic_group</th>\n",
              "      <th>agreement</th>\n",
              "      <th>user</th>\n",
              "      <th>group_user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Contributor shared different perspectives on p...</td>\n",
              "      <td>Contributor926: Huh! They spoil market!\\nDo yo...</td>\n",
              "      <td>no opinion</td>\n",
              "      <td>national_day</td>\n",
              "      <td>unanimous</td>\n",
              "      <td>04195570-f8b3-4eab-866f-32808d77d8e1</td>\n",
              "      <td>104195570-f8b3-4eab-866f-32808d77d8e1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Contributor expressed differing levels of trus...</td>\n",
              "      <td>Contributor926: Huh! They spoil market!\\nDo yo...</td>\n",
              "      <td>disagree</td>\n",
              "      <td>national_day</td>\n",
              "      <td>majority</td>\n",
              "      <td>04195570-f8b3-4eab-866f-32808d77d8e1</td>\n",
              "      <td>104195570-f8b3-4eab-866f-32808d77d8e1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47e28b8e-1827-4944-916d-4ee7a8215853')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-47e28b8e-1827-4944-916d-4ee7a8215853 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-47e28b8e-1827-4944-916d-4ee7a8215853');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3ad9b619-6671-4d94-8320-2747af77e289\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3ad9b619-6671-4d94-8320-2747af77e289')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3ad9b619-6671-4d94-8320-2747af77e289 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 7348,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stance\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1787,\n        \"samples\": [\n          \"Contributors expressed frustration over the rising cost of living, highlighting that increases in public transport fares and utility prices disproportionately impact low-income families, calling for more assistance and targeted subsidies.\",\n          \"Contributors expressed strong support for the government's initiative to enhance mental health services, emphasizing the importance of accessibility and destigmatization in fostering a healthier society.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content_concat\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5177,\n        \"samples\": [\n          \"Contributor989: Its interesting to consider how other countries manage housing for young families; perhaps theres something we can learn from their systems and approaches.\\nContributor414: Housing prices reflect a robust economy, offering opportunities for investment and growth.\\nContributor546: I see where youre coming from, but when I bought my first flat, it taught me the value of planning and patience; not everyone needs to rush into homeownership right away.Sometimes, as the saying goes, Tough times never last, but tough people do. It\\u2019s about resilience and adapting to the situation at hand.\\nContributor230: But havent we also seen some innovative housing solutions emerging that could provide more options for these young families?While its true that housing prices are steep for young families, as Oscar Wilde said, Life is far too important a thing ever to talk seriously about it.\\nContributor484: Its true that housing prices are a concern, but as John F. Kennedy once said, Change is the law of life, and sometimes, these shifts can lead to new opportunities.\\nContributor812: While concerns about rising out-of-pocket expenses for healthcare are valid, I wonder if there are alternative approaches to consider rather than solely reviewing MediShield Life. For instance, focusing on preventive care could significantly reduce overall healthcare costs in the long run. If the government invested more in community health programs aimed at educating the public about healthy lifestyles, we could see a decrease in chronic illnesses, which are a primary driver of high medical expenses.\\\\n\\\\nAdditionally, by promoting early intervention and regular health screenings, patients could avoid more costly treatments down the line. Countries with strong preventive care strategies tend to have better health outcomes and lower healthcare spending, which could serve as a model for Singapore. Its an interesting avenue that could complement discussions about MediShield Life.\\nContributor560: Isnt it also worth considering how individuals can actively manage their health and expenses, potentially reducing the need for extensive coverage in the first place?\\nContributor494: This isnt just about MediShield Life; perhaps we should consider enhancing personal responsibility for healthcare decisions instead.\\nContributor80: But ah, we also need to think about how to balance premiums leh, otherwise how to sustain the system long-term?\\nContributor494: Aiyoh, maybe if we just use our insurance wisely and plan ahead, can save kway teow, yknow?\\nContributor812: Its interesting how some people find that planning ahead, like saving for education, can alleviate unexpected costs later on; perhaps a similar mindset could be applied to healthcare expenses.\\nContributor306: \\u201cPerhaps looking into more preventive care initiatives could help reduce those expenses in the long run.\\u201d\\nContributor94: Some may argue that personal responsibility in health choices can also play a role in managing these costs.\\nContributor929: I think we shouldn\\u2019t always jump to changing the system; sometimes, being more mindful about our own health can prevent those high costs in the first place.\\nContributor566: Perhaps we should also consider the importance of personal responsibility in managing our own health costs.\\nContributor306: Indeed, as Benjamin Franklin once said, An ounce of prevention is worth a pound of cure, which suggests that perhaps focusing on preventive care could alleviate some of these rising costs in the first place.\\nContributor630: Existing resources are adequate and better allocation is needed.\\nContributor449: While funding and awareness are crucial, we should also explore how integrating mental health education into schools could proactively address stigma before it becomes entrenched in these communities.But hor, maybe we also need to consider how to involve community leaders so they can help reduce the stigma more effectively ah.\\nContributor314: Eh, but how come nobody talk about personal responsibility ah? Like, why not focus on families taking charge of their own well-being instead of relying on government help?While stigma is a concern, perhaps addressing the root causes of stress in low-income families could be a more effective approach.\\nContributor222: In reality, we might need to consider whether more funding for mental health services actually addresses the root causes of the issue, as low-income families often struggle with various systemic challenges beyond just mental health.\\nContributor446: But like, isnt it also important to think about how we could involve community leaders in promoting these resources ah?\\nContributor950: Its also important to consider the role of community support in fostering a safe environment for discussions around mental health.\\nContributor222: While its admirable to push for more resources, Ive often seen that just adding services doesnt automatically change peoples views on seeking help.\\nContributor630: But isnt it more effective if we focus on empowering families through education and job opportunities, rather than just funding mental health resources?\\nContributor449: While increasing funding and awareness are crucial, we should also explore integrating mental health education into school curriculums to address stigma from a young age and foster a more supportive environment.\\nContributor950: Its also important to consider whether the available resources are culturally sensitive and truly meet the unique needs of these families.\\nContributor222: While it\\u2019s important to address mental health, perhaps we should also focus on creating community-based programs that foster connection and resilience among families, rather than just emphasizing counseling as a solution.\\nContributor404: What if we also consider incorporating community-based support systems that allow families to connect with peers experiencing similar challenges, rather than just focusing on professional counseling?\\nContributor129: Its important to consider that while funding and resources are essential, there may be more effective approaches like community-led initiatives that empower individuals to support each other in mental health matters instead of solely relying on government services.\\nContributor222: While addressing mental health is crucial, perhaps we should consider that the greatest wealth is to live content with little, as it may lead us to rethink our approaches instead of solely increasing resources.\\nContributor449: In addition to funding and awareness, we should also consider integrating mental health education into school curriculums as part of a long-term solution.\\nContributor630: While the intention behind enhancing mental health resources is commendable, one must consider that simply increasing funding does not guarantee effective utilization; we should focus on community-driven initiatives that empower families to address their challenges in culturally relevant ways.\\nContributor213: While increasing funding is crucial, its also essential to ensure that the counselors are properly trained to understand the unique challenges faced by low-income families, as not all support will be effective without this foundational understanding.\\nContributor129: Sometimes, the best way to help is to empower individuals to find their own solutions; as Gandhi said, You must be the change you wish to see in the world.\\nContributor150: Its interesting to consider the role of schools in the mental health support structure for youth. For example, some schools have implemented counseling programs that provide students with access to mental health professionals on-site., That\\u2019s a valid point, and I\\u2019ve heard that peer support groups within schools can also contribute positively.\\nContributor906: Perhaps its worth considering how societal expectations and pressures might also play a role in youth mental health, influencing their ability to seek support.\",\n          \"Contributor32: Its like when you go out for a meal and suddenly notice that your favorite dish costs more than it used to; it makes you rethink your choices and sometimes even leads you to try something new.\\nContributor161: Its interesting to note that while the focus is on affordability for families, theres also a perspective that highlights how different regions in Singapore might be experiencing these costs very differently, potentially affecting local businesses and their operations.\\nContributor934: Have we considered the potential benefits of exploring alternative income sources or investments to help families cope with these rising costs?\\nContributor751: While addressing the rising cost of living is crucial, we should also consider how enhancing financial literacy among low- and middle-income families could empower them to manage their resources more effectively.\\nContributor175: Aiyoh, but some say gotta encourage people to adapt and find ways to earn more, not just rely on help, right?\\nContributor161: Have we considered the potential benefits of encouraging more sustainable lifestyles as a solution?\\nContributor618: While support for low- and middle-income families is crucial, we also need to consider how enhancing skills and education might empower them to better navigate the cost of living.\\nContributor985: I agree that transparency in government initiatives is crucial, especially when we talk about the rising cost of living. For instance, I think detailed explanations of how subsidies for essential goods are calculated could greatly help citizens understand their financial decisions better. Just look at the recent Pioneer Generation Package; many seniors still are unsure what benefits they qualify for and how to access them. If the government provided more clear guidelines and accessible information, it would allow people to make better choices regarding their spending. ,Yet, I wonder if the government might be holding back some information, especially regarding the long-term sustainability of these measures. Just like the recent hike in utility prices, understanding the reasons behind these adjustments could help alleviate public frustration. If there were more regular updates, akin to monthly briefings about what to expect in terms of economic changes, at least we could be more prepared and informed. \\nContributor731: I think sometimes too much transparency can overwhelm people; like when I plan a trip, too much information can lead to confusion instead of clarity.\\nContributor359: While transparency is crucial, we also need to consider how swiftly implementing new initiatives can lead to confusion among citizens, especially if information isnt synchronized across different platforms.\\nContributor731: People need to take personal responsibility for their finances; relying too much on government communication can lead to complacency, as seen in past housing booms.Its important to recognize that too much transparency can lead to confusion and misinterpretations among citizens, potentially complicating rather than clarifying their understanding of initiatives.In fact, too much transparency might lead to confusion rather than clarity for citizens.\\nContributor635: While clearer communication is essential, fostering a culture of financial literacy might empower citizens even more profoundly.\\nContributor769: Aiyoh, maybe more transparency not necessarily help leh; sometimes too much information just confuse people more, you know?\\nContributor635: While I agree that transparency is important, I wonder if it might also create an overwhelming amount of information that could confuse rather than clarify for some citizens, much like how too many choices at a hawker centre can make it hard to decide on a meal.\\nContributor759: While clearer communication is important, we should also recognize that some citizens might benefit more from comprehensive support programs rather than just information; for instance, initiatives like subsidized transport or food vouchers have tangible impacts on daily expenses.\\nContributor295: Perhaps we should consider that as Benjamin Franklin said, An investment in knowledge always pays the best interest, and empowering citizens with data might be more beneficial than just transparency alone.\\nContributor731: Sometimes less information can actually lead to simpler decisions in a complex economic landscape.\\nContributor23: Transparency is like sunlight; it eradicates the shadows of doubt, but even the clearest days can have clouds.\\nContributor731: Aiyoh, I think we should just trust the government to know what they doing lah, no need to question so much.\\nContributor759: Isnt it more important to focus on practical solutions rather than transparency?\\nContributor23: Perhaps it\\u2019s also worth considering how different demographics might perceive the same information differently.\\nContributor39: Indeed, while transparency is vital, its also crucial to ensure that the information shared is actionable; for instance, if citizens are given clear insights into available subsidies or assistance programs, they could more effectively manage their budgets during times of economic strain.\\nContributor359: It\\u2019s interesting you mention transparency, but I cant help but think that some programs might benefit from less visibility to avoid overwhelming citizens with information overload.\\nContributor576: Increased grants sound beneficial, but how about the timing of these updates?, Affordable homes are crucial, but are current infrastructure capacities keeping pace?\\nContributor805: Its crucial to also consider innovative housing solutions, like mixed-use developments, that can maximize land use while providing affordable options.\\nContributor100: More focus on increasing the variety of housing types could help cater to different needs and preferences in our growing population.\\nContributor196: While its great to see support for increased grants, one might recall that the best way to predict the future is to create it, and perhaps we should look at innovative housing solutions that can efficiently increase the supply.\\n\\n\\nContributor805: Maybe we can also look at build-to-order schemes to make better use of land, since supply really tight sia.\\nContributor570: But can we really believe that increased grants will solve the fundamental issue? As Gandhi said, The best way to find yourself is to lose yourself in the service of others. We need to focus on serving the communitys needs more directly instead of just throwing money at the problem.\\nContributor790: Its crucial to consider not just the financial aspects for first-time buyers but also the potential impact of increasing supply on the long-term sustainability of our urban environment.\\nContributor570: Increased grants alone may not solve the underlying issues of affordability; we might need to reconsider the overall planning strategies instead.\\nContributor545: The increased grants are a positive step, but we should also consider diversifying housing options to better meet the diverse needs of our population.\\nContributor196: While increased grants are certainly helpful, isnt it equally crucial to consider innovative housing solutions to complement those financial aids?Its like trying to fit everyone into a cafe that has only a few tables; even with more discounts on the food, we need to expand the space if we want everyone to sit comfortably.\\n\\n\\nContributor27: But what about sustaining the quality of living in existing neighborhoods?\\nContributor570: But shouldnt we also consider the potential long-term implications of continuously increasing the grants?\\nContributor448: Isnt it interesting how managing housing supply can sometimes feel like trying to solve a Rubiks Cube\\u2014every move affects another aspect, and what looks good in one direction might complicate things elsewhere?\\nContributor196: While increased grants are helpful, I wonder if they might inadvertently lead to price inflation in the housing market, as seen in previous years when subsidies were introduced.\\n\\n\\nContributor100: Its interesting to consider that while increasing grants is a step in the right direction, it might not fully address the underlying issues of land scarcity and urban planning that contribute to the affordable housing crunch.\\nContributor196: While addressing the supply shortage is crucial, we also need to consider how to maintain the quality and sustainability of these new affordable homes being built.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"human_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"disagree\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"topic_group\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"national_day\",\n          \"budget\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"agreement\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"majority\",\n          \"unanimous\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 229,\n        \"samples\": [\n          \"cb58a6f8-2604-4b8d-a0d9-b9718cc134af\",\n          \"5419fb94-a5af-46ee-b019-ee1e52cc006e\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"group_user\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 234,\n        \"samples\": [\n          \"45419fb94-a5af-46ee-b019-ee1e52cc006e\",\n          \"20c58c1ab-6e81-406a-9609-21b23cad0f62\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Code"
      ],
      "metadata": {
        "id": "Fect01YokzNU"
      },
      "id": "Fect01YokzNU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "Ofk3Tpma36__"
      },
      "id": "Ofk3Tpma36__"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model_name = \"Jsevisal/CrossEncoder-ModernBERT-base-qnli\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "rcF55xU84A3m",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949662364,
          "user_tz": -480,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "rcF55xU84A3m",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def tokenize_dataset(df, tokenizer, device, max_length=512):\n",
        "    \"\"\"\n",
        "    Tokenizes the dataset using the specified tokenizer.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The Hugging Face dataset to be tokenized.\n",
        "        tokenizer (AutoTokenizer): Tokenizer from Hugging Face Transformers.\n",
        "        max_length (int): Maximum sequence length for padding/truncation.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: Tokenized Hugging Face dataset.\n",
        "    \"\"\"\n",
        "    # Define the tokenization function\n",
        "    def tokenize_function(examples, max_length, device):\n",
        "        inputs = tokenizer(\n",
        "            examples,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "\n",
        "        )\n",
        "        return {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    df['sentence1'] = df['stance'].apply(lambda x: str(x))\n",
        "    df['sentence2'] = df['content_concat'].apply(lambda x: str(x))\n",
        "\n",
        "    inputs = df[[\"sentence1\", 'sentence2']].values.tolist()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenized = tokenize_function(inputs, max_length, device)\n",
        "\n",
        "    # Create a new dataset from the tokenized data\n",
        "    tokenized_dataset = Dataset.from_dict(tokenized)\n",
        "    tokenized_dataset = tokenized_dataset.add_column('label', df['label'])\n",
        "    return tokenized_dataset"
      ],
      "metadata": {
        "id": "YFokYSo6kvf7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949662364,
          "user_tz": -480,
          "elapsed": 84,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "YFokYSo6kvf7",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "train_dataloader = tokenize_dataset(train_df, tokenizer, device, max_length=8192)\n",
        "test_dataloader = tokenize_dataset(test_data, tokenizer, device, max_length=8192)\n",
        "vali_dataloader = tokenize_dataset(vali_data, tokenizer, device, max_length=8192)"
      ],
      "metadata": {
        "id": "IYKrtSAx35-c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949712819,
          "user_tz": -480,
          "elapsed": 50455,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "IYKrtSAx35-c",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "2GgOMivw38v2"
      },
      "id": "2GgOMivw38v2"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from lion_pytorch import Lion\n",
        "import wandb\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score,\n",
        "    auc,\n",
        "    accuracy_score\n",
        ")\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelBinarizer, label_binarize\n",
        "from tqdm import tqdm\n",
        "\n",
        "from typing import Callable\n",
        "from torch.optim import Optimizer\n",
        "from sentence_transformers.evaluation.SentenceEvaluator import SentenceEvaluator\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torch import nn\n",
        "from tqdm.autonotebook import tqdm, trange\n",
        "import torch\n",
        "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
        "import numpy as np\n",
        "import logging\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from transformers import get_scheduler\n",
        "from collections import Counter\n",
        "from accelerate import Accelerator\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItoCNeVQA-vy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944244448,
          "user_tz": -480,
          "elapsed": 11893,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "dc806c1a-9714-4727-8c25-33a2361a0542"
      },
      "id": "ItoCNeVQA-vy",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-11 06:17:21,700] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CrossEncoderForMultiClass():\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        num_labels: int = None,\n",
        "        batch_size: int = 32,\n",
        "        output_path: str = None\n",
        "    ):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if num_labels == 1:\n",
        "          ignore_mismatched_sizes = True\n",
        "        else:\n",
        "          ignore_mismatched_sizes = False\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels = num_labels,\n",
        "            ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
        "        ).to(device)\n",
        "        self.num_labels= num_labels\n",
        "        print(\"Model loaded successfully\")\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.batch_size = batch_size\n",
        "        self.output_path = output_path\n",
        "        self.early_stopping = 0\n",
        "        # Create the output path if it doesn't exist\n",
        "        if output_path is not None:\n",
        "            os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    def create_batch(self, dataset, batch_size=None, shuffle=True):\n",
        "        if batch_size is None:\n",
        "            batch_size = batch_size\n",
        "\n",
        "        # If shuffle is True, shuffle the dataset first\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(seed=42)  # Shuffle with a seed for reproducibility\n",
        "\n",
        "        # Set the format for Hugging Face datasets (before passing to DataLoader)\n",
        "        dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        train_data: DataLoader,\n",
        "        evaluator: SentenceEvaluator = None,\n",
        "        epochs: int = 1,\n",
        "        loss_fct=None,\n",
        "        #activation_fct = nn.Identity(),\n",
        "        scheduler: str = \"WarmupLinear\",\n",
        "        warmup_steps: int = 10000,\n",
        "        optimizer_class: type[Optimizer] = torch.optim.AdamW,\n",
        "        optimizer_params: dict[str, object] = {\"lr\": 2e-5},\n",
        "        weight_decay: float = 0.01,\n",
        "        evaluation_steps: int = 0,\n",
        "        save_best_model: bool = True,\n",
        "        #max_grad_norm: float = 1,\n",
        "        #use_amp: bool = False,\n",
        "        callback: Callable[[float, int, int], None] = None,\n",
        "        show_progress_bar: bool = True,\n",
        "    ):\n",
        "\n",
        "        # Initialize variables\n",
        "        self.best_score = -9999999\n",
        "        num_train_steps = int(len(train_data) * epochs)\n",
        "\n",
        "\n",
        "        # Define the loss function\n",
        "        if loss_fct is None:\n",
        "            # Initialize counters\n",
        "            #N_pos = 0\n",
        "            #N_neg = 0\n",
        "\n",
        "            # Iterate through the dataloader\n",
        "            #for batch in train_dataloader:\n",
        "            #    targets = batch[\"label\"]\n",
        "            #    # Assuming targets are binary (0 or 1)\n",
        "            #    N_pos += sum(targets == 1)\n",
        "            #    N_neg += sum(targets == 0)\n",
        "            #pos_weight = torch.tensor([N_neg / N_pos])\n",
        "\n",
        "            loss_fct = nn.BCEWithLogitsLoss() if self.num_labels == 1 else nn.CrossEntropyLoss()\n",
        "\n",
        "        # Prepare optimizer and scheduler\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": weight_decay,\n",
        "            },\n",
        "            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "\n",
        "        optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n",
        "\n",
        "        if isinstance(scheduler, str):\n",
        "            scheduler_optimizer = SentenceTransformer._get_scheduler(\n",
        "                optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps\n",
        "            )\n",
        "        print(\"Starting model training...\")\n",
        "        training_steps = 0\n",
        "        gradient_accumulation_steps = self.batch_size\n",
        "        micro_batch_size = 1\n",
        "\n",
        "        self.early_stopping = 0\n",
        "        for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\n",
        "            train_dataloader = self.create_batch(train_data, batch_size=micro_batch_size)\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            self.model.train()\n",
        "            loss_values = []\n",
        "            accumulation_counter = 0\n",
        "\n",
        "            for batch in tqdm(train_dataloader, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\n",
        "                batch_start_time = time.time()\n",
        "\n",
        "                features = {k: torch.Tensor(v).to(self.device) for k, v in batch.items() if k != 'label'}\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                self.model.train()\n",
        "                model_predictions = self.model(**features, return_dict=True)\n",
        "\n",
        "                if self.num_labels == 1:\n",
        "                    logits = model_predictions.logits.view(-1).float()\n",
        "                    labels = labels.float()\n",
        "\n",
        "                loss_value = loss_fct(logits, labels)\n",
        "                loss_value = loss_value / gradient_accumulation_steps  # Normalize loss\n",
        "                loss_value.backward()\n",
        "                accumulation_counter += 1\n",
        "\n",
        "                if accumulation_counter % gradient_accumulation_steps == 0:\n",
        "                    # Optional: clip gradients here\n",
        "                    # torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n",
        "                    optimizer.step()\n",
        "                    scheduler_optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    accumulation_counter = 0  # reset\n",
        "                training_steps += 1\n",
        "\n",
        "                loss_values.append(loss_value.item() * gradient_accumulation_steps)  # rescale to original loss\n",
        "                batch_end_time = time.time()\n",
        "\n",
        "                if evaluator is not None and evaluation_steps > 0 and training_steps % evaluation_steps == 0 and training_steps!=0:\n",
        "                    self.model.eval()\n",
        "                    self._eval_during_training(\n",
        "                        evaluator, sum(loss_values)/len(loss_values), batch_end_time-batch_start_time, save_best_model, epoch, training_steps\n",
        "                    )\n",
        "                    self.model.zero_grad()\n",
        "                    self.model.train()\n",
        "                    if self.early_stopping >= 8:\n",
        "                        break\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        dataset,\n",
        "        batch_size: int = 32,\n",
        "        show_progress_bar = None,\n",
        "        #num_workers: int = 0,\n",
        "        #activation_fct = None,\n",
        "        #apply_softmax = False,\n",
        "        convert_to_numpy: bool = True,\n",
        "        convert_to_tensor: bool = False,\n",
        "        output_prob = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Performs predictions with the CrossEncoder on the given sentence pairs.\n",
        "\n",
        "        Args:\n",
        "            sentences (Union[List[Tuple[str, str]], Tuple[str, str]]): A list of sentence pairs [(Sent1, Sent2), (Sent3, Sent4)]\n",
        "                or one sentence pair (Sent1, Sent2).\n",
        "            batch_size (int, optional): Batch size for encoding. Defaults to 32.\n",
        "            show_progress_bar (bool, optional): Output progress bar. Defaults to None.\n",
        "            num_workers (int, optional): Number of workers for tokenization. Defaults to 0.\n",
        "            activation_fct (callable, optional): Activation function applied on the logits output of the CrossEncoder.\n",
        "                If None, nn.Sigmoid() will be used if num_labels=1, else nn.Identity. Defaults to None.\n",
        "            convert_to_numpy (bool, optional): Convert the output to a numpy matrix. Defaults to True.\n",
        "            apply_softmax (bool, optional): If there are more than 2 dimensions and apply_softmax=True,\n",
        "                applies softmax on the logits output. Defaults to False.\n",
        "            convert_to_tensor (bool, optional): Convert the output to a tensor. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            Union[List[float], np.ndarray, torch.Tensor]: Predictions for the passed sentence pairs.\n",
        "            The return type depends on the `convert_to_numpy` and `convert_to_tensor` parameters.\n",
        "            If `convert_to_tensor` is True, the output will be a torch.Tensor.\n",
        "            If `convert_to_numpy` is True, the output will be a numpy.ndarray.\n",
        "            Otherwise, the output will be a list of float values.\n",
        "\n",
        "        Examples:\n",
        "            ::\n",
        "\n",
        "                from sentence_transformers import CrossEncoder\n",
        "\n",
        "                model = CrossEncoder(\"cross-encoder/stsb-roberta-base\")\n",
        "                sentences = [[\"I love cats\", \"Cats are amazing\"], [\"I prefer dogs\", \"Dogs are loyal\"]]\n",
        "                model.predict(sentences)\n",
        "                # => array([0.6912767, 0.4303499], dtype=float32)\n",
        "        \"\"\"\n",
        "        dataset = self.create_batch(dataset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "        pred_scores = []\n",
        "        self.model.eval()\n",
        "        self.model.to(self.device)\n",
        "        with torch.no_grad():\n",
        "           for batch in tqdm(\n",
        "                dataset, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar\n",
        "            ):\n",
        "                features = {k: torch.Tensor(v).to(self.device) for k, v in batch.items() if k != 'label'}\n",
        "                model_predictions = self.model(**features, return_dict=True)\n",
        "                #logits = activation_fct(model_predictions.logits)\n",
        "\n",
        "                #if apply_softmax and len(logits[0]) > 1:\n",
        "                if output_prob:\n",
        "                    if self.num_labels == 1:\n",
        "                        logits = torch.sigmoid(model_predictions.logits)\n",
        "                    else:\n",
        "                        logits = F.softmax(model_predictions.logits, dim=1)\n",
        "                else:\n",
        "                    logits = model_predictions.logits\n",
        "                pred_scores.extend(logits)\n",
        "\n",
        "        if self.num_labels == 1:\n",
        "            pred_scores = [score[0].cpu() for score in pred_scores]\n",
        "            if convert_to_tensor:\n",
        "              pred_scores = torch.Tensor(pred_scores)\n",
        "            elif convert_to_numpy:\n",
        "              pred_scores = np.array(pred_scores)\n",
        "        else:\n",
        "            if convert_to_tensor:\n",
        "                pred_scores = torch.stack(pred_scores)\n",
        "            elif convert_to_numpy:\n",
        "                pred_scores = np.asarray([score.cpu().detach().float().numpy() for score in pred_scores])\n",
        "\n",
        "        return pred_scores\n",
        "\n",
        "\n",
        "    def _eval_during_training(self, evaluator, train_loss, time, save_best_model, epoch, steps) -> None:\n",
        "        \"\"\"Runs evaluation during the training\"\"\"\n",
        "        if evaluator is not None:\n",
        "            score = evaluator(self, train_loss, time, epoch=epoch, steps=steps)\n",
        "            #if callback is not None:\n",
        "            #    callback(score, epoch, steps)\n",
        "            saved_model = False\n",
        "            if score[\"vali_f1\"] > self.best_score:\n",
        "                self.best_score = score[\"vali_f1\"]\n",
        "                self.early_stopping=0\n",
        "                if save_best_model:\n",
        "                    self.save(self.output_path)\n",
        "                    #saved_model = True\n",
        "            else:\n",
        "                self.early_stopping+=1\n",
        "            if save_best_model:\n",
        "                if saved_model == False:\n",
        "                    self.save(self.output_path)\n",
        "            return self.best_score\n",
        "\n",
        "    def save(self, path: str, *, safe_serialization: bool = True, **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        Saves the model and tokenizer to path; identical to `save_pretrained`\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"Save model to {path}\")\n",
        "        self.model.save_pretrained(path, safe_serialization=safe_serialization, **kwargs)\n",
        "        self.tokenizer.save_pretrained(path, **kwargs)\n",
        "\n"
      ],
      "metadata": {
        "id": "5BX6l6V2k8wS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944244449,
          "user_tz": -480,
          "elapsed": 8,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "5BX6l6V2k8wS",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CrossEncoderTrainer():\n",
        "    def __init__(self,\n",
        "                  project_name,\n",
        "                  model_name,\n",
        "                  num_labels,\n",
        "                  epochs,\n",
        "                  lr,\n",
        "                  train_batch_size,\n",
        "                  eval_batch_size,\n",
        "                  warmup_steps,\n",
        "                  model_save_path,\n",
        "                  metrics_save_path,\n",
        "                  weight_decay = 0,\n",
        "                  evaluation_steps = 100,\n",
        "                  optimizer_class = \"adam\",\n",
        "                  scheduler = None,\n",
        "                  version = \"v1\"\n",
        "              ):\n",
        "        self.num_labels = num_labels\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = CrossEncoderForMultiClass(\n",
        "              model_name = model_name,\n",
        "              num_labels = num_labels,\n",
        "              batch_size = train_batch_size,\n",
        "              output_path = model_save_path\n",
        "        )\n",
        "        self.model_name = model_name\n",
        "        self.loss = nn.BCEWithLogitsLoss() if num_labels == 1 else nn.CrossEntropyLoss()\n",
        "        self.epochs = epochs\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer_params  = {\"lr\": lr}\n",
        "        self.eval_batch_size = eval_batch_size\n",
        "        self.evaluation_steps = evaluation_steps\n",
        "        self.weight_decay = weight_decay\n",
        "        if optimizer_class == \"adam\":\n",
        "              self.optimizer_class = torch.optim.AdamW\n",
        "        elif optimizer_class == \"lion\":\n",
        "              self.optimizer_class = Lion\n",
        "        self.model_save_path = model_save_path\n",
        "        # Initialize WandB\n",
        "        config = {\n",
        "              \"model_name\": model_name,\n",
        "              \"learning_rate\": lr,\n",
        "              \"epochs\": epochs,\n",
        "              \"warmup_steps\": warmup_steps,\n",
        "              \"evaluation_steps\": evaluation_steps,\n",
        "              \"weight_decay\": weight_decay,\n",
        "              \"optimizer_class\": optimizer_class,\n",
        "              \"batch_size\": train_batch_size,\n",
        "        }\n",
        "        run_name = self.model_name +  datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        metrics_save_path = (metrics_save_path + \"/\") if (metrics_save_path[-1] != \"/\") else metrics_save_path\n",
        "        self.json_file = metrics_save_path + (\"classification_evaluation\" + (\"_\" + model_name if model_name else \"\") + \"_\" + version + \"_results.json\").replace(\"/\", \"_\")\n",
        "\n",
        "        wandb.init(\n",
        "              project=project_name,\n",
        "              name= run_name[:255],\n",
        "              config= config\n",
        "        )\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train(self,\n",
        "              train_dataloader,\n",
        "              eval_dataloader,\n",
        "              vali_data\n",
        "                        ):\n",
        "        evaluator = CrossEncoderEvaluator(\n",
        "              eval_dataloader,\n",
        "              vali_data,\n",
        "              name = self.model_name,\n",
        "              batch_size = self.eval_batch_size,\n",
        "              show_progress_bar= True,\n",
        "              metrics_save_path=self.json_file,\n",
        "              loss_fn= self.loss\n",
        "        )\n",
        "        print(\"Training model\")\n",
        "        self.model.fit(\n",
        "              train_data=train_dataloader,\n",
        "              evaluator=evaluator,\n",
        "              evaluation_steps =self.evaluation_steps,\n",
        "              optimizer_params= self.optimizer_params,\n",
        "              loss_fct  = self.loss,\n",
        "              epochs=self.epochs,\n",
        "              warmup_steps=self.warmup_steps,\n",
        "              save_best_model = True,\n",
        "              #callback = self.callback,\n",
        "              show_progress_bar = True,\n",
        "              #scheduler = self.scheduler\n",
        "        )"
      ],
      "metadata": {
        "id": "TnbiW6YAl5r0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944244449,
          "user_tz": -480,
          "elapsed": 7,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "TnbiW6YAl5r0",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from transformers import get_scheduler, Trainer, TrainingArguments\n",
        "import torch\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CrossEncoderForMultiClass():\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        num_labels: int = None,\n",
        "        batch_size: int = 32,\n",
        "        output_path: str = None\n",
        "    ):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if num_labels == 1:\n",
        "            ignore_mismatched_sizes = True\n",
        "        else:\n",
        "            ignore_mismatched_sizes = False\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels,\n",
        "            ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.batch_size = batch_size\n",
        "        self.output_path = output_path\n",
        "\n",
        "        if output_path is not None:\n",
        "            os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    def create_batch(self, dataset, batch_size=None, shuffle=True):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(seed=42)\n",
        "        dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "        return dataloader\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        train_data: DataLoader,\n",
        "        evaluator: SentenceEvaluator = None,\n",
        "        epochs: int = 1,\n",
        "        loss_fct=None,\n",
        "        scheduler_type: str = \"linear\",  # Added parameter for scheduler type\n",
        "        warmup_steps: int = 10000,\n",
        "        optimizer_class: type[Optimizer] = torch.optim.AdamW,\n",
        "        optimizer_params: dict[str, object] = {\"lr\": 2e-5},\n",
        "        weight_decay: float = 0.01,\n",
        "        evaluation_steps: int = 0,\n",
        "        save_best_model: bool = True,\n",
        "        max_grad_norm: float = 1,\n",
        "        callback: Callable[[float, int, int], None] = None,\n",
        "        show_progress_bar: bool = True,\n",
        "    ):\n",
        "        # Initialize variables\n",
        "        self.best_score = -9999999\n",
        "        num_train_steps = int(len(train_data) * epochs)\n",
        "\n",
        "        # Define loss function\n",
        "        if loss_fct is None:\n",
        "            loss_fct = nn.CrossEntropyLoss() if self.num_labels > 1 else nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Prepare optimizer and scheduler\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": weight_decay,\n",
        "            },\n",
        "            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n",
        "\n",
        "        # Initialize scheduler using Hugging Face's get_scheduler\n",
        "        scheduler = get_scheduler(\n",
        "            name=scheduler_type,  # Scheduler type: \"linear\", \"cosine\", etc.\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=num_train_steps\n",
        "        )\n",
        "\n",
        "        # Define training arguments\n",
        "        training_steps = 0\n",
        "        gradient_accumulation_steps = self.batch_size  # effective batch size\n",
        "        micro_batch_size = 2\n",
        "\n",
        "        for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\n",
        "            train_dataloader = self.create_batch(train_data, batch_size=micro_batch_size)\n",
        "            self.model.zero_grad()\n",
        "            self.model.train()\n",
        "            loss_values = []\n",
        "            accumulation_counter = 0\n",
        "\n",
        "            for batch in tqdm(train_dataloader, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\n",
        "                batch_start_time = time.time()\n",
        "\n",
        "                features = {k: torch.Tensor(v).to(self.device) for k, v in batch.items() if k != 'label'}\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                model_predictions = self.model(**features, return_dict=True)\n",
        "\n",
        "                if self.num_labels == 1:\n",
        "                    logits = model_predictions.logits.view(-1).float()\n",
        "                    labels = labels.float()\n",
        "\n",
        "                loss_value = loss_fct(logits, labels)\n",
        "                loss_value = loss_value / gradient_accumulation_steps  # Normalize loss\n",
        "                loss_value.backward()\n",
        "                accumulation_counter += 1\n",
        "\n",
        "                if accumulation_counter % gradient_accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()  # Step the scheduler\n",
        "                    optimizer.zero_grad()\n",
        "                    accumulation_counter = 0  # reset\n",
        "                    training_steps += 1\n",
        "\n",
        "                loss_values.append(loss_value.item() * gradient_accumulation_steps)  # rescale to original loss\n",
        "                batch_end_time = time.time()\n",
        "\n",
        "                if evaluator is not None and evaluation_steps > 0 and training_steps % evaluation_steps == 0 and training_steps != 0:\n",
        "                    self.model.eval()\n",
        "                    self._eval_during_training(\n",
        "                        evaluator, sum(loss_values) / len(loss_values), batch_end_time - batch_start_time, save_best_model, epoch, training_steps\n",
        "                    )\n",
        "\n",
        "                    self.model.zero_grad()\n",
        "                    self.model.train()\n",
        "\n",
        "    def _eval_during_training(self, evaluator, train_loss, time, save_best_model, epoch, steps) -> None:\n",
        "        \"\"\"Runs evaluation during the training\"\"\"\n",
        "        if evaluator is not None:\n",
        "            score = evaluator(self, train_loss, time, epoch=epoch, steps=steps)\n",
        "            saved_model = False\n",
        "            if score[\"vali_f1\"] > self.best_score:\n",
        "                self.best_score = score[\"vali_f1\"]\n",
        "                if save_best_model:\n",
        "                    self.save(self.output_path)\n",
        "\n",
        "            if save_best_model:\n",
        "                if not saved_model:\n",
        "                    self.save(self.output_path)\n",
        "\n",
        "    def save(self, path: str, *, safe_serialization: bool = True, **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        Saves the model and tokenizer to path.\n",
        "        \"\"\"\n",
        "        print(f\"Saving model to {path}\")\n",
        "        self.model.save_pretrained(path, safe_serialization=safe_serialization, **kwargs)\n",
        "        self.tokenizer.save_pretrained(path, **kwargs)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "1-zhFTYNUSRy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944244449,
          "user_tz": -480,
          "elapsed": 6,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "daa8e616-af08-4591-9067-f3277fd6b792"
      },
      "id": "1-zhFTYNUSRy",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from transformers import get_scheduler, Trainer, TrainingArguments\\nimport torch\\nimport time\\nfrom tqdm import tqdm\\n\\nclass CrossEncoderForMultiClass():\\n    def __init__(\\n        self,\\n        model_name: str,\\n        num_labels: int = None,\\n        batch_size: int = 32,\\n        output_path: str = None\\n    ):\\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\\n        if num_labels == 1:\\n            ignore_mismatched_sizes = True\\n        else:\\n            ignore_mismatched_sizes = False\\n        self.model = AutoModelForSequenceClassification.from_pretrained(\\n            model_name,\\n            num_labels=num_labels,\\n            ignore_mismatched_sizes=ignore_mismatched_sizes,\\n        ).to(self.device)\\n\\n        self.num_labels = num_labels\\n        self.batch_size = batch_size\\n        self.output_path = output_path\\n\\n        if output_path is not None:\\n            os.makedirs(output_path, exist_ok=True)\\n\\n    def create_batch(self, dataset, batch_size=None, shuffle=True):\\n        if batch_size is None:\\n            batch_size = self.batch_size\\n        if shuffle:\\n            dataset = dataset.shuffle(seed=42)\\n        dataset.set_format(type=\\'torch\\', columns=[\\'input_ids\\', \\'attention_mask\\', \\'label\\'])\\n        dataloader = DataLoader(dataset, batch_size=batch_size)\\n        return dataloader\\n\\n    def fit(\\n        self,\\n        train_data: DataLoader,\\n        evaluator: SentenceEvaluator = None,\\n        epochs: int = 1,\\n        loss_fct=None,\\n        scheduler_type: str = \"linear\",  # Added parameter for scheduler type\\n        warmup_steps: int = 10000,\\n        optimizer_class: type[Optimizer] = torch.optim.AdamW,\\n        optimizer_params: dict[str, object] = {\"lr\": 2e-5},\\n        weight_decay: float = 0.01,\\n        evaluation_steps: int = 0,\\n        save_best_model: bool = True,\\n        max_grad_norm: float = 1,\\n        callback: Callable[[float, int, int], None] = None,\\n        show_progress_bar: bool = True,\\n    ):\\n        # Initialize variables\\n        self.best_score = -9999999\\n        num_train_steps = int(len(train_data) * epochs)\\n\\n        # Define loss function\\n        if loss_fct is None:\\n            loss_fct = nn.CrossEntropyLoss() if self.num_labels > 1 else nn.BCEWithLogitsLoss()\\n\\n        # Prepare optimizer and scheduler\\n        param_optimizer = list(self.model.named_parameters())\\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\\n        optimizer_grouped_parameters = [\\n            {\\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\\n                \"weight_decay\": weight_decay,\\n            },\\n            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\\n        ]\\n        optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\\n\\n        # Initialize scheduler using Hugging Face\\'s get_scheduler\\n        scheduler = get_scheduler(\\n            name=scheduler_type,  # Scheduler type: \"linear\", \"cosine\", etc.\\n            optimizer=optimizer,\\n            num_warmup_steps=warmup_steps,\\n            num_training_steps=num_train_steps\\n        )\\n\\n        # Define training arguments\\n        training_steps = 0\\n        gradient_accumulation_steps = self.batch_size  # effective batch size\\n        micro_batch_size = 2\\n\\n        for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\\n            train_dataloader = self.create_batch(train_data, batch_size=micro_batch_size)\\n            self.model.zero_grad()\\n            self.model.train()\\n            loss_values = []\\n            accumulation_counter = 0\\n\\n            for batch in tqdm(train_dataloader, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\\n                batch_start_time = time.time()\\n\\n                features = {k: torch.Tensor(v).to(self.device) for k, v in batch.items() if k != \\'label\\'}\\n                labels = batch[\\'label\\'].to(self.device)\\n\\n                model_predictions = self.model(**features, return_dict=True)\\n\\n                if self.num_labels == 1:\\n                    logits = model_predictions.logits.view(-1).float()\\n                    labels = labels.float()\\n\\n                loss_value = loss_fct(logits, labels)\\n                loss_value = loss_value / gradient_accumulation_steps  # Normalize loss\\n                loss_value.backward()\\n                accumulation_counter += 1\\n\\n                if accumulation_counter % gradient_accumulation_steps == 0:\\n                    optimizer.step()\\n                    scheduler.step()  # Step the scheduler\\n                    optimizer.zero_grad()\\n                    accumulation_counter = 0  # reset\\n                    training_steps += 1\\n\\n                loss_values.append(loss_value.item() * gradient_accumulation_steps)  # rescale to original loss\\n                batch_end_time = time.time()\\n\\n                if evaluator is not None and evaluation_steps > 0 and training_steps % evaluation_steps == 0 and training_steps != 0:\\n                    self.model.eval()\\n                    self._eval_during_training(\\n                        evaluator, sum(loss_values) / len(loss_values), batch_end_time - batch_start_time, save_best_model, epoch, training_steps\\n                    )\\n\\n                    self.model.zero_grad()\\n                    self.model.train()\\n\\n    def _eval_during_training(self, evaluator, train_loss, time, save_best_model, epoch, steps) -> None:\\n        \"\"\"Runs evaluation during the training\"\"\"\\n        if evaluator is not None:\\n            score = evaluator(self, train_loss, time, epoch=epoch, steps=steps)\\n            saved_model = False\\n            if score[\"vali_f1\"] > self.best_score:\\n                self.best_score = score[\"vali_f1\"]\\n                if save_best_model:\\n                    self.save(self.output_path)\\n\\n            if save_best_model:\\n                if not saved_model:\\n                    self.save(self.output_path)\\n\\n    def save(self, path: str, *, safe_serialization: bool = True, **kwargs) -> None:\\n        \"\"\"\\n        Saves the model and tokenizer to path.\\n        \"\"\"\\n        print(f\"Saving model to {path}\")\\n        self.model.save_pretrained(path, safe_serialization=safe_serialization, **kwargs)\\n        self.tokenizer.save_pretrained(path, **kwargs)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm, trange\n",
        "from typing import Callable\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CrossEncoderForMultiClass:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        num_labels: int = None,\n",
        "        batch_size: int = 32,\n",
        "        output_path: str = None\n",
        "    ):\n",
        "        # Initialize the accelerator\n",
        "        self.accelerator = Accelerator()\n",
        "\n",
        "        # Initialize the tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        ignore_mismatched_sizes = num_labels == 1\n",
        "\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels,\n",
        "            ignore_mismatched_sizes=ignore_mismatched_sizes\n",
        "        )\n",
        "        self.num_labels = num_labels\n",
        "        self.batch_size = batch_size\n",
        "        self.output_path = output_path\n",
        "\n",
        "        if output_path:\n",
        "            os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        self.accelerator.print(\"Model loaded successfully\")\n",
        "\n",
        "    def create_batch(self, dataset, batch_size=None, shuffle=True):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "        dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "        return DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        train_data,\n",
        "        evaluator: Callable = None,\n",
        "        epochs: int = 1,\n",
        "        loss_fct=None,\n",
        "        scheduler: str = \"WarmupLinear\",\n",
        "        warmup_steps: int = 10000,\n",
        "        optimizer_class: type = torch.optim.AdamW,\n",
        "        optimizer_params: dict = {\"lr\": 2e-5},\n",
        "        weight_decay: float = 0.01,\n",
        "        evaluation_steps: int = 0,\n",
        "        save_best_model: bool = True,\n",
        "        max_grad_norm: float = 1,\n",
        "        callback: Callable[[float, int, int], None] = None,\n",
        "        show_progress_bar: bool = True,\n",
        "    ):\n",
        "        # Define training arguments and setup scheduler\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.output_path,\n",
        "            num_train_epochs=epochs,\n",
        "            per_device_train_batch_size=self.batch_size,\n",
        "            gradient_accumulation_steps=1,\n",
        "            eval_strategy=\"steps\",\n",
        "            save_steps=evaluation_steps,\n",
        "            logging_dir='./logs',\n",
        "            weight_decay=weight_decay,\n",
        "            learning_rate=2e-5,\n",
        "            lr_scheduler_type=scheduler,\n",
        "            warmup_steps=warmup_steps,\n",
        "            logging_steps=500,\n",
        "            save_total_limit=1,\n",
        "            max_grad_norm=max_grad_norm\n",
        "        )\n",
        "\n",
        "        # Create a Trainer with custom callbacks and features\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_data,\n",
        "            eval_dataset=None,  # Add validation dataset if available\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=evaluator,\n",
        "            callbacks=[callback] if callback else None,\n",
        "            optimizers=(optimizer_class, optimizer_params),\n",
        "            data_collator=None,\n",
        "            #lr_scheduler_type=scheduler,\n",
        "            # Pass the custom loss function and other parameters\n",
        "            compute_loss_func=loss_fct if loss_fct else nn.CrossEntropyLoss(),\n",
        "            max_grad_norm=max_grad_norm\n",
        "        )\n",
        "\n",
        "        # Wrap the model with accelerator for multi-device (e.g., multi-GPU)\n",
        "        trainer.model, trainer.optimizer, trainer.data_collator = self.accelerator.prepare(\n",
        "            trainer.model, trainer.optimizer, trainer.data_collator\n",
        "        )\n",
        "\n",
        "        self.accelerator.print(\"Starting training...\")\n",
        "        trainer.train()\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        dataset,\n",
        "        batch_size: int = 32,\n",
        "        show_progress_bar=None,\n",
        "        convert_to_numpy: bool = True,\n",
        "        convert_to_tensor: bool = False,\n",
        "        output_prob: bool = True\n",
        "    ):\n",
        "        # Use the accelerator to prepare the model for inference\n",
        "        dataloader = self.create_batch(dataset, batch_size=batch_size, shuffle=False)\n",
        "        model = self.accelerator.prepare(self.model)\n",
        "\n",
        "        model.eval()\n",
        "        pred_scores = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Iteration\", disable=not show_progress_bar):\n",
        "                outputs = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    return_dict=True\n",
        "                )\n",
        "                logits = outputs.logits\n",
        "\n",
        "                if output_prob:\n",
        "                    logits = torch.sigmoid(logits) if self.num_labels == 1 else F.softmax(logits, dim=1)\n",
        "\n",
        "                pred_scores.extend(self.accelerator.gather(logits).cpu())\n",
        "\n",
        "        pred_scores = torch.stack(pred_scores)\n",
        "\n",
        "        if convert_to_tensor:\n",
        "            return pred_scores\n",
        "        if convert_to_numpy:\n",
        "            return pred_scores.numpy()\n",
        "        return pred_scores.tolist()\n",
        "\n",
        "    def save(self, path: str, *, safe_serialization: bool = True, **kwargs) -> None:\n",
        "        self.accelerator.print(f\"Save model to {path}\")\n",
        "        self.accelerator.wait_for_everyone()\n",
        "        if self.accelerator.is_main_process:\n",
        "            self.model.save_pretrained(path, safe_serialization=safe_serialization, **kwargs)\n",
        "            self.tokenizer.save_pretrained(path, **kwargs)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "YrvCWvDFHMB8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944244449,
          "user_tz": -480,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fa5df377-da1c-4765-e60c-89ec2da1ca73"
      },
      "id": "YrvCWvDFHMB8",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\\nfrom accelerate import Accelerator\\nimport torch\\nimport os\\nimport time\\nfrom tqdm import tqdm, trange\\nfrom typing import Callable\\nfrom torch import nn\\nimport torch.nn.functional as F\\nfrom torch.utils.data import DataLoader\\n\\nclass CrossEncoderForMultiClass:\\n    def __init__(\\n        self,\\n        model_name: str,\\n        num_labels: int = None,\\n        batch_size: int = 32,\\n        output_path: str = None\\n    ):\\n        # Initialize the accelerator\\n        self.accelerator = Accelerator()\\n\\n        # Initialize the tokenizer and model\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\\n        ignore_mismatched_sizes = num_labels == 1\\n\\n        self.model = AutoModelForSequenceClassification.from_pretrained(\\n            model_name,\\n            num_labels=num_labels,\\n            ignore_mismatched_sizes=ignore_mismatched_sizes\\n        )\\n        self.num_labels = num_labels\\n        self.batch_size = batch_size\\n        self.output_path = output_path\\n\\n        if output_path:\\n            os.makedirs(output_path, exist_ok=True)\\n\\n        self.accelerator.print(\"Model loaded successfully\")\\n\\n    def create_batch(self, dataset, batch_size=None, shuffle=True):\\n        if batch_size is None:\\n            batch_size = self.batch_size\\n\\n        if shuffle:\\n            dataset = dataset.shuffle(seed=42)\\n\\n        dataset.set_format(type=\\'torch\\', columns=[\\'input_ids\\', \\'attention_mask\\', \\'label\\'])\\n        return DataLoader(dataset, batch_size=batch_size)\\n\\n    def fit(\\n        self,\\n        train_data,\\n        evaluator: Callable = None,\\n        epochs: int = 1,\\n        loss_fct=None,\\n        scheduler: str = \"WarmupLinear\",\\n        warmup_steps: int = 10000,\\n        optimizer_class: type = torch.optim.AdamW,\\n        optimizer_params: dict = {\"lr\": 2e-5},\\n        weight_decay: float = 0.01,\\n        evaluation_steps: int = 0,\\n        save_best_model: bool = True,\\n        max_grad_norm: float = 1,\\n        callback: Callable[[float, int, int], None] = None,\\n        show_progress_bar: bool = True,\\n    ):\\n        # Define training arguments and setup scheduler\\n        training_args = TrainingArguments(\\n            output_dir=self.output_path,\\n            num_train_epochs=epochs,\\n            per_device_train_batch_size=self.batch_size,\\n            gradient_accumulation_steps=1,\\n            eval_strategy=\"steps\",\\n            save_steps=evaluation_steps,\\n            logging_dir=\\'./logs\\',\\n            weight_decay=weight_decay,\\n            learning_rate=2e-5,\\n            lr_scheduler_type=scheduler,\\n            warmup_steps=warmup_steps,\\n            logging_steps=500,\\n            save_total_limit=1,\\n            max_grad_norm=max_grad_norm\\n        )\\n\\n        # Create a Trainer with custom callbacks and features\\n        trainer = Trainer(\\n            model=self.model,\\n            args=training_args,\\n            train_dataset=train_data,\\n            eval_dataset=None,  # Add validation dataset if available\\n            tokenizer=self.tokenizer,\\n            compute_metrics=evaluator,\\n            callbacks=[callback] if callback else None,\\n            optimizers=(optimizer_class, optimizer_params),\\n            data_collator=None,\\n            #lr_scheduler_type=scheduler,\\n            # Pass the custom loss function and other parameters\\n            compute_loss_func=loss_fct if loss_fct else nn.CrossEntropyLoss(),\\n            max_grad_norm=max_grad_norm\\n        )\\n\\n        # Wrap the model with accelerator for multi-device (e.g., multi-GPU)\\n        trainer.model, trainer.optimizer, trainer.data_collator = self.accelerator.prepare(\\n            trainer.model, trainer.optimizer, trainer.data_collator\\n        )\\n\\n        self.accelerator.print(\"Starting training...\")\\n        trainer.train()\\n\\n    def predict(\\n        self,\\n        dataset,\\n        batch_size: int = 32,\\n        show_progress_bar=None,\\n        convert_to_numpy: bool = True,\\n        convert_to_tensor: bool = False,\\n        output_prob: bool = True\\n    ):\\n        # Use the accelerator to prepare the model for inference\\n        dataloader = self.create_batch(dataset, batch_size=batch_size, shuffle=False)\\n        model = self.accelerator.prepare(self.model)\\n\\n        model.eval()\\n        pred_scores = []\\n\\n        with torch.no_grad():\\n            for batch in tqdm(dataloader, desc=\"Iteration\", disable=not show_progress_bar):\\n                outputs = model(\\n                    input_ids=batch[\"input_ids\"],\\n                    attention_mask=batch[\"attention_mask\"],\\n                    return_dict=True\\n                )\\n                logits = outputs.logits\\n\\n                if output_prob:\\n                    logits = torch.sigmoid(logits) if self.num_labels == 1 else F.softmax(logits, dim=1)\\n\\n                pred_scores.extend(self.accelerator.gather(logits).cpu())\\n\\n        pred_scores = torch.stack(pred_scores)\\n\\n        if convert_to_tensor:\\n            return pred_scores\\n        if convert_to_numpy:\\n            return pred_scores.numpy()\\n        return pred_scores.tolist()\\n\\n    def save(self, path: str, *, safe_serialization: bool = True, **kwargs) -> None:\\n        self.accelerator.print(f\"Save model to {path}\")\\n        self.accelerator.wait_for_everyone()\\n        if self.accelerator.is_main_process:\\n            self.model.save_pretrained(path, safe_serialization=safe_serialization, **kwargs)\\n            self.tokenizer.save_pretrained(path, **kwargs)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import csv\n",
        "import logging\n",
        "import os\n",
        "from contextlib import nullcontext\n",
        "from typing import TYPE_CHECKING\n",
        "import json\n",
        "import wandb\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sentence_transformers.evaluation.SentenceEvaluator import SentenceEvaluator\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class CrossEncoderEvaluator(SentenceEvaluator):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        val_dataloader,\n",
        "        vali_data,\n",
        "        metrics_save_path,\n",
        "        name: str = \"\",\n",
        "        batch_size: int = 32,\n",
        "        show_progress_bar: bool = False,\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        #write_csv: bool = True,\n",
        "        #truncate_dim: int | None = None,\n",
        "    ):\n",
        "        self.val_dataloader= val_dataloader\n",
        "        self.vali_data = vali_data\n",
        "        self.name = name\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.show_progress_bar = show_progress_bar\n",
        "        self.loss_fn = loss_fn\n",
        "        self.json_file = metrics_save_path\n",
        "\n",
        "    def append_to_json(self, file_path, new_dict, overwrite = False):\n",
        "        # Check if the file exists\n",
        "        if overwrite == False:\n",
        "            if os.path.exists(file_path):\n",
        "                # Open and load existing JSON data\n",
        "                with open(file_path, 'r') as file:\n",
        "                    data = json.load(file)\n",
        "\n",
        "                # Ensure data is a list\n",
        "                if not isinstance(data, list):\n",
        "                    raise ValueError(\"JSON file must contain a list of dictionaries.\")\n",
        "            else:\n",
        "                # If file doesn't exist, start with an empty list\n",
        "                data = []\n",
        "\n",
        "            # Append the new dictionary\n",
        "            data.append(new_dict)\n",
        "        else:\n",
        "            data = new_dict\n",
        "\n",
        "        # Ensure the directory exists\n",
        "        folder_path = os.path.dirname(file_path)\n",
        "        if folder_path:  # Check if there is a folder path in file_path\n",
        "            os.makedirs(folder_path, exist_ok=True)\n",
        "        # Save the updated data back to the file\n",
        "        with open(file_path, 'w') as file:\n",
        "            json.dump(data, file)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        model: SentenceTransformer,\n",
        "        train_loss,\n",
        "        time,\n",
        "        epoch: int = -1,\n",
        "        steps: int = -1\n",
        "    ) -> dict[str, float]:\n",
        "\n",
        "        if epoch != -1:\n",
        "            if steps == -1:\n",
        "                out_txt = f\" after epoch {epoch}\"\n",
        "            else:\n",
        "                out_txt = f\" in epoch {epoch} after {steps} steps\"\n",
        "        else:\n",
        "            out_txt = \"\"\n",
        "        #if self.truncate_dim is not None:\n",
        "        #    out_txt += f\" (truncated to {self.truncate_dim})\"\n",
        "\n",
        "        scores = self.compute_metrices(model)\n",
        "        scores[\"train_loss\"] = train_loss\n",
        "\n",
        "        print(f\"Epoch: {epoch}, step: {steps}, time: {time}, scores: \", scores)\n",
        "        scores[\"epoch\"] = epoch\n",
        "        scores[\"steps\"] = steps\n",
        "\n",
        "        scores[\"train_time\"] = time\n",
        "\n",
        "        wandb.log(scores)\n",
        "        self.append_to_json(self.json_file, scores)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def compute_metrices(self, model):\n",
        "        best_f1 = -0.01\n",
        "        best_threshold = 0.50\n",
        "        scores = model.predict(self.val_dataloader, output_prob = False, batch_size = self.batch_size)\n",
        "        for threshold in np.arange(0.01, 1.0, 0.01):\n",
        "            pred = (np.array(scores) > threshold).astype(int)\n",
        "            self.vali_data[\"pred\"] = pred\n",
        "            #grouped_scores = self.vali_data.groupby([\"stance\", \"contributor_id\", \"chat_group_id\"]).agg({\"pred\": \"max\", \"label\": \"max\"}).reset_index()\n",
        "            #grouped_scores[\"pred\"]= (np.array(grouped_scores[\"pred\"]) > 1).astype(int)\n",
        "            f1 = f1_score(self.vali_data[\"label\"], self.vali_data[\"pred\"])\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        pred = (np.array(scores) > best_threshold).astype(int)\n",
        "        self.vali_data[\"pred\"] = pred\n",
        "        #grouped_scores = self.vali_data.groupby([\"stance\", \"contributor_id\", \"chat_group_id\"]).agg({\"pred\": \"max\", \"label\": \"max\"}).reset_index()\n",
        "        #grouped_scores[\"pred\"]= (np.array(grouped_scores[\"pred\"]) > 1).astype(int)\n",
        "        y_true = self.vali_data[\"label\"].values\n",
        "        y_pred = self.vali_data[\"pred\"].values\n",
        "\n",
        "        output_scores = {}\n",
        "        #display(y_true, y_scores, y_pred, scores)\n",
        "\n",
        "        # Calculate metrics with 'weighted' average to account for class imbalance\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=1)\n",
        "        precision = precision_score(y_true, y_pred, zero_division=1)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=1)\n",
        "\n",
        "        output_scores = {\n",
        "            \"vali_f1\": f1,\n",
        "            #\"vali_auc\": auc_score,\n",
        "            \"vali_precision\": precision,\n",
        "            \"vali_recall\": recall,\n",
        "            #\"vali_loss\": loss.item(),\n",
        "            \"weighted_precision\":  precision_score(y_true, y_pred, zero_division=1, average = \"weighted\"),\n",
        "            \"weighted_recall\":  recall_score(y_true, y_pred, zero_division=1, average = \"weighted\"),\n",
        "            \"weighted_f1\":  f1_score(y_true, y_pred, zero_division=1, average = \"weighted\"),\n",
        "            \"macro_precision\":  precision_score(y_true, y_pred, zero_division=1, average = \"macro\"),\n",
        "            \"macro_recall\":  recall_score(y_true, y_pred, zero_division=1, average = \"macro\"),\n",
        "            \"macro_f1\":  f1_score(y_true, y_pred, zero_division=1, average = \"macro\")\n",
        "        }\n",
        "\n",
        "        return output_scores"
      ],
      "metadata": {
        "id": "VCGlMApGdzlW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944244449,
          "user_tz": -480,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "VCGlMApGdzlW",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "print(torch.cuda.is_available())  # Should return True\n",
        "print(torch.cuda.current_device())  # Check current device ID\n",
        "print(torch.cuda.get_device_name(0))  # Should return your GPU model"
      ],
      "metadata": {
        "id": "UMAvRMM63p-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944244523,
          "user_tz": -480,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d30b802c-1daa-4095-ae43-05e8db30efe7"
      },
      "id": "UMAvRMM63p-8",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "0\n",
            "NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True"
      ],
      "metadata": {
        "id": "iyLQKR6YKsGc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944244523,
          "user_tz": -480,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "iyLQKR6YKsGc",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels = 1,\n",
        "            ignore_mismatched_sizes=True)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape, param.requires_grad)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Q9_PS9TP3sDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944247873,
          "user_tz": -480,
          "elapsed": 3351,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e0ef8e71-7e4c-4a06-9347-e2725ff98ac2"
      },
      "id": "Q9_PS9TP3sDF",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at Jsevisal/CrossEncoder-ModernBERT-base-qnli and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.embeddings.tok_embeddings.weight torch.Size([50368, 768]) True\n",
            "model.embeddings.norm.weight torch.Size([768]) True\n",
            "model.layers.0.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.0.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.0.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.0.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.0.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.1.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.1.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.1.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.1.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.1.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.1.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.2.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.2.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.2.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.2.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.2.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.2.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.3.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.3.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.3.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.3.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.3.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.3.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.4.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.4.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.4.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.4.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.4.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.4.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.5.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.5.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.5.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.5.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.5.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.5.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.6.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.6.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.6.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.6.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.6.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.6.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.7.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.7.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.7.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.7.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.7.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.7.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.8.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.8.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.8.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.8.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.8.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.8.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.9.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.9.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.9.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.9.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.9.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.9.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.10.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.10.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.10.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.10.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.10.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.10.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.11.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.11.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.11.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.11.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.11.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.11.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.12.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.12.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.12.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.12.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.12.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.12.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.13.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.13.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.13.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.13.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.13.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.13.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.14.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.14.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.14.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.14.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.14.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.14.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.15.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.15.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.15.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.15.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.15.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.15.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.16.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.16.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.16.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.16.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.16.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.16.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.17.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.17.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.17.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.17.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.17.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.17.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.18.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.18.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.18.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.18.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.18.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.18.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.19.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.19.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.19.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.19.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.19.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.19.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.20.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.20.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.20.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.20.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.20.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.20.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.21.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.21.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.21.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.21.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.21.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.21.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.final_norm.weight torch.Size([768]) True\n",
            "head.dense.weight torch.Size([768, 768]) True\n",
            "head.norm.weight torch.Size([768]) True\n",
            "classifier.weight torch.Size([1, 768]) True\n",
            "classifier.bias torch.Size([1]) True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModernBertForSequenceClassification(\n",
              "  (model): ModernBertModel(\n",
              "    (embeddings): ModernBertEmbeddings(\n",
              "      (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
              "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): ModernBertEncoderLayer(\n",
              "        (attn_norm): Identity()\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "      (1-21): 21 x ModernBertEncoderLayer(\n",
              "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (head): ModernBertPredictionHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
              "    (act): GELUActivation()\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (drop): Dropout(p=0.0, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Assuming `train`, `test`, and `vali` are already defined as Dataset objects\n",
        "data = DatasetDict({\n",
        "    \"train\": train_dataloader,\n",
        "    \"test\": test_dataloader,\n",
        "    \"validation\": vali_dataloader\n",
        "})\n",
        "\n",
        "# Access the train dataset\n",
        "print(data[\"train\"])\n",
        "\n",
        "# View the split dataset sizes\n",
        "print(\"Train size:\", len(data['train']))\n",
        "print(\"Validation size:\", len(data['validation']))\n",
        "print(\"Test size:\", len(data['test']))"
      ],
      "metadata": {
        "id": "rHTMjeCGmTGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746944247873,
          "user_tz": -480,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ad6ede0a-4c4d-4e32-ab9e-1269bcd1e2c6"
      },
      "id": "rHTMjeCGmTGc",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'label'],\n",
            "    num_rows: 7348\n",
            "})\n",
            "Train size: 7348\n",
            "Validation size: 674\n",
            "Test size: 1349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "version = \"v6\"\n",
        "folder = f\"/content/classification/{model_name}_{version}\"\n",
        "model_save_path = f\"{folder}/model/\"\n",
        "metrics_save_path = f\"{folder}/metrics/\"\n",
        "num_labels = 1\n",
        "epochs = 1\n",
        "lr = 0.0000001\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 8\n",
        "warmup_steps = 1000\n",
        "weight_decay = 0\n",
        "evaluation_steps = 100 # How many steps before you eval\n",
        "optimizer_class = \"adam\"\n",
        "scheduler = 'warmupcosinewithhardrestarts' # constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n",
        "\n",
        "\n",
        "trainer = CrossEncoderTrainer(\n",
        "    project_name = \"colab_test\",\n",
        "    model_name = model_name,\n",
        "    num_labels = num_labels,\n",
        "    epochs = epochs,\n",
        "    lr=lr,\n",
        "    train_batch_size = train_batch_size,\n",
        "    eval_batch_size = eval_batch_size,\n",
        "    warmup_steps = warmup_steps,\n",
        "    weight_decay = weight_decay,\n",
        "    evaluation_steps = evaluation_steps,\n",
        "    optimizer_class = optimizer_class,\n",
        "    model_save_path = model_save_path,\n",
        "    metrics_save_path=metrics_save_path,\n",
        "    scheduler = scheduler,\n",
        "    version = version\n",
        ")\n",
        "\n",
        "trainer.train(\n",
        "    data[\"train\"],\n",
        "    data[\"validation\"],\n",
        "    vali_data\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y96IIJHRmVDT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fd1ebaa7f20840a398836b89dcbecb3f",
            "631eefb622294c0592e377caa809ea33",
            "1fe0de4836a44cf5a3b4f6ed2aeb3669",
            "1cb2d8cfbd6a403ba2e768fd4e2c3bc8",
            "134e9298ecca47c4a9a66cc676a63de8",
            "364924e3493c4fa19de18fe672a5453a",
            "ae063933f3a7425c9815c6ef684a10ce",
            "237f1fbf46f341b4963e4e29f290b889",
            "fd87e743ef1349e7b84b318c4f84790d",
            "85d5739787e346b5b59c8e7496ab2f57",
            "861a631d696a48388173feba479c4f2c",
            "7f6cb7ee653a46d7b349ba0c4a12c40d",
            "0be6a25354f34daf979cf4845be9ee8f",
            "82480bdbcb1a419fa02d93701cd28f2f",
            "a65622bfb5504f5cafc45706931ace1d",
            "478be8dc1d3a46f294e26ab3ba428af6",
            "3cb7b8a0a98f435e89f3b85742241a40",
            "4616225b5999411c9d9287ab3c53a3c5",
            "56797593bc6148e4bad13ca9cf191664",
            "540dc59b3c3c4592af9bfc0673699244",
            "6d226ff5b0e4498c96d89db73db7e3ca",
            "aeb1738482154f9ab0710991e6c82bf6"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746948462363,
          "user_tz": -480,
          "elapsed": 1333320,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e1290515-09b3-4551-9095-9bb2362facd0"
      },
      "id": "y96IIJHRmVDT",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at Jsevisal/CrossEncoder-ModernBERT-base-qnli and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwilson_ng\u001b[0m (\u001b[33mwilson_ng-govtech\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250511_061730-sd4f3wkq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/wilson_ng-govtech/colab_test/runs/sd4f3wkq' target=\"_blank\">Jsevisal/CrossEncoder-ModernBERT-base-qnli2025-05-11 06:17:29</a></strong> to <a href='https://wandb.ai/wilson_ng-govtech/colab_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/wilson_ng-govtech/colab_test' target=\"_blank\">https://wandb.ai/wilson_ng-govtech/colab_test</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/wilson_ng-govtech/colab_test/runs/sd4f3wkq' target=\"_blank\">https://wandb.ai/wilson_ng-govtech/colab_test/runs/sd4f3wkq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model\n",
            "Starting model training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd1ebaa7f20840a398836b89dcbecb3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Iteration:   0%|          | 0/7348 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f6cb7ee653a46d7b349ba0c4a12c40d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT compiled_embeddings /usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py line 212 \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/3f/c3fpix7nh3hsjglitgkptxxbdc6braye7l2zl5hs44lnoh7dflqq.py\", line 111, in <module>\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/3f/c3fpix7nh3hsjglitgkptxxbdc6braye7l2zl5hs44lnoh7dflqq.py\", line 111, in <module>\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 06:17:34.836000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
            "  warnings.warn(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT compiled_mlp /usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py line 531 \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/de/cdelu4gt73bckdf6sypz6icdwndoetm7lkaarmiln3emc4cwsusc.py\", line 152, in <module>\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/de/cdelu4gt73bckdf6sypz6icdwndoetm7lkaarmiln3emc4cwsusc.py\", line 152, in <module>\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 06:17:35.757000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py line 245 \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/3o/c3oxy72yaghqtauhbd4uku5ed6pa6neywkmbpviymmkgrx3hrrub.py\", line 83, in <module>\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/3o/c3oxy72yaghqtauhbd4uku5ed6pa6neywkmbpviymmkgrx3hrrub.py\", line 83, in <module>\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 06:17:36.212000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/activations.py line 77 \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/5h/c5hftdqfozgaamehaugza3fc2sj2sk42s4nwqphywds5r2jxuinp.py\", line 79, in <module>\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/5h/c5hftdqfozgaamehaugza3fc2sj2sk42s4nwqphywds5r2jxuinp.py\", line 79, in <module>\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 06:17:37.038000 94 torch/_dynamo/convert_frame.py:1125] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, step: 100, time: 1.555772304534912, scores:  {'vali_f1': 0.5828437132784959, 'vali_precision': 0.4834307992202729, 'vali_recall': 0.7337278106508875, 'weighted_precision': 0.462275256943517, 'weighted_recall': 0.4732937685459941, 'weighted_f1': 0.4347198443444089, 'macro_precision': 0.46221229402007435, 'macro_recall': 0.4725186672302057, 'macro_f1': 0.4342789994963908, 'train_loss': 0.6859716203808784}\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n",
            "Epoch: 0, step: 200, time: 1.5695741176605225, scores:  {'vali_f1': 0.5828437132784959, 'vali_precision': 0.4834307992202729, 'vali_recall': 0.7337278106508875, 'weighted_precision': 0.462275256943517, 'weighted_recall': 0.4732937685459941, 'weighted_f1': 0.4347198443444089, 'macro_precision': 0.46221229402007435, 'macro_recall': 0.4725186672302057, 'macro_f1': 0.4342789994963908, 'train_loss': 0.6886652563512325}\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n",
            "Epoch: 0, step: 300, time: 1.558321237564087, scores:  {'vali_f1': 0.5828437132784959, 'vali_precision': 0.4834307992202729, 'vali_recall': 0.7337278106508875, 'weighted_precision': 0.462275256943517, 'weighted_recall': 0.4732937685459941, 'weighted_f1': 0.4347198443444089, 'macro_precision': 0.46221229402007435, 'macro_recall': 0.4725186672302057, 'macro_f1': 0.4342789994963908, 'train_loss': 0.6965185672044754}\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n",
            "Epoch: 0, step: 400, time: 1.5720548629760742, scores:  {'vali_f1': 0.5828437132784959, 'vali_precision': 0.4834307992202729, 'vali_recall': 0.7337278106508875, 'weighted_precision': 0.462275256943517, 'weighted_recall': 0.4732937685459941, 'weighted_f1': 0.4347198443444089, 'macro_precision': 0.46221229402007435, 'macro_recall': 0.4725186672302057, 'macro_f1': 0.4342789994963908, 'train_loss': 0.7020650819689035}\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n",
            "Epoch: 0, step: 500, time: 1.5599069595336914, scores:  {'vali_f1': 0.5828437132784959, 'vali_precision': 0.4834307992202729, 'vali_recall': 0.7337278106508875, 'weighted_precision': 0.462275256943517, 'weighted_recall': 0.4732937685459941, 'weighted_f1': 0.4347198443444089, 'macro_precision': 0.46221229402007435, 'macro_recall': 0.4725186672302057, 'macro_f1': 0.4342789994963908, 'train_loss': 0.6935156386494636}\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n",
            "Epoch: 0, step: 600, time: 1.5621964931488037, scores:  {'vali_f1': 0.5828437132784959, 'vali_precision': 0.4834307992202729, 'vali_recall': 0.7337278106508875, 'weighted_precision': 0.462275256943517, 'weighted_recall': 0.4732937685459941, 'weighted_f1': 0.4347198443444089, 'macro_precision': 0.46221229402007435, 'macro_recall': 0.4725186672302057, 'macro_f1': 0.4342789994963908, 'train_loss': 0.6911096640427907}\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n",
            "Epoch: 0, step: 700, time: 1.56449556350708, scores:  {'vali_f1': 0.5828437132784959, 'vali_precision': 0.4834307992202729, 'vali_recall': 0.7337278106508875, 'weighted_precision': 0.462275256943517, 'weighted_recall': 0.4732937685459941, 'weighted_f1': 0.4347198443444089, 'macro_precision': 0.46221229402007435, 'macro_recall': 0.4725186672302057, 'macro_f1': 0.4342789994963908, 'train_loss': 0.6921556530679975}\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n",
            "Epoch: 0, step: 800, time: 1.582956314086914, scores:  {'vali_f1': 0.5811764705882353, 'vali_precision': 0.482421875, 'vali_recall': 0.7307692307692307, 'weighted_precision': 0.4604122448208594, 'weighted_recall': 0.47181008902077154, 'weighted_f1': 0.43359773883454916, 'macro_precision': 0.4603467399691358, 'macro_recall': 0.4710393772893773, 'macro_f1': 0.43315851641861564, 'train_loss': 0.6918482928350568}\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n",
            "Epoch: 0, step: 900, time: 1.5830843448638916, scores:  {'vali_f1': 0.5811764705882353, 'vali_precision': 0.482421875, 'vali_recall': 0.7307692307692307, 'weighted_precision': 0.4604122448208594, 'weighted_recall': 0.47181008902077154, 'weighted_f1': 0.43359773883454916, 'macro_precision': 0.4603467399691358, 'macro_recall': 0.4710393772893773, 'macro_f1': 0.43315851641861564, 'train_loss': 0.6899432947238286}\n",
            "Save model to /content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /content/classification/Jsevisal gs://mddi-reach-conversation/modernbert-output/"
      ],
      "metadata": {
        "id": "7K8CM0hFuR1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746948493702,
          "user_tz": -480,
          "elapsed": 31340,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "81251a45-0c0e-4939-9380-23172d1d3c94"
      },
      "id": "7K8CM0hFuR1t",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/metrics/classification_evaluation_Jsevisal_CrossEncoder-ModernBERT-base-qnli_v6_results.json [Content-Type=application/json]...\n",
            "/ [0/36 files][    0.0 B/  3.4 GiB]   0% Done                                   \rCopying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/special_tokens_map.json [Content-Type=application/json]...\n",
            "/ [0/36 files][    0.0 B/  3.4 GiB]   0% Done                                   \rCopying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/config.json [Content-Type=application/json]...\n",
            "/ [0/36 files][    0.0 B/  3.4 GiB]   0% Done                                   \rCopying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/tokenizer.json [Content-Type=application/json]...\n",
            "/ [0/36 files][    0.0 B/  3.4 GiB]   0% Done                                   \rCopying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/tokenizer_config.json [Content-Type=application/json]...\n",
            "/ [0/36 files][    0.0 B/  3.4 GiB]   0% Done                                   \rCopying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v6/model/model.safetensors [Content-Type=application/octet-stream]...\n",
            "/ [0/36 files][    0.0 B/  3.4 GiB]   0% Done                                   \rCopying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v5/model/special_tokens_map.json [Content-Type=application/json]...\n",
            "/ [0/36 files][    0.0 B/  3.4 GiB]   0% Done                                   \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v5/metrics/classification_evaluation_Jsevisal_CrossEncoder-ModernBERT-base-qnli_v5_results.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v5/model/config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v5/model/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v5/model/tokenizer.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v5/model/model.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v3/metrics/classification_evaluation_Jsevisal_CrossEncoder-ModernBERT-base-qnli_v3_results.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v3/model/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v3/model/config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v3/model/tokenizer.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v3/model/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v3/model/model.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v4/metrics/classification_evaluation_Jsevisal_CrossEncoder-ModernBERT-base-qnli_v4_results.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v4/model/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v2/model/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v2/model/config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v4/model/model.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v4/model/config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v4/model/tokenizer.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v2/model/tokenizer.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v4/model/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v2/model/model.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v2/model/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v2/metrics/classification_evaluation_Jsevisal_CrossEncoder-ModernBERT-base-qnli_v2_results.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v1/model/model.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v1/model/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v1/model/tokenizer.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v1/model/config.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v1/metrics/classification_evaluation_Jsevisal_CrossEncoder-ModernBERT-base-qnli_v1_results.json [Content-Type=application/json]...\n",
            "Copying file:///content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v1/model/special_tokens_map.json [Content-Type=application/json]...\n",
            "/ [36/36 files][  3.4 GiB/  3.4 GiB] 100% Done 111.9 MiB/s ETA 00:00:00         \n",
            "Operation completed over 36 objects/3.4 GiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-02EzyzYuhP7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746948493703,
          "user_tz": -480,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "-02EzyzYuhP7",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "Ugvapc3U6uPh"
      },
      "id": "Ugvapc3U6uPh"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "model_name =\"Jsevisal/CrossEncoder-ModernBERT-base-qnli\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "folder = f\"/content/classification/Jsevisal/CrossEncoder-ModernBERT-base-qnli_v2/model\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(folder, local_files_only=True)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape, param.requires_grad)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jctFi0nK6v0i",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949574334,
          "user_tz": -480,
          "elapsed": 11027,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d4a8a2d6-e510-4b56-e5ea-a470b2be4b3c"
      },
      "id": "jctFi0nK6v0i",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-11 07:46:11,597] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "model.embeddings.tok_embeddings.weight torch.Size([50368, 768]) True\n",
            "model.embeddings.norm.weight torch.Size([768]) True\n",
            "model.layers.0.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.0.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.0.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.0.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.0.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.1.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.1.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.1.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.1.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.1.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.1.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.2.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.2.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.2.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.2.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.2.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.2.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.3.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.3.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.3.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.3.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.3.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.3.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.4.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.4.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.4.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.4.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.4.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.4.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.5.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.5.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.5.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.5.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.5.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.5.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.6.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.6.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.6.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.6.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.6.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.6.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.7.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.7.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.7.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.7.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.7.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.7.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.8.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.8.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.8.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.8.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.8.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.8.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.9.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.9.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.9.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.9.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.9.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.9.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.10.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.10.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.10.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.10.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.10.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.10.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.11.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.11.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.11.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.11.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.11.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.11.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.12.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.12.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.12.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.12.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.12.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.12.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.13.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.13.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.13.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.13.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.13.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.13.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.14.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.14.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.14.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.14.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.14.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.14.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.15.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.15.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.15.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.15.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.15.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.15.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.16.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.16.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.16.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.16.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.16.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.16.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.17.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.17.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.17.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.17.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.17.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.17.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.18.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.18.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.18.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.18.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.18.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.18.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.19.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.19.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.19.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.19.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.19.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.19.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.20.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.20.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.20.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.20.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.20.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.20.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.layers.21.attn_norm.weight torch.Size([768]) True\n",
            "model.layers.21.attn.Wqkv.weight torch.Size([2304, 768]) True\n",
            "model.layers.21.attn.Wo.weight torch.Size([768, 768]) True\n",
            "model.layers.21.mlp_norm.weight torch.Size([768]) True\n",
            "model.layers.21.mlp.Wi.weight torch.Size([2304, 768]) True\n",
            "model.layers.21.mlp.Wo.weight torch.Size([768, 1152]) True\n",
            "model.final_norm.weight torch.Size([768]) True\n",
            "head.dense.weight torch.Size([768, 768]) True\n",
            "head.norm.weight torch.Size([768]) True\n",
            "classifier.weight torch.Size([1, 768]) True\n",
            "classifier.bias torch.Size([1]) True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModernBertForSequenceClassification(\n",
              "  (model): ModernBertModel(\n",
              "    (embeddings): ModernBertEmbeddings(\n",
              "      (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
              "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): ModernBertEncoderLayer(\n",
              "        (attn_norm): Identity()\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "      (1-21): 21 x ModernBertEncoderLayer(\n",
              "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (head): ModernBertPredictionHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
              "    (act): GELUActivation()\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (drop): Dropout(p=0.0, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "version = \"v6\"\n",
        "model_version_name = model_name.replace(\"/\", \"_\") + \"_\" + version\n",
        "\n",
        "def create_batch(dataset, batch_size=None, shuffle=True, stratified=True):\n",
        "\n",
        "\n",
        "    # If shuffle is True, shuffle the dataset first\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(seed=42)  # Shuffle with a seed for reproducibility\n",
        "\n",
        "    # Set the format for Hugging Face datasets (before passing to DataLoader)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    if stratified:\n",
        "        # Get the labels and calculate the class distribution\n",
        "        labels = np.array(dataset['label'])  # Convert labels to numpy array\n",
        "        class_counts = torch.tensor([sum(labels == i) for i in set(labels)])  # Count each class\n",
        "        class_weights = 1.0 / class_counts  # Calculate inverse frequency (more weight for less frequent classes)\n",
        "        sample_weights = class_weights[labels]  # Assign a weight to each sample based on its label\n",
        "\n",
        "        # Create a WeightedRandomSampler for stratified sampling\n",
        "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "        # Create the DataLoader using the sampler\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "    else:\n",
        "        # Without stratified sampling\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    return dataloader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "imrF0qH-74AN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949623480,
          "user_tz": -480,
          "elapsed": 75,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "imrF0qH-74AN",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n"
      ],
      "metadata": {
        "id": "CpXNjiY_U3lQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746949866695,
          "user_tz": -480,
          "elapsed": 75,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "CpXNjiY_U3lQ",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_batchloader = create_batch(test_dataloader, batch_size=1, shuffle=False, stratified=False)\n",
        "\n",
        "label = []\n",
        "test_pred_scores = []\n",
        "for batch in tqdm(\n",
        "    test_batchloader, desc=\"Iteration\", smoothing=0.05\n",
        "):\n",
        "    model.eval()\n",
        "    features = {k: torch.Tensor(v).to(device) for k, v in batch.items() if k != 'label'}\n",
        "    model_predictions = model(**features, return_dict=True)\n",
        "    label += batch[\"label\"].tolist()\n",
        "    logits = torch.sigmoid(model_predictions.logits).detach().cpu().numpy().tolist()\n",
        "    test_pred_scores.extend(logits)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YBjSspwh757V",
        "executionInfo": {
          "status": "error",
          "timestamp": 1746950738154,
          "user_tz": -480,
          "elapsed": 869568,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7761d816-8f61-4504-a2c8-33d70ab147d3"
      },
      "id": "YBjSspwh757V",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rIteration:   0%|          | 0/1349 [00:00<?, ?it/s]W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT compiled_embeddings /usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py line 212 \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/tq/ctqedvdzrejqkciyg4peg2d4pbuul4eifhtbrrxie6gnmf4epjhg.py\", line 111, in <module>\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/tq/ctqedvdzrejqkciyg4peg2d4pbuul4eifhtbrrxie6gnmf4epjhg.py\", line 111, in <module>\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 07:51:09.152000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
            "  warnings.warn(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT compiled_mlp /usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py line 531 \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/6f/c6fnn2547bxazigouluxzgktppfuw25gifv7q6qtsxxt5hum53id.py\", line 152, in <module>\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/6f/c6fnn2547bxazigouluxzgktppfuw25gifv7q6qtsxxt5hum53id.py\", line 152, in <module>\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 71, in __init__\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125]     raise TypeError(\"Signature keys must be string\")\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] TypeError: Signature keys must be string\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 07:51:10.017000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py line 245 \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/t7/ct73aa2uwiib34s5zhjitka2ai3g53ci7pr3amkqdpc7ewvua5tx.py\", line 83, in <module>\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/t7/ct73aa2uwiib34s5zhjitka2ai3g53ci7pr3amkqdpc7ewvua5tx.py\", line 83, in <module>\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 07:51:10.485000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/activations.py line 77 \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/ka/ckaf3k2oh4d4twr3rtv25dxytsply3v2ebkkd3slg27cdyg2t7yx.py\", line 79, in <module>\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1867, in _compile_to_module\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     mod = PyCodeCache.load_by_key_path(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 2876, in load_by_key_path\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     mod = _reload_python_module(key, path)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     exec(code, mod.__dict__, mod.__dict__)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/tmp/torchinductor_root/ka/ckaf3k2oh4d4twr3rtv25dxytsply3v2ebkkd3slg27cdyg2t7yx.py\", line 79, in <module>\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     async_compile.wait(globals())\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/async_compile.py\", line 276, in wait\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     scope[key] = result.result()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 3341, in result\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     result = self.future.result()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return self.__get_result()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     raise self._exception\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] torch._inductor.compile_worker.subproc_pool.SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] SubprocException: An exception occurred in a subprocess:\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 270, in do_job\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     result = job()\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     load_kernel().precompile(warm_cache_only=True)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 244, in precompile\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     compiled_binary, launcher = self._precompile_config(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 375, in _precompile_config\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     ASTSource(\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 63, in __init__\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125]     assert isinstance(k, tuple)\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] AssertionError\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0511 07:51:10.912000 18446 torch/_dynamo/convert_frame.py:1125] \n",
            "Iteration: 100%|| 1349/1349 [14:29<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fdb07042f70a>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mbest_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "prediction = [pred[0] for pred in test_pred_scores]\n",
        "test_data[\"score\"] = prediction\n",
        "\n",
        "best_f1 = 0\n",
        "best_threshold = 0\n",
        "\n",
        "for threshold in np.arange(0.01, 1.0, 0.01):\n",
        "    test_data[\"pred\"] = (np.array(prediction) > threshold).astype(int)\n",
        "\n",
        "    score = f1_score(test_data[\"label\"], test_data[\"pred\"])\n",
        "    if score > best_f1:\n",
        "        best_f1 = score\n",
        "        best_threshold = threshold\n"
      ],
      "metadata": {
        "id": "X2a6dNvTYwBB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746950910668,
          "user_tz": -480,
          "elapsed": 382,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "X2a6dNvTYwBB",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_data[\"pred\"] = (np.array(test_data[\"score\"] ) > best_threshold).astype(int)\n",
        "\n",
        "\n",
        "def compute_metrics(true_labels, prediction):\n",
        "    metrics = {}\n",
        "    metrics[\"acuracy\"] = accuracy_score(true_labels, prediction)\n",
        "    metrics[\"f1\"] = f1_score(true_labels, prediction)\n",
        "    metrics[\"precision\"] = precision_score(true_labels, prediction)\n",
        "    metrics[\"recall\"] = recall_score(true_labels, prediction)\n",
        "    metrics[\"f1_weighted\"] = f1_score(true_labels, prediction, average = \"weighted\")\n",
        "    metrics[\"recall_weighted\"] = recall_score(true_labels,prediction, average = \"weighted\")\n",
        "    metrics[\"precision_weighted\"] = precision_score(true_labels, prediction, average = \"weighted\")\n",
        "    metrics[\"f1_marco\"] = f1_score(true_labels, prediction, average = \"macro\")\n",
        "    metrics[\"precision_marco\"] = precision_score(true_labels, prediction, average = \"macro\")\n",
        "    metrics[\"recall_marco\"] = recall_score(true_labels, prediction, average = \"macro\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "compute_metrics(test_data[\"label\"], test_data[\"pred\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQvk6d7j79mG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1746950911546,
          "user_tz": -480,
          "elapsed": 73,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "aa6d1a25-1720-4896-f5af-a41abd0987e5"
      },
      "id": "YQvk6d7j79mG",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acuracy': 0.508524833209785,\n",
              " 'f1': 0.6696562032884903,\n",
              " 'precision': 0.5048835462058603,\n",
              " 'recall': 0.9940828402366864,\n",
              " 'f1_weighted': 0.35578815100782857,\n",
              " 'recall_weighted': 0.508524833209785,\n",
              " 'precision_weighted': 0.6410272214081587,\n",
              " 'f1_marco': 0.35508859368476614,\n",
              " 'precision_marco': 0.6413306619918191,\n",
              " 'recall_marco': 0.507442608825624}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "reach_modernbert_truncated",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd1ebaa7f20840a398836b89dcbecb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_631eefb622294c0592e377caa809ea33",
              "IPY_MODEL_1fe0de4836a44cf5a3b4f6ed2aeb3669",
              "IPY_MODEL_1cb2d8cfbd6a403ba2e768fd4e2c3bc8"
            ],
            "layout": "IPY_MODEL_134e9298ecca47c4a9a66cc676a63de8"
          }
        },
        "631eefb622294c0592e377caa809ea33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_364924e3493c4fa19de18fe672a5453a",
            "placeholder": "",
            "style": "IPY_MODEL_ae063933f3a7425c9815c6ef684a10ce",
            "value": "Epoch:100%"
          }
        },
        "1fe0de4836a44cf5a3b4f6ed2aeb3669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237f1fbf46f341b4963e4e29f290b889",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd87e743ef1349e7b84b318c4f84790d",
            "value": 1
          }
        },
        "1cb2d8cfbd6a403ba2e768fd4e2c3bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d5739787e346b5b59c8e7496ab2f57",
            "placeholder": "",
            "style": "IPY_MODEL_861a631d696a48388173feba479c4f2c",
            "value": "1/1[1:10:10&lt;00:00,4210.93s/it]"
          }
        },
        "134e9298ecca47c4a9a66cc676a63de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "364924e3493c4fa19de18fe672a5453a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae063933f3a7425c9815c6ef684a10ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "237f1fbf46f341b4963e4e29f290b889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd87e743ef1349e7b84b318c4f84790d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85d5739787e346b5b59c8e7496ab2f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "861a631d696a48388173feba479c4f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f6cb7ee653a46d7b349ba0c4a12c40d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0be6a25354f34daf979cf4845be9ee8f",
              "IPY_MODEL_82480bdbcb1a419fa02d93701cd28f2f",
              "IPY_MODEL_a65622bfb5504f5cafc45706931ace1d"
            ],
            "layout": "IPY_MODEL_478be8dc1d3a46f294e26ab3ba428af6"
          }
        },
        "0be6a25354f34daf979cf4845be9ee8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cb7b8a0a98f435e89f3b85742241a40",
            "placeholder": "",
            "style": "IPY_MODEL_4616225b5999411c9d9287ab3c53a3c5",
            "value": "Iteration:12%"
          }
        },
        "82480bdbcb1a419fa02d93701cd28f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56797593bc6148e4bad13ca9cf191664",
            "max": 7348,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_540dc59b3c3c4592af9bfc0673699244",
            "value": 899
          }
        },
        "a65622bfb5504f5cafc45706931ace1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d226ff5b0e4498c96d89db73db7e3ca",
            "placeholder": "",
            "style": "IPY_MODEL_aeb1738482154f9ab0710991e6c82bf6",
            "value": "899/7348[1:10:10&lt;2:59:14,1.67s/it]"
          }
        },
        "478be8dc1d3a46f294e26ab3ba428af6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb7b8a0a98f435e89f3b85742241a40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4616225b5999411c9d9287ab3c53a3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56797593bc6148e4bad13ca9cf191664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "540dc59b3c3c4592af9bfc0673699244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d226ff5b0e4498c96d89db73db7e3ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeb1738482154f9ab0710991e6c82bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}