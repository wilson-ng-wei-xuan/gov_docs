from __future__ import annotations

from typing import Annotated, Any

from atlas.genai.schemas import Chat as ChatBase
from atlas.genai.schemas import Query as ChatMessageBase
from atlas.schemas.base import Uuid
from pydantic import BaseModel, ConfigDict, Field, StrictStr

__doc__ = """
Data models for Chats, includes reusable fields and MongoDB schema models
"""

__all__ = ("Chunk", "Citation", "RAGQuery", "ChatMessage", "Chat", "ChatFull")


class Chunk(BaseModel):
    """
    Chunks generated from the embeddings process

    Attributes:
        id (str): ID of the Chunk from the respective pipelines
        source (str): Source of the Chunk
        chunk (str | None): Chunk of text, defaults to None
        knowledge_base (Uuid | None): Knowledge Base Uuid reference
                                      of the Chunk, defaults to
                                      None
        score (float | None): Score of the chunk, defaults to None
        metadata (dict[str, Any]): Additional metadata associated
                                   with a chunk, defaults to an
                                   empty dictionary
    """

    model_config: ConfigDict = ConfigDict(extra="allow")

    id: str
    source: str
    chunk: str | None = None
    knowledge_base: Annotated[Uuid | None, Field(None, alias="knowledgeBase")]
    score: float | None = None
    metadata: dict[str, Any] = {}

    def prepare_chunk(self) -> str:
        """
        Prepares the chunk for sending to the LLM

        Returns:
            str: Formatted chunk
        """
        return f"\n\n=====\nSource:{self.source}\n-----\n{self.chunk}\n=====\n"


class Citation(BaseModel):
    """
    Representation of citation details of texts retrieved from the
    model

    Attributes:
        source (str): Source name of the associated chunk
        knowledge_base (Uuid | None): Knowledge Base ID, defaults
                                      to None
    """

    model_config: ConfigDict = ConfigDict(extra="allow")

    source: str
    knowledge_base: Uuid | None = None


class RAGQuery(BaseModel):
    """
    Representation of RAG query details returned from a RAG pipeline

    Attributes:
        id (Uuid | None): RAG pipeline ID, defaults to None
        type (str | None): RAG pipeline type, defaults to None
        chunks (list[Chunk]): List of returned chunks, defaults to an
                              empty list
        citations (list[Citation]): List of citations utilised in the
    """

    model_config: ConfigDict = ConfigDict(extra="allow")

    id: Uuid | None = None
    type: str | None = None
    chunks: list[Chunk] = []
    citations: list[Citation] = []

    def prepare_chunks(self) -> str:
        """
        Prepares all the chunks for sending to LLM

        Returns:
            str: Prepared chunk
        """
        return "\n".join(c.prepare_chunk() for c in self.chunks)

    def get_citations(self, response: str) -> None:
        """
        Populates the citation details based on the response
        from the LLM

        Args:
            response (str): Response returned from the LLM

        Returns:
            None
        """
        for chunk in self.chunks:
            if chunk.source in response:
                self.citations.append(
                    Citation(
                        source=chunk.source,
                        knowledge_base=chunk.knowledge_base,
                    )
                )


class ChatMessage(ChatMessageBase):
    """
    Representation of an individual query to an LLM and its
    associated response

    Attributes:
        id (Uuid): UUID string, autogenerated
        chat (Uuid): UUID reference to the Chat
        query (ChatInteraction): Query to the LLM
        response (ChatInteraction | None): Response from the LLM, defaults to
                                           None
        tokens (Tokens): Number of tokens consumed by the interaction, defaults to
                         default Tokens values
        model (ModelID): LLM model associated with Chat Message, inherited
        system_prompt (ChatInteraction): System prompt template, defaults to
                                         None
        params (dict[str, Any]): AI Model parameters used to generate the Chat Message,
                                 inherited
        properties (dict[str, Any]): Additional properties used in the business logic,
                                     inherited
        liked (bool | None): Indicates if the message has been liked, defaults to None
        pinned (bool): Indicates if the message has been pinned, defaults to False
        rag (RAGQuery): RAG query details retrieved from the RAG pipeline, defaults to
                        the default RAGQuery details
    """  # noqa: E501

    chat: Uuid
    liked: bool | None = None
    pinned: bool = False
    rag: RAGQuery = RAGQuery()


class Chat(ChatBase):
    """
    Generic representation of a Chat conversation that stores
    properties regarding the interactions with an LLM.

    Attributes:
        id (Uuid): UUID string
        name (StrictStr): Name field, defaults to an empty string
        user_prompt (PromptTemplate | None): User prompt template, defaults to
                                             None
        meta (Meta): Metadata associated with the resource,
                     defaults to the default Meta values
        modifications (ModificationDict): Modifications made to the Chat, inherited
        agents (list[Uuid]): List of agents associated with the Chat, defaults to an
                             empty list

        tokens (ChatTokens): Number of tokens cumulatively consumed, defaults to
                             default ChatTokens values
        model (ModelID): LLM model associated with Chat Message, inherited
        system_prompt (PromptTemplate): System prompt template, defaults
                                        to default Prompt Template values
        params (dict[str, Any]): AI Model parameters used to generate the Chat Message,
                                 inherited
        properties (dict[str, Any]): Additional properties used in the business logic,
                                     inherited
    """  # noqa: E501

    name: StrictStr = ""
    agents: list[Uuid] = []


class ChatFull(Chat):
    """
    Full chat conversation with historical messages.

    Attributes:
        id (Uuid): UUID string, inherited
        name (constr): Name field, only allows hexadecimal values,
                       hyphen and underscore, inherited
        user_prompt (PromptTemplate | None): User prompt template, inherited
        meta (Meta): Metadata associated with the resource,
                     defaults to the default Meta values
        modifications (ModificationDict): Modifications made to the Chat, inherited
        messages (list[Uuid] | list[ChatMessage]): Chat Messages associated with the
                                                   conversation, defaults to an empty
                                                   list
        agents (list[Uuid]): List of agents associated with the Chat, defaults to an
                             empty list

        tokens (ChatTokens): Number of tokens cumulatively consumed, inherited
        model (ModelID): LLM model associated with Chat Message, inherited
        system_prompt (PromptTemplate): System prompt template, defaults
                                        to default Prompt Template values
        params (dict[str, Any]): AI Model parameters used to generate the Chat Message,
                                 inherited
        properties (dict[str, Any]): Additional properties used in the business logic,
                                     inherited
    """  # noqa: E501

    messages: list[Uuid] | list[ChatMessage] = []
